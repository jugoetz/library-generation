{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Design Validation Plates\n",
    "\n",
    "We want to design 4 plates for experimental validation\n",
    "\n",
    "## exp100 / \"cherry-picking plate\"\n",
    "One plate containing 320 compounds.\n",
    "Design Process:\n",
    "1. Select 15 representative Initiators, 15 representative Monomers and 10 representative Terminators\n",
    "2. Filter out any known combinations\n",
    "3. Let the model predict reaction outcome and remove all combinations where the predicted reaction outcome is negative\n",
    "4. Randomly select 320 of the remaining compounds\n",
    "5. Design source plate and transfer files for this plate\n",
    "\n",
    "## exp101 / \"extrapolation plates\"\n",
    "Three plates of 320 compounds each for a total of 960 compounds\n",
    "12 I x 10 M x 8 T\n",
    "We have identified 12 previously unused Initiators\n",
    "Design process:\n",
    "1. Select 10 representative Monomers and 8 representative Terminators \n",
    "    (here we can just use the first 10, resp. 8) sampled for the other experiment as sampling is done before prediction and thus unbiased.\n",
    "2. We will use a full factorial design and only need to design the appropriate source plate and transfer files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pathlib\n",
    "import sys\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from rdkit import Chem\n",
    "from rdkit import SimDivFilters, DataStructs\n",
    "from rdkit.Chem import Draw, rdMolDescriptors\n",
    "\n",
    "sys.path.append(str(pathlib.Path().resolve().parents[1]))\n",
    "from src.util.db_utils import SynFermDatabaseConnection\n",
    "from src.util.rdkit_util import desalt_building_block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "con = SynFermDatabaseConnection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = con.con.execute(\"SELECT * FROM building_blocks\").fetchall()\n",
    "header = [i[1] for i in con.con.execute(\"PRAGMA table_info(building_blocks)\").fetchall()]\n",
    "df = pd.DataFrame(res, columns=header)\n",
    "initiators = df.loc[df[\"category\"] == \"I\"]\n",
    "monomers = df.loc[df[\"category\"] == \"M\"]\n",
    "terminators = df.loc[df[\"category\"] == \"T\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Idea 1: RDKit's MinMaxPicker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mols = [Chem.MolFromSmiles(smi) for smi in terminators[\"SMILES\"]]\n",
    "\n",
    "fps = [rdMolDescriptors.GetMorganFingerprintAsBitVect(m,2) for m in mols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://rdkit.blogspot.com/2014/08/optimizing-diversity-picking-in-rdkit.html\n",
    "def dmat_sim(fps,ntopick):\n",
    "    ds=[]\n",
    "    for i in range(1,len(fps)):\n",
    "         ds.extend(DataStructs.BulkTanimotoSimilarity(fps[i],fps[:i],returnDistance=True))\n",
    "    mmp =SimDivFilters.MaxMinPicker()\n",
    "    ids=mmp.Pick(np.array(ds),len(fps),ntopick)\n",
    "    return ids\n",
    "\n",
    "dmat_ids=dmat_sim(fps, 10)\n",
    "\n",
    "Draw.MolsToGridImage([mols[x] for x in dmat_ids],molsPerRow=5, subImgSize=(300,300))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "### Idea 1 conclusion\n",
    "Due to how the MinMaxPicker works we sample the \"weirdest\" compounds, the ones most disparate from each other and thus our data set. This does not make sense when we want representative compounds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Idea 2: Random picking\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# still need to filter to only have the ones that were not excluded during data analysis\n",
    "experiments = con.get_experiments_table_as_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_exp = experiments.loc[(~experiments[\"valid\"].str.contains(\"ERROR\", na=False)) & (experiments[\"exp_nr\"].between(4,29))]\n",
    "valid_ini = set(valid_exp[\"initiator_long\"].to_numpy().tolist())\n",
    "valid_mon = set(valid_exp[\"monomer_long\"].to_numpy().tolist())\n",
    "valid_ter = set(valid_exp[\"terminator_long\"].to_numpy().tolist())\n",
    "\n",
    "len(valid_ini), len(valid_mon), len(valid_ter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw randomly\n",
    "select_ini = initiators.loc[initiators[\"long\"].isin(valid_ini)].sample(11, random_state=1)\n",
    "select_mon = monomers.loc[monomers[\"long\"].isin(valid_mon)].sample(15, random_state=2)\n",
    "select_ter = terminators.loc[terminators[\"long\"].isin(valid_ter)].sample(10, random_state=3)\n",
    "\n",
    "selected = pd.concat((select_ini, select_mon, select_ter))\n",
    "\n",
    "Draw.MolsToGridImage([Chem.MolFromSmiles(smi) for smi in selected[\"SMILES\"]], legends=selected[\"long\"].to_numpy().tolist(), molsPerRow=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not use 4-Pyrazole002, BiPh009, Mon082. These are the building blocks we had to remove in data curation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from these, we remove all combinations where we attempted synthesis\n",
    "products = [f\"{i} + {j} + {k}\" for i in select_ini[\"long\"] for j in select_mon[\"long\"] for k in select_ter[\"long\"]]\n",
    "len(products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "attempted = experiments.loc[experiments[\"exp_nr\"].between(4,29), \"long_name\"].to_numpy().tolist()\n",
    "not_attempted = [p for p in products if p not in attempted]\n",
    "len(not_attempted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "not_attempted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for all the not attempted ones, write a file with the following columns:\n",
    "# idx, vl_id, long_name, reaction_SMILES_atom_mapped\n",
    "vl_ids, smiles = [], []\n",
    "for long in not_attempted:\n",
    "    res = con.con.execute(\"SELECT id, SMILES FROM virtuallibrary WHERE long_name IN (?) AND type = 'A';\", (long,)).fetchall()\n",
    "    assert len(res) == 1\n",
    "    vl_ids.append(res[0][0])\n",
    "    smiles.append(res[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"vl_id\": vl_ids, \"long_name\": not_attempted, \"product_A_smiles\": smiles})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"../../data/curated_data/validation-plate_candidates.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "(at this point, predictions where run on a different machine in notebook `inference_validation-plate.ipynb`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load predictions\n",
    "preds = pd.read_csv(\"../../data/curated_data/validation-plate_candidates_predictions.csv\")\n",
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "When preparing for the experiment, it turned out that our stock of Spiro003 is depleted. We remove this building block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many possible experiments will be removed?\n",
    "len(preds.loc[preds[\"long_name\"].str.contains(\"Spiro003\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly sample 320 of the positive predictions to synthesize\n",
    "sample = preds.loc[(preds[\"pred_A\"] == 1) & (~preds[\"long_name\"].str.contains(\"Spiro003\"))].sample(320, random_state=42)\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "building_blocks = sample[\"long_name\"].str.split(\"+\", expand=True).applymap(lambda x: x.strip())\n",
    "building_blocks[0].drop_duplicates().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "building_blocks[1].drop_duplicates().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "building_blocks[2].drop_duplicates().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# design the source plate\n",
    "from labware.plates import Plate384, Plate384Echo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_plate = Plate384(max_vol=65000, dead_vol=15000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add initiators to source plate\n",
    "for name, count in building_blocks[0].value_counts().items():\n",
    "    source_plate.fill_well(source_plate.free(), name, count*990+15000)\n",
    "\n",
    "# add monomers to source plate, starting at row F\n",
    "for name, count in building_blocks[1].value_counts().items():\n",
    "    source_plate.fill_well(source_plate.free(from_well=\"F1\"), name, count*990+15000)\n",
    "\n",
    "# add terminators to source plate, starting at row K\n",
    "for name, count in building_blocks[2].value_counts().items():\n",
    "    source_plate.fill_well(source_plate.free(from_well=\"K1\"), name, count*1100+15000)\n",
    "    \n",
    "# add oxalic acid. For 320 reactions, we need two source wells, but we just fill up the entire bottom row for redundancy\n",
    "for _ in range(24):\n",
    "    source_plate.fill_well(source_plate.free(from_well=\"P1\"), \"X\", 65000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(source_plate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_plate.to_csv(\"../../data/plates/exp100/source_plate_layout.csv\", save_volumes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now make the target plate\n",
    "target_plate = Plate384Echo()\n",
    "# add placeholders\n",
    "target_plate.fill_span(\"A1\", \"P2\", \"placeholder\", target_plate.max_vol)\n",
    "target_plate.fill_span(\"A23\", \"P24\", \"placeholder\", target_plate.max_vol)\n",
    "# add sampled reactions\n",
    "for i, row in sample.iterrows():\n",
    "    well = target_plate.free()\n",
    "    compounds = row[\"long_name\"].split(\" + \")\n",
    "    target_plate.fill_well(well, compounds[0], 990)\n",
    "    target_plate.fill_well(well, compounds[1], 990)\n",
    "    target_plate.fill_well(well, compounds[2], 1100)\n",
    "    target_plate.fill_well(well, \"X\", 220)\n",
    "# remove placeholders\n",
    "target_plate.empty_span(\"A1\", \"P2\")\n",
    "target_plate.empty_span(\"A23\", \"P24\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(target_plate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_plate.to_csv(\"../../data/plates/exp100/plate_layout_plate1.csv\", save_volumes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "compound_location = {compound[0]: well for well, compound in source_plate.to_dict().items() if len(compound) == 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to prepare the tranfer files, just iterate the target plate\n",
    "header = ['Source Barcode', 'Source Well', 'Destination Barcode', 'Destination Well', 'Volume']\n",
    "step1_transfers, step2_transfers = [], []\n",
    "step1_transfers.insert(0, header)\n",
    "step2_transfers.insert(0, header)\n",
    "source_barcode = 'Source1'\n",
    "destination_barcode = 'Synthesis1'\n",
    "for dest_well, compounds, volume in target_plate.iterate_wells():\n",
    "    if volume == 0:\n",
    "        continue  # skip empty wells\n",
    "    step1_transfers.append([source_barcode, compound_location[compounds[0]], destination_barcode, dest_well, 990])\n",
    "    step1_transfers.append([source_barcode, compound_location[compounds[1]], destination_barcode, dest_well, 990])\n",
    "    step2_transfers.append([source_barcode, compound_location[compounds[2]], destination_barcode, dest_well, 1100])\n",
    "\n",
    "    # add oxalic acid\n",
    "    if int(dest_well[1:]) < 13:\n",
    "        step1_transfers.append([source_barcode, \"P1\", destination_barcode, dest_well, 220])\n",
    "    else:\n",
    "        step1_transfers.append([source_barcode, \"P2\", destination_barcode, dest_well, 220])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# correct number of transfers?\n",
    "assert len(step1_transfers) == 320 * 3 + 1\n",
    "# source wells occur no more than 50 times (volume limit)?\n",
    "used_wells = [l[1] for l in step1_transfers[1:]]\n",
    "for k, v in Counter(used_wells).items():\n",
    "    if k.startswith(\"A\") or k.startswith(\"F\"):\n",
    "        assert v <= 50\n",
    "    elif k.startswith(\"P\"):\n",
    "        assert v == 160\n",
    "    else:\n",
    "        raise ValueError(f\"unexpected well {k}\")\n",
    "# all transfers are unique\n",
    "assert len(step1_transfers) == len(set([tuple(line) for line in step1_transfers]))\n",
    "# all destination wells are used exactly thrice\n",
    "used_dest_wells = [l[2] + \"_\" + l[3] for l in step1_transfers[1:]]\n",
    "for k, v in Counter(used_dest_wells).items():\n",
    "    assert v == 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to file\n",
    "with open('validation_exp100_step1.csv', 'w') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerows(step1_transfers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# correct number of transfers?\n",
    "assert len(step2_transfers) == 320 + 1\n",
    "# source wells occur no more than 45 times (volume limit)?\n",
    "used_wells = [l[1] for l in step2_transfers[1:]]\n",
    "for k, v in Counter(used_wells).items():\n",
    "    if k.startswith(\"K\"):\n",
    "        assert v <= 50\n",
    "    else:\n",
    "        raise ValueError(f\"unexpected well {k}\")\n",
    "# all transfers are unique\n",
    "assert len(step2_transfers) == len(set([tuple(line) for line in step2_transfers]))\n",
    "# all destination wells are used exactly once\n",
    "used_dest_wells = [l[2] + \"_\" + l[3] for l in step2_transfers[1:]]\n",
    "for k, v in Counter(used_dest_wells).items():\n",
    "    assert v == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to file\n",
    "with open('validation_exp100_step2.csv', 'w') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerows(step2_transfers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "## exp101\n",
    "For exp101, we still need to choose monomers and terminators and we need to make the plate layout files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to choose M/T, we simply use the first 10/8 of the sets randomly selected earlier for exp100 (excluding Spiro003 - not available)\n",
    "mon101 = select_mon.loc[select_mon[\"long\"] != \"Spiro003\"].iloc[0:10].sort_values(by=\"long\")\n",
    "mon101.to_csv(\"../../data/plates/exp101/monomers.csv\", index=False)\n",
    "mon101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "ter101 = select_ter.iloc[0:8].sort_values(by=\"long\")\n",
    "ter101.to_csv(\"../../data/plates/exp101/terminators.csv\", index=False)\n",
    "ter101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ini_lcms_mass(smi):\n",
    "    mol = desalt_building_block(smi)\n",
    "    return Chem.Descriptors.ExactMolWt(mol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ini_lcms_formula(smi):\n",
    "    mol = desalt_building_block(smi)\n",
    "    return Chem.rdMolDescriptors.CalcMolFormula(mol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "initiators = [\"Ph037\", \"Ph038\", \"Ph039\", \"Ph004\", \"BiAl004\", \"Ph040\", \"Ph016\", \"Ph041\", \"Ph042\", \"Ph011\", \"BiAl005\", \"Ph043\"]\n",
    "\n",
    "ini101 = pd.DataFrame(initiators, columns=[\"long\"])\n",
    "\n",
    "ini101[\"SMILES\"] = [Chem.MolToSmiles(Chem.MolFromSmiles(smi)) for smi in [\n",
    "    \"O=S(C1=CC=C(C([B-](F)(F)F)=O)C=C1)(C)=O.[K+]\",\n",
    "    \"O=C([B-](F)(F)F)C1=CC2=C(C=CC=C2)C=C1.[K+]\",\n",
    "    \"O=C([B-](F)(F)F)C1=CC(C#N)=CC=C1.[K+]\",\n",
    "    \"C1C(OC)=CC(C(=O)[B-](F)(F)F)=CC=1.[K+]\",\n",
    "    \"O=C([B-](F)(F)F)CCCCCCl.[K+]\",\n",
    "    \"FC1=C(C(F)(F)F)C=C(C([B-](F)(F)F)=O)C=C1.[K+]\",\n",
    "    \"O=C([B-](F)(F)F)C1=CC(NC=C2)=C2C=C1.[K+]\",\n",
    "    \"O=C([B-](F)(F)F)C1=CC(C(C)(C)C)=CC=C1.[K+]\",\n",
    "    \"O=C([B-](F)(F)F)C1=CC(F)=C(C)C=C1.[K+]\",\n",
    "    \"O=C([B-](F)(F)F)C1=CC=C([N+]([O-])=O)C=C1.[K+]\",\n",
    "    \"O=C([B-](F)(F)F)CCCCO.[K+]\",\n",
    "    \"O=C([B-](F)(F)F)CCCCO.[K+]\",\n",
    "]]\n",
    "ini101[\"category\"] = [\"I\" for _ in range(len(ini101))]\n",
    "ini101[\"boc\"] = 0\n",
    "ini101[\"cbz\"] = 0\n",
    "ini101[\"tbu\"] = 0\n",
    "ini101[\"tms\"] = 0\n",
    "ini101[\"lcms_mass_1\"] = ini101[\"SMILES\"].apply(lambda x: get_ini_lcms_mass(x))\n",
    "ini101[\"lcms_mass_alt\"] = None\n",
    "ini101[\"comment\"] = None\n",
    "ini101[\"lcms_formula_1\"] = ini101[\"SMILES\"].apply(lambda x: get_ini_lcms_formula(x))\n",
    "ini101[\"lcms_formula_alt\"] = None\n",
    "ini101[\"reactant_class\"] = [\n",
    "    \"KAT_arom\",\n",
    "    \"KAT_arom\",\n",
    "    \"KAT_arom\",\n",
    "    \"KAT_arom\",\n",
    "    \"KAT_al\",\n",
    "    \"KAT_arom\",\n",
    "    \"KAT_hetarom\",\n",
    "    \"KAT_arom\",\n",
    "    \"KAT_arom\",\n",
    "    \"KAT_arom\",\n",
    "    \"KAT_al\",\n",
    "    \"KAT_arom\",\n",
    "]\n",
    "ini101.to_csv(\"../../data/plates/exp101/initiators.csv\", index=False)\n",
    "ini101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
