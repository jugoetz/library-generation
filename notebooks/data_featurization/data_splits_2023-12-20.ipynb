{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Splits\n",
    "\n",
    "We want to split the SynFerm data set into train, validation and test data.\n",
    "For all splits, we will do 9 random repetitions.\n",
    "For 1D and 2D split, which both use 3 different groups to split on, these will divide into 3 random repetitions for each of the 3 groups. \n",
    "\n",
    "### 0D Split\n",
    "For the 0D split, we use a random train-test split.\n",
    "We use a 80/10/10 split into train, val, and test set.\n",
    "\n",
    "### 1D Split\n",
    "For the 1D split, we use a (1D) GroupShuffleSplit.\n",
    "Each individual split will be 70/15/15 train/test (of groups not samples!).\n",
    "As groups, we use either initiator, monomer, or terminator.\n",
    "\n",
    "### 2D Split\n",
    "For the 2D split, we use a (2D) GroupShuffleSplit.\n",
    "Each individual split will use 20% of groups as test set and 25% of remaining groups as validation set. \n",
    "Due to the dimensionality, this means we expect 0.2 * 0.2 = 4% of samples in the test and validation set and 0.800^2 * 0.75^2 = 36.0% of samples in the training set.\n",
    "The remaining samples are not used to prevent leakage.\n",
    "As groups, we use either \\[initiator, monomer], \\[monomer, terminator] or \\[initiator, terminator].\n",
    "\n",
    "### 3D Split\n",
    "For the 3D split, we use a (3D) GroupShuffleSplit.\n",
    "Each individual split will use 25% of groups as test set, 33% of remaining groups as validation set, and the remaining groups as training set.\n",
    "Due to the dimensionality, this means we expect 0.25^3 = 1.5% of samples in the test and validation set and 0.75^3 * 0.67^3 = 12.5% of sample in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T17:04:53.936618Z",
     "start_time": "2023-09-07T17:04:53.237504Z"
    }
   },
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import sys\n",
    "\n",
    "sys.path.append(str(pathlib.Path().resolve().parents[1]))\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GroupShuffleSplit, ShuffleSplit\n",
    "\n",
    "from src.definitions import DATA_DIR\n",
    "from src.util.train_test_split import GroupShuffleSplitND\n",
    "from util import write_indices_and_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T17:04:54.279631Z",
     "start_time": "2023-09-07T17:04:53.937616Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39486, 27)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "data_filename = \"synferm_dataset_2023-12-20_39486records.csv\"\n",
    "data_name = data_filename.rsplit(\"_\", maxsplit=1)[0]\n",
    "df = pd.read_csv(DATA_DIR / \"curated_data\" / data_filename)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T17:04:54.290593Z",
     "start_time": "2023-09-07T17:04:54.278995Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>I_long</th>\n",
       "      <th>M_long</th>\n",
       "      <th>T_long</th>\n",
       "      <th>product_A_smiles</th>\n",
       "      <th>I_smiles</th>\n",
       "      <th>M_smiles</th>\n",
       "      <th>T_smiles</th>\n",
       "      <th>reaction_smiles</th>\n",
       "      <th>reaction_smiles_atom_mapped</th>\n",
       "      <th>experiment_id</th>\n",
       "      <th>...</th>\n",
       "      <th>binary_H</th>\n",
       "      <th>scaled_A</th>\n",
       "      <th>scaled_B</th>\n",
       "      <th>scaled_C</th>\n",
       "      <th>scaled_D</th>\n",
       "      <th>scaled_E</th>\n",
       "      <th>scaled_F</th>\n",
       "      <th>scaled_G</th>\n",
       "      <th>scaled_H</th>\n",
       "      <th>major_A-C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2-Pyr003</td>\n",
       "      <td>Fused002</td>\n",
       "      <td>TerABT004</td>\n",
       "      <td>COc1ccc(CCOC(=O)N2C[C@H](NC(=O)c3cccc(Cl)n3)[C...</td>\n",
       "      <td>O=C(c1cccc(Cl)n1)[B-](F)(F)F.[K+]</td>\n",
       "      <td>COc1ccc(CCOC(=O)N2C[C@@H]3NO[C@]4(OC5(CCCCC5)O...</td>\n",
       "      <td>Nc1ccc(F)cc1S</td>\n",
       "      <td>O=C(c1cccc(Cl)n1)[B-](F)(F)F.COc1ccc(CCOC(=O)N...</td>\n",
       "      <td>F[B-](F)(F)[C:1](=[O:2])[c:15]1[cH:16][cH:18][...</td>\n",
       "      <td>56113</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.036021</td>\n",
       "      <td>0.003427</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.020975</td>\n",
       "      <td>0.002958</td>\n",
       "      <td>0.941981</td>\n",
       "      <td>0.914281</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2-Pyr003</td>\n",
       "      <td>Fused002</td>\n",
       "      <td>TerABT007</td>\n",
       "      <td>COc1ccc(CCOC(=O)N2C[C@H](NC(=O)c3cccc(Cl)n3)[C...</td>\n",
       "      <td>O=C(c1cccc(Cl)n1)[B-](F)(F)F.[K+]</td>\n",
       "      <td>COc1ccc(CCOC(=O)N2C[C@@H]3NO[C@]4(OC5(CCCCC5)O...</td>\n",
       "      <td>Nc1cc(Br)ccc1S</td>\n",
       "      <td>O=C(c1cccc(Cl)n1)[B-](F)(F)F.COc1ccc(CCOC(=O)N...</td>\n",
       "      <td>F[B-](F)(F)[C:1](=[O:2])[c:15]1[cH:16][cH:18][...</td>\n",
       "      <td>56114</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006159</td>\n",
       "      <td>0.364398</td>\n",
       "      <td>0.928851</td>\n",
       "      <td>1.106548</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>no_product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2-Pyr003</td>\n",
       "      <td>Fused002</td>\n",
       "      <td>TerABT013</td>\n",
       "      <td>COc1ccc(CCOC(=O)N2C[C@H](NC(=O)c3cccc(Cl)n3)[C...</td>\n",
       "      <td>O=C(c1cccc(Cl)n1)[B-](F)(F)F.[K+]</td>\n",
       "      <td>COc1ccc(CCOC(=O)N2C[C@@H]3NO[C@]4(OC5(CCCCC5)O...</td>\n",
       "      <td>Nc1cc(C(F)(F)F)ccc1S</td>\n",
       "      <td>O=C(c1cccc(Cl)n1)[B-](F)(F)F.COc1ccc(CCOC(=O)N...</td>\n",
       "      <td>F[B-](F)(F)[C:1](=[O:2])[c:15]1[cH:16][cH:18][...</td>\n",
       "      <td>56106</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.014212</td>\n",
       "      <td>2.166420</td>\n",
       "      <td>1.013596</td>\n",
       "      <td>0.537785</td>\n",
       "      <td>0.05686</td>\n",
       "      <td>no_product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2-Pyr003</td>\n",
       "      <td>Fused002</td>\n",
       "      <td>TerABT014</td>\n",
       "      <td>COc1ccc(CCOC(=O)N2C[C@H](NC(=O)c3cccc(Cl)n3)[C...</td>\n",
       "      <td>O=C(c1cccc(Cl)n1)[B-](F)(F)F.[K+]</td>\n",
       "      <td>COc1ccc(CCOC(=O)N2C[C@@H]3NO[C@]4(OC5(CCCCC5)O...</td>\n",
       "      <td>Nc1ccc(Cl)cc1S</td>\n",
       "      <td>O=C(c1cccc(Cl)n1)[B-](F)(F)F.COc1ccc(CCOC(=O)N...</td>\n",
       "      <td>F[B-](F)(F)[C:1](=[O:2])[c:15]1[cH:16][cH:18][...</td>\n",
       "      <td>56112</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.028915</td>\n",
       "      <td>0.005039</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.015578</td>\n",
       "      <td>0.504057</td>\n",
       "      <td>0.992614</td>\n",
       "      <td>0.890646</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2-Pyr003</td>\n",
       "      <td>Fused002</td>\n",
       "      <td>TerTH001</td>\n",
       "      <td>COc1ccc(CCOC(=O)N2C[C@H](NC(=O)c3cccc(Cl)n3)[C...</td>\n",
       "      <td>O=C(c1cccc(Cl)n1)[B-](F)(F)F.[K+]</td>\n",
       "      <td>COc1ccc(CCOC(=O)N2C[C@@H]3NO[C@]4(OC5(CCCCC5)O...</td>\n",
       "      <td>[Cl-].[NH3+]NC(=S)c1ccccc1</td>\n",
       "      <td>O=C(c1cccc(Cl)n1)[B-](F)(F)F.COc1ccc(CCOC(=O)N...</td>\n",
       "      <td>F[B-](F)(F)[C:1](=[O:2])[c:11]1[cH:12][cH:14][...</td>\n",
       "      <td>56109</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.350061</td>\n",
       "      <td>0.643219</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.031689</td>\n",
       "      <td>0.613596</td>\n",
       "      <td>0.109309</td>\n",
       "      <td>0.439018</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     I_long    M_long     T_long  \\\n",
       "0  2-Pyr003  Fused002  TerABT004   \n",
       "1  2-Pyr003  Fused002  TerABT007   \n",
       "2  2-Pyr003  Fused002  TerABT013   \n",
       "3  2-Pyr003  Fused002  TerABT014   \n",
       "4  2-Pyr003  Fused002   TerTH001   \n",
       "\n",
       "                                    product_A_smiles  \\\n",
       "0  COc1ccc(CCOC(=O)N2C[C@H](NC(=O)c3cccc(Cl)n3)[C...   \n",
       "1  COc1ccc(CCOC(=O)N2C[C@H](NC(=O)c3cccc(Cl)n3)[C...   \n",
       "2  COc1ccc(CCOC(=O)N2C[C@H](NC(=O)c3cccc(Cl)n3)[C...   \n",
       "3  COc1ccc(CCOC(=O)N2C[C@H](NC(=O)c3cccc(Cl)n3)[C...   \n",
       "4  COc1ccc(CCOC(=O)N2C[C@H](NC(=O)c3cccc(Cl)n3)[C...   \n",
       "\n",
       "                            I_smiles  \\\n",
       "0  O=C(c1cccc(Cl)n1)[B-](F)(F)F.[K+]   \n",
       "1  O=C(c1cccc(Cl)n1)[B-](F)(F)F.[K+]   \n",
       "2  O=C(c1cccc(Cl)n1)[B-](F)(F)F.[K+]   \n",
       "3  O=C(c1cccc(Cl)n1)[B-](F)(F)F.[K+]   \n",
       "4  O=C(c1cccc(Cl)n1)[B-](F)(F)F.[K+]   \n",
       "\n",
       "                                            M_smiles  \\\n",
       "0  COc1ccc(CCOC(=O)N2C[C@@H]3NO[C@]4(OC5(CCCCC5)O...   \n",
       "1  COc1ccc(CCOC(=O)N2C[C@@H]3NO[C@]4(OC5(CCCCC5)O...   \n",
       "2  COc1ccc(CCOC(=O)N2C[C@@H]3NO[C@]4(OC5(CCCCC5)O...   \n",
       "3  COc1ccc(CCOC(=O)N2C[C@@H]3NO[C@]4(OC5(CCCCC5)O...   \n",
       "4  COc1ccc(CCOC(=O)N2C[C@@H]3NO[C@]4(OC5(CCCCC5)O...   \n",
       "\n",
       "                     T_smiles  \\\n",
       "0               Nc1ccc(F)cc1S   \n",
       "1              Nc1cc(Br)ccc1S   \n",
       "2        Nc1cc(C(F)(F)F)ccc1S   \n",
       "3              Nc1ccc(Cl)cc1S   \n",
       "4  [Cl-].[NH3+]NC(=S)c1ccccc1   \n",
       "\n",
       "                                     reaction_smiles  \\\n",
       "0  O=C(c1cccc(Cl)n1)[B-](F)(F)F.COc1ccc(CCOC(=O)N...   \n",
       "1  O=C(c1cccc(Cl)n1)[B-](F)(F)F.COc1ccc(CCOC(=O)N...   \n",
       "2  O=C(c1cccc(Cl)n1)[B-](F)(F)F.COc1ccc(CCOC(=O)N...   \n",
       "3  O=C(c1cccc(Cl)n1)[B-](F)(F)F.COc1ccc(CCOC(=O)N...   \n",
       "4  O=C(c1cccc(Cl)n1)[B-](F)(F)F.COc1ccc(CCOC(=O)N...   \n",
       "\n",
       "                         reaction_smiles_atom_mapped experiment_id  ...  \\\n",
       "0  F[B-](F)(F)[C:1](=[O:2])[c:15]1[cH:16][cH:18][...         56113  ...   \n",
       "1  F[B-](F)(F)[C:1](=[O:2])[c:15]1[cH:16][cH:18][...         56114  ...   \n",
       "2  F[B-](F)(F)[C:1](=[O:2])[c:15]1[cH:16][cH:18][...         56106  ...   \n",
       "3  F[B-](F)(F)[C:1](=[O:2])[c:15]1[cH:16][cH:18][...         56112  ...   \n",
       "4  F[B-](F)(F)[C:1](=[O:2])[c:11]1[cH:12][cH:14][...         56109  ...   \n",
       "\n",
       "   binary_H  scaled_A  scaled_B  scaled_C  scaled_D  scaled_E  scaled_F  \\\n",
       "0         0  0.036021  0.003427       0.0  0.020975  0.002958  0.941981   \n",
       "1         0  0.000000  0.000000       0.0  0.006159  0.364398  0.928851   \n",
       "2         1  0.000000  0.000000       0.0  0.014212  2.166420  1.013596   \n",
       "3         0  0.028915  0.005039       0.0  0.015578  0.504057  0.992614   \n",
       "4         0  0.350061  0.643219       0.0  0.031689  0.613596  0.109309   \n",
       "\n",
       "   scaled_G  scaled_H   major_A-C  \n",
       "0  0.914281   0.00000           A  \n",
       "1  1.106548   0.00000  no_product  \n",
       "2  0.537785   0.05686  no_product  \n",
       "3  0.890646   0.00000           A  \n",
       "4  0.439018   0.00000           B  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# M_long_dia will be to sort diastereomers into the same group on group shuffle splits\n",
    "diastereomers = {\n",
    "    \"Mon001\": \"Mon087\",\n",
    "    \"Mon003\": \"Mon078\",\n",
    "    \"Mon011\": \"Mon088\",\n",
    "    \"Mon013\": \"Mon074\",\n",
    "    \"Mon014\": \"Mon090\",\n",
    "    \"Mon015\": \"Mon076\",\n",
    "    \"Mon016\": \"Mon096\",\n",
    "    \"Mon017\": \"Mon075\",\n",
    "    \"Mon019\": \"Mon091\",\n",
    "    \"Mon020\": \"Mon077\",\n",
    "    \"Mon080\": \"Mon010\",\n",
    "}\n",
    "df[\"M_long_dia\"] = df[\"M_long\"].replace(diastereomers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0D split (not needed, see 0D_80 in truncated splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0D split final-retrain\n",
    "To retrain the best model after selection, for the 0D split we only use one fold, split into training and validation set (used for early stopping of FFN training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(35537, 3949, 0)]\n",
      "[(array([29166, 20532, 10145]), array([3246, 2272, 1124]), array([0., 0., 0.]))]\n"
     ]
    }
   ],
   "source": [
    "splitter = ShuffleSplit(n_splits=1, test_size=0.1, random_state=42)\n",
    "\n",
    "indices = []\n",
    "sizes = []\n",
    "pos_class = []\n",
    "for idx_train, idx_val in splitter.split(df):\n",
    "    idx_test = []  # placeholder so we can use the write_indices_and_stats function, just delete fold0_test.csv later\n",
    "    # add to list\n",
    "    indices.append((idx_train, idx_val, idx_test))\n",
    "    sizes.append((len(idx_train), len(idx_val), len(idx_test)))\n",
    "    pos_class.append(\n",
    "        (np.sum(df[['binary_A', 'binary_B', 'binary_C']].loc[idx_train]).to_numpy(), \n",
    "         np.sum(df[['binary_A', 'binary_B', 'binary_C']].loc[idx_val]).to_numpy(), \n",
    "         np.sum(df[['binary_A', 'binary_B', 'binary_C']].loc[idx_test]).to_numpy(), \n",
    "\n",
    "        )\n",
    "    )\n",
    "    \n",
    "print(sizes)\n",
    "print(pos_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/julian/PycharmProjects/library-generation/notebooks/data_featurization/util.py:91: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  f\"Test samples binary_A has label 1: {count_pos[2][0]} ({count_pos[2][0]/size[2]:.1%})\\n\"\n",
      "/Users/julian/PycharmProjects/library-generation/notebooks/data_featurization/util.py:94: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  f\"Test samples binary_B has label 1: {count_pos[2][1]} ({count_pos[2][1]/size[2]:.1%})\\n\"\n",
      "/Users/julian/PycharmProjects/library-generation/notebooks/data_featurization/util.py:97: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  f\"Test samples binary_C has label 1: {count_pos[2][2]} ({count_pos[2][2]/size[2]:.1%})\\n\"\n",
      "/Users/julian/PycharmProjects/library-generation/notebooks/data_featurization/util.py:104: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  f\"Chance level average precision macro on test set: {np.sum(count_pos[2]) / (3 * size[2]):.3f}\\n\"\n",
      "/Users/julian/PycharmProjects/library-generation/notebooks/data_featurization/util.py:166: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  f\"Mean Test samples binary_A has label 1: {sum_pos_class[2][0] / n_folds:.0f} ({sum_pos_class[2][0]/sum_sizes[2]:.1%})\\n\"\n",
      "/Users/julian/PycharmProjects/library-generation/notebooks/data_featurization/util.py:169: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  f\"Mean Test samples binary_B has label 1: {sum_pos_class[2][1] / n_folds:.0f} ({sum_pos_class[2][1]/sum_sizes[2]:.1%})\\n\"\n",
      "/Users/julian/PycharmProjects/library-generation/notebooks/data_featurization/util.py:172: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  f\"Mean Test samples binary_C has label 1: {sum_pos_class[2][2] / n_folds:.0f} ({sum_pos_class[2][2]/sum_sizes[2]:.1%})\\n\"\n",
      "/Users/julian/PycharmProjects/library-generation/notebooks/data_featurization/util.py:179: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  f\"Mean chance level average precision macro on test set: {np.sum(sum_pos_class[2]) / (3 * sum_sizes[2]):.3f}\\n\"\n"
     ]
    }
   ],
   "source": [
    "write_indices_and_stats(\n",
    "    indices, \n",
    "    sizes, \n",
    "    pos_class,\n",
    "    total_size=len(df),\n",
    "    data_name=data_name,\n",
    "    split_dimension=0, \n",
    "    save_indices=True, \n",
    "    train_size=\"final_retrain\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1D split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T17:04:54.453760Z",
     "start_time": "2023-09-07T17:04:54.450483Z"
    }
   },
   "outputs": [],
   "source": [
    "splitter = GroupShuffleSplit(n_splits=3, test_size=0.15, random_state=np.random.RandomState(42))  # here, we reuse the outer splitter as well, so we use RandomState\n",
    "inner_splitter = GroupShuffleSplit(n_splits=1, test_size=0.15/0.85, random_state=np.random.RandomState(42))  # we use a RandomState instance, not an int, because we will reuse this splitter several times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T17:04:54.633295Z",
     "start_time": "2023-09-07T17:04:54.457693Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(27196, 5672, 6618), (25001, 8543, 5942), (28990, 5536, 4960), (25495, 9037, 4954), (27626, 6433, 5427), (26904, 6701, 5881), (28292, 5161, 6033), (27420, 5605, 6461), (28020, 5449, 6017)]\n",
      "[(array([22402, 15429,  7746]), array([4277, 3422, 1582]), array([5733, 3953, 1941])), (array([20347, 14227,  6881]), array([7424, 5302, 2781]), array([4641, 3275, 1607])), (array([24239, 16853,  8631]), array([4214, 3305, 1456]), array([3959, 2646, 1182])), (array([20427, 14342,  6415]), array([8317, 5985, 3464]), array([3668, 2477, 1390])), (array([22505, 15898,  7994]), array([5115, 3473, 1511]), array([4792, 3433, 1764])), (array([22187, 15431,  7698]), array([5309, 3951, 1930]), array([4916, 3422, 1641])), (array([22839, 16777,  8612]), array([4477, 2222, 1088]), array([5096, 3805, 1569])), (array([23560, 18246,  9601]), array([3992, 1341,  324]), array([4860, 3217, 1344])), (array([22754, 16454,  7849]), array([4982, 3519, 2172]), array([4676, 2831, 1248]))]\n"
     ]
    }
   ],
   "source": [
    "indices = []\n",
    "sizes = []\n",
    "pos_class = []\n",
    "for idx_train_val, idx_test in splitter.split(list(range(len(df))), groups=df[\"I_long\"]):\n",
    "    train, val = next(inner_splitter.split(idx_train_val, groups=df[\"I_long\"][idx_train_val]))\n",
    "    # use indices to index indices :P (we need to obtain indices referring to the original dataframe)\n",
    "    idx_train = idx_train_val[train]\n",
    "    idx_val = idx_train_val[val]\n",
    "    indices.append((idx_train, idx_val, idx_test))\n",
    "    sizes.append((len(idx_train), len(idx_val), len(idx_test)))\n",
    "    pos_class.append(\n",
    "        (np.sum(df[['binary_A', 'binary_B', 'binary_C']].loc[idx_train]).to_numpy(), \n",
    "         np.sum(df[['binary_A', 'binary_B', 'binary_C']].loc[idx_val]).to_numpy(), \n",
    "         np.sum(df[['binary_A', 'binary_B', 'binary_C']].loc[idx_test]).to_numpy(),\n",
    "        )\n",
    "    )\n",
    "\n",
    "# note: for M_long, we need to obtain diastereomer relationships to sort them into the same group\n",
    "for idx_train_val, idx_test in splitter.split(list(range(len(df))), groups=df[\"M_long_dia\"]):\n",
    "    train, val = next(inner_splitter.split(idx_train_val, groups=df[\"M_long_dia\"][idx_train_val]))\n",
    "    # use indices to index indices :P (we need to obtain indices referring to the original dataframe)\n",
    "    idx_train = idx_train_val[train]\n",
    "    idx_val = idx_train_val[val]\n",
    "    indices.append((idx_train, idx_val, idx_test))\n",
    "    sizes.append((len(idx_train), len(idx_val), len(idx_test)))\n",
    "    pos_class.append(\n",
    "        (np.sum(df[['binary_A', 'binary_B', 'binary_C']].loc[idx_train]).to_numpy(), \n",
    "         np.sum(df[['binary_A', 'binary_B', 'binary_C']].loc[idx_val]).to_numpy(), \n",
    "         np.sum(df[['binary_A', 'binary_B', 'binary_C']].loc[idx_test]).to_numpy(),\n",
    "        )\n",
    "    )\n",
    "        \n",
    "for idx_train_val, idx_test in splitter.split(list(range(len(df))), groups=df[\"T_long\"]):\n",
    "    train, val = next(inner_splitter.split(idx_train_val, groups=df[\"T_long\"][idx_train_val]))\n",
    "    # use indices to index indices :P (we need to obtain indices referring to the original dataframe)\n",
    "    idx_train = idx_train_val[train]\n",
    "    idx_val = idx_train_val[val]\n",
    "    indices.append((idx_train, idx_val, idx_test))\n",
    "    sizes.append((len(idx_train), len(idx_val), len(idx_test)))\n",
    "    pos_class.append(\n",
    "        (np.sum(df[['binary_A', 'binary_B', 'binary_C']].loc[idx_train]).to_numpy(), \n",
    "         np.sum(df[['binary_A', 'binary_B', 'binary_C']].loc[idx_val]).to_numpy(), \n",
    "         np.sum(df[['binary_A', 'binary_B', 'binary_C']].loc[idx_test]).to_numpy(),\n",
    "        )\n",
    "    )\n",
    "    \n",
    "print(sizes)\n",
    "print(pos_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_indices_and_stats(\n",
    "    indices, \n",
    "    sizes, \n",
    "    pos_class,\n",
    "    total_size=len(df),\n",
    "    data_name=data_name,\n",
    "    split_dimension=1, \n",
    "    save_indices=True, \n",
    "    train_size=\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2D split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T17:04:54.762079Z",
     "start_time": "2023-09-07T17:04:54.758896Z"
    }
   },
   "outputs": [],
   "source": [
    "splitter = GroupShuffleSplitND(n_splits=3, test_size=0.2, random_state=np.random.RandomState(42))  # here, we reuse the outer splitter as well, so we use RandomState\n",
    "inner_splitter = GroupShuffleSplitND(n_splits=1, test_size=0.2/0.8, random_state=np.random.RandomState(42))  # we use a RandomState instance, not an int, because we will reuse this splitter several times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T17:04:55.197117Z",
     "start_time": "2023-09-07T17:04:54.777330Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(13206, 1738, 1756), (13087, 1922, 1658), (14890, 1415, 1615), (13375, 1546, 1891), (14278, 1574, 1487), (13462, 1461, 1916), (14138, 1398, 1957), (14268, 1619, 1624), (12864, 1791, 1736)]\n",
      "[(array([10818,  7819,  3570]), array([1404,  959,  516]), array([1505,  998,  536])), (array([10668,  7626,  3429]), array([1683, 1166,  692]), array([1299,  892,  449])), (array([12164,  8560,  4204]), array([1218,  877,  539]), array([1239,  849,  283])), (array([11046,  7905,  3957]), array([1322, 1163,  391]), array([1371,  740,  459])), (array([12408,  9551,  4882]), array([1058,  800,  308]), array([1174,  594,  289])), (array([11314,  7766,  4157]), array([932, 583, 195]), array([1737, 1420,  719])), (array([11569,  7415,  3662]), array([1182,  825,  462]), array([1527, 1229,  535])), (array([11423,  7291,  3659]), array([1426, 1183,  572]), array([1321, 1089,  502])), (array([9914, 5868, 2664]), array([1573, 1305,  799]), array([1505, 1314,  623]))]\n"
     ]
    }
   ],
   "source": [
    "indices = []\n",
    "sizes = []\n",
    "pos_class = []\n",
    "for idx_train_val, idx_test in splitter.split(df, groups=df[[\"I_long\", \"M_long_dia\"]]):\n",
    "    train, val = next(inner_splitter.split(df.iloc[idx_train_val], groups=df[[\"I_long\", \"M_long_dia\"]].iloc[idx_train_val]))\n",
    "    # use indices to index indices :P (we need to obtain indices referring to the original dataframe)\n",
    "    idx_train = idx_train_val[train]\n",
    "    idx_val = idx_train_val[val]\n",
    "    indices.append((idx_train, idx_val, idx_test))\n",
    "    sizes.append((len(idx_train), len(idx_val), len(idx_test)))\n",
    "    pos_class.append(\n",
    "        (np.sum(df[['binary_A', 'binary_B', 'binary_C']].loc[idx_train]).to_numpy(), \n",
    "         np.sum(df[['binary_A', 'binary_B', 'binary_C']].loc[idx_val]).to_numpy(), \n",
    "         np.sum(df[['binary_A', 'binary_B', 'binary_C']].loc[idx_test]).to_numpy(),\n",
    "        )\n",
    "    )\n",
    "    \n",
    "for idx_train_val, idx_test in splitter.split(list(range(len(df))), groups=df[[\"M_long_dia\", \"T_long\"]]):\n",
    "    train, val = next(inner_splitter.split(idx_train_val, groups=df[[\"M_long_dia\", \"T_long\"]].iloc[idx_train_val]))\n",
    "    # use indices to index indices :P (we need to obtain indices referring to the original dataframe)\n",
    "    idx_train = idx_train_val[train]\n",
    "    idx_val = idx_train_val[val]\n",
    "    indices.append((idx_train, idx_val, idx_test))\n",
    "    sizes.append((len(idx_train), len(idx_val), len(idx_test)))\n",
    "    pos_class.append(\n",
    "        (np.sum(df[['binary_A', 'binary_B', 'binary_C']].loc[idx_train]).to_numpy(), \n",
    "         np.sum(df[['binary_A', 'binary_B', 'binary_C']].loc[idx_val]).to_numpy(), \n",
    "         np.sum(df[['binary_A', 'binary_B', 'binary_C']].loc[idx_test]).to_numpy(),\n",
    "        )\n",
    "    )\n",
    "    \n",
    "for idx_train_val, idx_test in splitter.split(list(range(len(df))), groups=df[[\"I_long\", \"T_long\"]]):\n",
    "    train, val = next(inner_splitter.split(idx_train_val, groups=df[[\"I_long\", \"T_long\"]].iloc[idx_train_val]))\n",
    "    # use indices to index indices :P (we need to obtain indices referring to the original dataframe)\n",
    "    idx_train = idx_train_val[train]\n",
    "    idx_val = idx_train_val[val]\n",
    "    indices.append((idx_train, idx_val, idx_test))\n",
    "    sizes.append((len(idx_train), len(idx_val), len(idx_test)))\n",
    "    pos_class.append(\n",
    "        (np.sum(df[['binary_A', 'binary_B', 'binary_C']].loc[idx_train]).to_numpy(), \n",
    "         np.sum(df[['binary_A', 'binary_B', 'binary_C']].loc[idx_val]).to_numpy(), \n",
    "         np.sum(df[['binary_A', 'binary_B', 'binary_C']].loc[idx_test]).to_numpy(),\n",
    "        )\n",
    "    )\n",
    "        \n",
    "print(sizes)\n",
    "print(pos_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_indices_and_stats(\n",
    "    indices, \n",
    "    sizes, \n",
    "    pos_class,\n",
    "    total_size=len(df),\n",
    "    data_name=data_name,\n",
    "    split_dimension=2, \n",
    "    save_indices=True, \n",
    "    train_size=\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3D split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T17:04:55.275691Z",
     "start_time": "2023-09-07T17:04:55.270964Z"
    }
   },
   "outputs": [],
   "source": [
    "splitter = GroupShuffleSplitND(n_splits=9, test_size=0.25, random_state=np.random.RandomState(42))  # here, we reuse the outer splitter as well, so we use RandomState (not true, copyPaste error from before. Anyway, not a problem)\n",
    "inner_splitter = GroupShuffleSplitND(n_splits=1, test_size=0.25/0.75, random_state=np.random.RandomState(42))  # we use a RandomState instance, not an int, because we will reuse this splitter several times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T17:04:55.662319Z",
     "start_time": "2023-09-07T17:04:55.277028Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(4481, 613, 834), (4384, 672, 688), (4506, 580, 714), (3999, 554, 735), (5221, 590, 539), (5498, 554, 502), (4626, 672, 515), (5028, 584, 563), (4225, 638, 736)]\n",
      "[(array([3608, 2827, 1349]), array([517, 319, 180]), array([697, 491, 201])), (array([3674, 2726, 1073]), array([548, 266, 177]), array([553, 435, 230])), (array([3458, 2533, 1416]), array([509, 348, 149]), array([589, 373, 160])), (array([3470, 2729, 1498]), array([408, 148,  56]), array([581, 475, 213])), (array([4275, 3253, 1292]), array([474, 321, 248]), array([445, 268, 124])), (array([4746, 2687, 1420]), array([486, 302, 116]), array([344, 334, 127])), (array([3870, 3192, 1655]), array([584, 394, 132]), array([359, 184,  66])), (array([4388, 2779, 1668]), array([449, 386, 182]), array([443, 298, 113])), (array([3440, 2135,  934]), array([567, 394, 307]), array([598, 498, 191]))]\n"
     ]
    }
   ],
   "source": [
    "indices = []\n",
    "sizes = []\n",
    "pos_class = []\n",
    "for idx_train_val, idx_test in splitter.split(df, groups=df[[\"I_long\", \"M_long_dia\", \"T_long\"]]):\n",
    "    train, val = next(inner_splitter.split(df.iloc[idx_train_val], groups=df[[\"I_long\", \"M_long_dia\", \"T_long\"]].iloc[idx_train_val]))\n",
    "    # use indices to index indices :P (we need to obtain indices referring to the original dataframe)\n",
    "    idx_train = idx_train_val[train]\n",
    "    idx_val = idx_train_val[val]\n",
    "    indices.append((idx_train, idx_val, idx_test))\n",
    "    sizes.append((len(idx_train), len(idx_val), len(idx_test)))\n",
    "    pos_class.append(\n",
    "        (np.sum(df[['binary_A', 'binary_B', 'binary_C']].loc[idx_train]).to_numpy(), \n",
    "         np.sum(df[['binary_A', 'binary_B', 'binary_C']].loc[idx_val]).to_numpy(), \n",
    "         np.sum(df[['binary_A', 'binary_B', 'binary_C']].loc[idx_test]).to_numpy(),\n",
    "        )\n",
    "    )\n",
    "    \n",
    "print(sizes)\n",
    "print(pos_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_indices_and_stats(\n",
    "    indices, \n",
    "    sizes, \n",
    "    pos_class,\n",
    "    total_size=len(df),\n",
    "    data_name=data_name,\n",
    "    split_dimension=3, \n",
    "    save_indices=True, \n",
    "    train_size=\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Control: Check splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1D split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T17:04:56.477788Z",
     "start_time": "2023-09-07T17:04:56.188304Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "split_dimension = 1\n",
    "split_dir = DATA_DIR / \"curated_data\" / \"splits\" / f\"{data_name}_{split_dimension}D_split\"\n",
    "    \n",
    "for fold_idx in range(9): # only these are split on monomers\n",
    "    \n",
    "    # import indices\n",
    "    train_idx = pd.read_csv(split_dir / f\"fold{fold_idx}_train.csv\")[\"index\"].to_numpy()\n",
    "    val_idx = pd.read_csv(split_dir / f\"fold{fold_idx}_val.csv\")[\"index\"].to_numpy()\n",
    "    test_idx = pd.read_csv(split_dir / f\"fold{fold_idx}_test.csv\")[\"index\"].to_numpy()\n",
    "\n",
    "    # check mutually exclusive\n",
    "    assert len(np.intersect1d(train_idx, val_idx)) == 0\n",
    "    assert len(np.intersect1d(train_idx, test_idx)) == 0\n",
    "    assert len(np.intersect1d(val_idx, test_idx)) == 0\n",
    "\n",
    "    # check 1D groups are mutually exclusive\n",
    "    if fold_idx < 3: # first three are split on initiator\n",
    "        assert len(np.intersect1d(df[\"I_long\"].iloc[train_idx], df[\"I_long\"].iloc[val_idx])) == 0\n",
    "        assert len(np.intersect1d(df[\"I_long\"].iloc[train_idx], df[\"I_long\"].iloc[test_idx])) == 0\n",
    "        assert len(np.intersect1d(df[\"I_long\"].iloc[val_idx], df[\"I_long\"].iloc[test_idx])) == 0\n",
    "    elif fold_idx < 6:  # next three are split on monomer\n",
    "        assert len(np.intersect1d(df[\"M_long\"].iloc[train_idx], df[\"M_long\"].iloc[val_idx])) == 0\n",
    "        assert len(np.intersect1d(df[\"M_long\"].iloc[train_idx], df[\"M_long\"].iloc[test_idx])) == 0\n",
    "        assert len(np.intersect1d(df[\"M_long\"].iloc[val_idx], df[\"M_long\"].iloc[test_idx])) == 0\n",
    "        assert len(np.intersect1d(df[\"M_long_dia\"].iloc[train_idx], df[\"M_long_dia\"].iloc[val_idx])) == 0\n",
    "        assert len(np.intersect1d(df[\"M_long_dia\"].iloc[train_idx], df[\"M_long_dia\"].iloc[test_idx])) == 0\n",
    "        assert len(np.intersect1d(df[\"M_long_dia\"].iloc[val_idx], df[\"M_long_dia\"].iloc[test_idx])) == 0\n",
    "        \n",
    "    else:  # last three are split on terminator\n",
    "        assert len(np.intersect1d(df[\"T_long\"].iloc[train_idx], df[\"T_long\"].iloc[val_idx])) == 0\n",
    "        assert len(np.intersect1d(df[\"T_long\"].iloc[train_idx], df[\"T_long\"].iloc[test_idx])) == 0\n",
    "        assert len(np.intersect1d(df[\"T_long\"].iloc[val_idx], df[\"T_long\"].iloc[test_idx])) == 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2D split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T17:04:56.993523Z",
     "start_time": "2023-09-07T17:04:56.483643Z"
    }
   },
   "outputs": [],
   "source": [
    "split_dimension = 2\n",
    "split_dir = DATA_DIR / \"curated_data\" / \"splits\" / f\"{data_name}_{split_dimension}D_split\"\n",
    "    \n",
    "for fold_idx in range(9):\n",
    "    # import indices\n",
    "    train_idx = pd.read_csv(split_dir / f\"fold{fold_idx}_train.csv\")[\"index\"].to_numpy()\n",
    "    val_idx = pd.read_csv(split_dir / f\"fold{fold_idx}_val.csv\")[\"index\"].to_numpy()\n",
    "    test_idx = pd.read_csv(split_dir / f\"fold{fold_idx}_test.csv\")[\"index\"].to_numpy()\n",
    "\n",
    "    # check mutually exclusive\n",
    "    assert len(np.intersect1d(train_idx, val_idx)) == 0\n",
    "    assert len(np.intersect1d(train_idx, test_idx)) == 0\n",
    "    assert len(np.intersect1d(val_idx, test_idx)) == 0\n",
    "\n",
    "    # check 2D groups are mutually exclusive\n",
    "    if fold_idx < 3: # first three are split on initiator and monomer\n",
    "        assert len(np.intersect1d(np.unique(df[[\"I_long\", \"M_long_dia\"]].iloc[train_idx]), np.unique(df[[\"I_long\", \"M_long_dia\"]].iloc[val_idx]))) == 0\n",
    "        assert len(np.intersect1d(np.unique(df[[\"I_long\", \"M_long_dia\"]].iloc[train_idx]), np.unique(df[[\"I_long\", \"M_long_dia\"]].iloc[test_idx]))) == 0\n",
    "        assert len(np.intersect1d(np.unique(df[[\"I_long\", \"M_long_dia\"]].iloc[val_idx]), np.unique(df[[\"I_long\", \"M_long_dia\"]].iloc[test_idx]))) == 0\n",
    "    elif fold_idx < 6:  # next three are split on monomer and terminator\n",
    "        assert len(np.intersect1d(np.unique(df[[\"M_long\", \"T_long\"]].iloc[train_idx]), np.unique(df[[\"M_long\", \"T_long\"]].iloc[val_idx]))) == 0\n",
    "        assert len(np.intersect1d(np.unique(df[[\"M_long\", \"T_long\"]].iloc[train_idx]), np.unique(df[[\"M_long\", \"T_long\"]].iloc[test_idx]))) == 0\n",
    "        assert len(np.intersect1d(np.unique(df[[\"M_long\", \"T_long\"]].iloc[val_idx]), np.unique(df[[\"M_long\", \"T_long\"]].iloc[test_idx]))) == 0\n",
    "        assert len(np.intersect1d(np.unique(df[[\"M_long_dia\", \"T_long\"]].iloc[train_idx]), np.unique(df[[\"M_long_dia\", \"T_long\"]].iloc[val_idx]))) == 0\n",
    "        assert len(np.intersect1d(np.unique(df[[\"M_long_dia\", \"T_long\"]].iloc[train_idx]), np.unique(df[[\"M_long_dia\", \"T_long\"]].iloc[test_idx]))) == 0\n",
    "        assert len(np.intersect1d(np.unique(df[[\"M_long_dia\", \"T_long\"]].iloc[val_idx]), np.unique(df[[\"M_long_dia\", \"T_long\"]].iloc[test_idx]))) == 0\n",
    "    else:  # last three are split on initiator and terminator\n",
    "        assert len(np.intersect1d(np.unique(df[[\"I_long\", \"T_long\"]].iloc[train_idx]), np.unique(df[[\"I_long\", \"T_long\"]].iloc[val_idx]))) == 0\n",
    "        assert len(np.intersect1d(np.unique(df[[\"I_long\", \"T_long\"]].iloc[train_idx]), np.unique(df[[\"I_long\", \"T_long\"]].iloc[test_idx]))) == 0\n",
    "        assert len(np.intersect1d(np.unique(df[[\"I_long\", \"T_long\"]].iloc[val_idx]), np.unique(df[[\"I_long\", \"T_long\"]].iloc[test_idx]))) == 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3D split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T17:04:57.240116Z",
     "start_time": "2023-09-07T17:04:56.997579Z"
    }
   },
   "outputs": [],
   "source": [
    "split_dimension = 3\n",
    "split_dir = DATA_DIR / \"curated_data\" / \"splits\" / f\"{data_name}_{split_dimension}D_split\"\n",
    "    \n",
    "for fold_idx in range(9):\n",
    "    # import indices\n",
    "    train_idx = pd.read_csv(split_dir / f\"fold{fold_idx}_train.csv\")[\"index\"].to_numpy()\n",
    "    val_idx = pd.read_csv(split_dir / f\"fold{fold_idx}_val.csv\")[\"index\"].to_numpy()\n",
    "    test_idx = pd.read_csv(split_dir / f\"fold{fold_idx}_test.csv\")[\"index\"].to_numpy()\n",
    "\n",
    "    # check mutually exclusive\n",
    "    assert len(np.intersect1d(train_idx, val_idx)) == 0\n",
    "    assert len(np.intersect1d(train_idx, test_idx)) == 0\n",
    "    assert len(np.intersect1d(val_idx, test_idx)) == 0\n",
    "\n",
    "    # check 3D groups are mutually exclusive\n",
    "    assert len(np.intersect1d(np.unique(df[[\"I_long\", \"M_long\", \"T_long\"]].iloc[train_idx]), np.unique(df[[\"I_long\", \"M_long\", \"T_long\"]].iloc[val_idx]))) == 0\n",
    "    assert len(np.intersect1d(np.unique(df[[\"I_long\", \"M_long\", \"T_long\"]].iloc[train_idx]), np.unique(df[[\"I_long\", \"M_long\", \"T_long\"]].iloc[test_idx]))) == 0\n",
    "    assert len(np.intersect1d(np.unique(df[[\"I_long\", \"M_long\", \"T_long\"]].iloc[val_idx]), np.unique(df[[\"I_long\", \"M_long\", \"T_long\"]].iloc[test_idx]))) == 0\n",
    "    \n",
    "    assert len(np.intersect1d(np.unique(df[[\"I_long\", \"M_long_dia\", \"T_long\"]].iloc[train_idx]), np.unique(df[[\"I_long\", \"M_long_dia\", \"T_long\"]].iloc[val_idx]))) == 0\n",
    "    assert len(np.intersect1d(np.unique(df[[\"I_long\", \"M_long_dia\", \"T_long\"]].iloc[train_idx]), np.unique(df[[\"I_long\", \"M_long_dia\", \"T_long\"]].iloc[test_idx]))) == 0\n",
    "    assert len(np.intersect1d(np.unique(df[[\"I_long\", \"M_long_dia\", \"T_long\"]].iloc[val_idx]), np.unique(df[[\"I_long\", \"M_long_dia\", \"T_long\"]].iloc[test_idx]))) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
