{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SynFerm Exploratory Data Analysis\n",
    "#### Targets:\n",
    "- See some of the basic trends in the data (which BBs react well? which not at all?)\n",
    "- perform basic statistics (How many successful reactions?)\n",
    "\n",
    "#### Sections:\n",
    "1. [Load and filter data](#1)\n",
    "2. [Investigate \"invalid\" reactions](#2)\n",
    "3. [General Statistics](#3)\n",
    "4. [Building Block Statistics](#4)\n",
    "5. [Trends within building block classes](#5)\n",
    "6. [Scaling the LC-MS responses](#6)\n",
    "7. [Outcome for the main product](#7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import re\n",
    "import sys\n",
    "from typing import Tuple, Union\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import rc\n",
    "from matplotlib.collections import PatchCollection\n",
    "from matplotlib.patches import Rectangle\n",
    "from rdkit.Chem.AllChem import Compute2DCoords\n",
    "from scipy.stats import gaussian_kde\n",
    "from sklearn.metrics import r2_score, accuracy_score, balanced_accuracy_score, confusion_matrix, recall_score\n",
    "%matplotlib inline\n",
    "\n",
    "sys.path.append(str(pathlib.Path().resolve().parents[1]))\n",
    "from src.util.db_utils import SynFermDatabaseConnection\n",
    "from src.util.color_palette import BodeColorPalette\n",
    "from src.util.pandas_utils import alphanumeric_index_sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings\n",
    "bode_palette = BodeColorPalette()\n",
    "rc('font',**{'family':'sans-serif','sans-serif':['Helvetica']})\n",
    "plt.rcParams['figure.dpi'] = 100  # this makes the figures bigger in jupyter nb\n",
    "\n",
    "sns.set_theme(context=\"paper\",\n",
    "              style=\"white\",\n",
    "              font_scale=0.7,\n",
    "              rc={\"savefig.transparent\": True,\n",
    "                  \"axes.grid\": False,\n",
    "                  \"axes.spines.bottom\": True,\n",
    "                  \"axes.spines.left\": False,\n",
    "                  \"axes.spines.right\": False,\n",
    "                  \"axes.spines.top\": False,\n",
    "                  \"axes.labelweight\": \"bold\",\n",
    "                  },\n",
    "             )\n",
    "SAVE_DIR = pathlib.Path(\"/Users/julian/Desktop/SF_exploratory_data_analysis/\")\n",
    "SAVE_DIR.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and filter data <a id=1></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = SynFermDatabaseConnection()  # we will use this for various simple queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = db.get_experiments_table_as_df()\n",
    "print(f'Number of reactions (in total): {len(df_full)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select experiments for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show all available experiments with reaction counts\n",
    "df_full[\"exp_nr\"].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select experiment numbers for further analysis\n",
    "start_exp_nr = 4  # kick out invalid experiments (SOP changed after exp3)\n",
    "end_exp_nr = 29  # (inclusive)\n",
    "\n",
    "df_full = df_full.loc[df_full['exp_nr'].between(start_exp_nr, end_exp_nr)]  \n",
    "print(f'Number of reactions since SOP change and without test reactions: {len(df_full)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate \"invalid\" reactions <a id=2></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expand the \"valid\" column to work with it more easily\n",
    "# add columns for error and warning\n",
    "df_full[\"has_error\"] = df_full[\"valid\"].notna() & df_full[\"valid\"].str.contains(\"ERROR\")\n",
    "df_full[\"has_warning\"] = df_full[\"valid\"].notna() & df_full[\"valid\"].str.contains(\"WARNING\")\n",
    "# add columns for error categories\n",
    "df_full[\"has_error_product_peaks\"] = df_full[\"valid\"].notna() & df_full[\"valid\"].str.contains(\"ERROR: multiple peaks for product A\")\n",
    "df_full[\"has_error_internal_standard\"] = df_full[\"valid\"].notna() & (\n",
    "        df_full[\"valid\"].str.contains(\"ERROR: IS response <50% of plate median\")\n",
    "        | df_full[\"valid\"].str.contains(\"ERROR: multiple peaks for IS\")\n",
    "        | df_full[\"valid\"].str.contains(\"ERROR: IS response >200% of plate median\")\n",
    ")\n",
    "df_full[\"has_error_transfer\"] = df_full[\"valid\"].notna() & df_full[\"valid\"].str.contains(\"transfer error\")\n",
    "df_full[\"has_error_low_volume\"] = df_full[\"valid\"].notna() & df_full[\"valid\"].str.contains(\"ERROR: Dilution survey low volume\")\n",
    "df_full[\"has_error_other\"] = df_full[\"valid\"].notna() & (df_full[\"valid\"].str.contains(\"ERROR: No monomer\") | df_full[\"valid\"].str.contains(\"ERROR: Monomer solution had lower concentration than required\") )\n",
    "df_full[\"has_warning_product_peaks\"] = df_full[\"valid\"].notna() & df_full[\"valid\"].str.contains(\"WARNING: multiple peaks for product A\")\n",
    "df_full[\"has_warning_quality_control\"] = df_full[\"valid\"].notna() & df_full[\"valid\"].str.contains(\"WARNING\") & df_full[\"valid\"].str.contains(\"QC\")\n",
    "df_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full[[col for col in df_full.columns if col.startswith(\"has_\")]].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's split this into the individual errors/warnings\n",
    "expanded_warnings = df_full.loc[:,'valid'].str.split(\";\", expand=True)\n",
    "expanded_warnings.columns = [f\"problem_{i}\" for i in expanded_warnings.columns]\n",
    "df_full = df_full.join(expanded_warnings, how=\"left\")\n",
    "# sanitize\n",
    "df_full.replace(' ', np.nan, inplace=True)  # replace whitespace strings with NaN (artifact from splitting)\n",
    "df_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain IS areas from the database\n",
    "res = db.con.execute(\"SELECT exp_nr, plate_nr, lcms_areas FROM experiments INNER JOIN lcms l on experiments.id = l.synthesis_id;\").fetchall()\n",
    "df_lcms_areas = pd.DataFrame(res, columns=[\"exp_nr\", \"plate_nr\", \"lcms_areas\"])\n",
    "df_lcms_areas[\"IS_area\"] = df_lcms_areas[\"lcms_areas\"].apply(lambda x: float(x.strip(\"[]\").split(\", \")[-1]))\n",
    "df_lcms_areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = df_lcms_areas.groupby([\"exp_nr\", \"plate_nr\"])\n",
    "df_lcms_is_mean = grouped.mean(numeric_only=True)\n",
    "df_lcms_is_std = grouped.std(numeric_only=True)\n",
    "df_lcms_is = df_lcms_is_mean.join(df_lcms_is_std, lsuffix=\"_mean\", rsuffix=\"_std\")\n",
    "df_lcms_is = df_lcms_is.reset_index()\n",
    "df_lcms_is[\"plate\"] = df_lcms_is[\"exp_nr\"] * 7 + df_lcms_is[\"plate_nr\"]\n",
    "df_lcms_is = df_lcms_is.loc[df_lcms_is[\"exp_nr\"].between(4, 30)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7.5, 5))\n",
    "plt.bar(\n",
    "    x=df_lcms_is[\"plate\"],\n",
    "    height=df_lcms_is[\"IS_area_mean\"],\n",
    "    #yerr=df_lcms_is[\"IS_area_std\"],\n",
    "    width=1,\n",
    "\n",
    "    )\n",
    "plt.savefig(SAVE_DIR / \"IS_areas.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the standard deviations\n",
    "plt.figure(figsize=(7.5, 5))\n",
    "plt.bar(\n",
    "    x=df_lcms_is[\"plate\"],\n",
    "    height=df_lcms_is[\"IS_area_std\"],\n",
    "    #yerr=df_lcms_is[\"IS_area_std\"],\n",
    "    width=1,\n",
    "\n",
    "    )\n",
    "plt.savefig(SAVE_DIR / \"IS_areas_std.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion from IS response statistics\n",
    "There is no clear trend in the IS response. The standard deviation is generally quite high.\n",
    "I would have expected to see the SD increase with plate number within an experiment block, because that correlates with the age of the plate at measurement time, but we do not observe a trend like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many reactions have errors?\n",
    "df_full.has_error.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many reactions have warnings?\n",
    "df_full.has_warning.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what are the most common problems?\n",
    "df_full[[col for col in df_full.columns if col.startswith(\"problem_\")]].stack().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a histogram showing the distribution of errors across experiments\n",
    "plt.figure(figsize=(7.5, 5))\n",
    "sns.barplot(x=df_full.loc[df_full.has_error, \"exp_nr\"].value_counts().sort_index().index,\n",
    "            y=df_full.loc[df_full.has_error, \"exp_nr\"].value_counts().sort_index().values,\n",
    "            color=bode_palette.blue,\n",
    "            )\n",
    "plt.xlabel('Experiment number')\n",
    "plt.ylabel('Number of errors')\n",
    "plt.axvline(x=8.5, color=\"black\", linestyle='--', label='Changed to OT2 source preparation before exp13')\n",
    "plt.axvline(x=12.5, color=\"black\", linestyle='--', label='Changed to full automation before exp17')\n",
    "plt.axvline(x=16.5, color=\"black\", linestyle='--', label='Changed to Eppendorf heat seal before exp21')\n",
    "plt.axvline(x=20.5, color=\"black\", linestyle='--', label='Changed to Agilent heat seal before exp21')\n",
    "plt.savefig(SAVE_DIR / \"number_of_errors_per_experiment.png\", dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a histogram showing the distribution of errors across experiments\n",
    "# Function to remove the value in parentheses\n",
    "def remove_parentheses_value(s):\n",
    "    if isinstance(s, str):\n",
    "        return re.sub(r'\\s\\(\\d+\\)', '', s)\n",
    "    return s\n",
    "\n",
    "\n",
    "# Keep only the relevant columns\n",
    "relevant_columns = ['exp_nr'] + [col for col in df_full.columns if 'problem_' in col]\n",
    "df_relevant = df_full[relevant_columns].copy()\n",
    "\n",
    "for col in relevant_columns:\n",
    "    if col != 'exp_nr':\n",
    "        df_relevant[col] = df_relevant[col].apply(remove_parentheses_value)\n",
    "        df_relevant[col] = df_relevant[col].apply(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "\n",
    "# right now we are only interested in errors\n",
    "df_relevant_errors = df_relevant.replace(re.compile('.*WARNING.*'), pd.NA)\n",
    "\n",
    "# Melt the DataFrame\n",
    "df_melted = pd.melt(df_relevant_errors, id_vars=['exp_nr'], var_name='columns', value_name='value')\n",
    "\n",
    "# Filter out None values\n",
    "df_filtered = df_melted[df_melted['value'].notnull()]\n",
    "\n",
    "# Create the histogram\n",
    "plt.figure(figsize=(7.5, 5))\n",
    "ax = sns.histplot(data=df_filtered, x='exp_nr', hue='value', multiple='stack', discrete=True, shrink=.8)\n",
    "plt.axvline(x=12.5, color=bode_palette.orange, linestyle='--', label='Changed to OT2 source preparation before exp13')\n",
    "plt.axvline(x=16.5, color=bode_palette.orange, linestyle='--', label='Changed to full automation before exp17')\n",
    "plt.axvline(x=20.5, color=bode_palette.orange, linestyle='--', label='Changed to Eppendorf heat seal before exp21')\n",
    "plt.axvline(x=24.5, color=bode_palette.orange, linestyle='--', label='Changed to Agilent heat seal before exp25')\n",
    "plt.xlim(3.5, 29.5)\n",
    "plt.xlabel('Experiment number')\n",
    "plt.ylabel('Number of errors')\n",
    "# Move the legend outside the plot to the right\n",
    "sns.move_legend(ax, \"upper left\", bbox_to_anchor=(1, 1))\n",
    "plt.savefig(SAVE_DIR / \"type_of_errors_per_experiment.png\", dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a histogram showing the distribution of errors across experiments, but use the error bins instead of the individual errors\n",
    "\n",
    "# Keep only the relevant columns\n",
    "relevant_columns = ['exp_nr'] + [col for col in df_full.columns if 'has_error_' in col]\n",
    "df_relevant = df_full[relevant_columns].copy()\n",
    "\n",
    "# Reshape the DataFrame\n",
    "df_grouped = df_relevant.groupby('exp_nr').sum().reset_index().melt(id_vars=[\"exp_nr\"], var_name=\"columns\", value_name='value')\n",
    "# Filter out None values\n",
    "df_grouped = df_grouped[df_grouped['value'].notnull()]\n",
    "# Plot the histogram\n",
    "plt.figure(figsize=(9, 6))\n",
    "ax = sns.barplot(data=df_grouped, x=\"exp_nr\", y=\"value\", hue='columns', dodge=True, width=.95)\n",
    "plt.axvline(x=8.5, color=\"black\", linestyle='--', label='Changed to OT2 source preparation before exp13')\n",
    "plt.axvline(x=12.5, color=\"black\", linestyle='--', label='Changed to full automation before exp17')\n",
    "plt.axvline(x=16.5, color=\"black\", linestyle='--', label='Changed to Eppendorf heat seal before exp21')\n",
    "plt.axvline(x=20.5, color=\"black\", linestyle='--', label='Changed to Agilent heat seal before exp25')\n",
    "plt.xlabel('Experiment number')\n",
    "plt.ylabel('Number of errors')\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "ax.legend(title = \"Error type\", loc='upper left', bbox_to_anchor=(1, 1), handles=handles[4:], labels=[l.removeprefix(\"has_error_\").replace(\"_\", \" \").strip().capitalize() for l in labels[4:]])\n",
    "plt.savefig(SAVE_DIR / \"type_of_errors_per_experiment_binned.png\", dpi=300, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next, we look at the distribution of errors by plate number within an experiment\n",
    "# Keep only the relevant columns\n",
    "relevant_columns = ['plate_nr'] + [col for col in df_full.columns if 'has_error_' in col]\n",
    "df_relevant = df_full[relevant_columns].copy()\n",
    "df_grouped = df_relevant.groupby('plate_nr').sum().reset_index().melt(id_vars=[\"plate_nr\"], var_name=\"columns\", value_name='value')\n",
    "# Filter out None values\n",
    "df_grouped = df_grouped[df_grouped['value'].notnull()]\n",
    "# Plot the histogram\n",
    "plt.figure(figsize=(5, 3.75))\n",
    "ax = sns.barplot(data=df_grouped, x=\"plate_nr\", y=\"value\", hue='columns', dodge=True, width=.5)\n",
    "plt.xlabel('Plate number')\n",
    "plt.ylabel('Number of errors')\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "ax.legend(title = \"Error type\", loc='upper left', bbox_to_anchor=(1, 1), handles=handles, labels=[l.removeprefix(\"has_error_\").replace(\"_\", \" \").strip().capitalize() for l in labels])\n",
    "plt.savefig(SAVE_DIR / \"type_of_errors_per_plate_binned.png\", dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the worst offenders. there might be a structural reason why they give multiple peaks in LCMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df_full.loc[df_full['valid'].notna() & df_full['valid'].str.contains('multiple peaks for product A'), ['initiator']].value_counts()\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's look at these molecules\n",
    "mols = []\n",
    "for bb in x.head(5).index.to_list():\n",
    "    mols.append(db.get_mol(short=bb[0]))\n",
    "from rdkit.Chem import Draw\n",
    "Draw.MolsToGridImage(mols, subImgSize=(300,300), legends=[bb[0] for bb in x.index.to_list()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df_full.loc[df_full['valid'].notna() & df_full['valid'].str.contains('multiple peaks for product A'), ['monomer']].value_counts()\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's look at these molecules\n",
    "mols = []\n",
    "for bb in x.head(10).index.to_list():\n",
    "    mols.append(db.get_mol(short=bb[0]))\n",
    "from rdkit.Chem import Draw\n",
    "Draw.MolsToGridImage(mols, subImgSize=(300,300), legends=[bb[0] for bb in x.index.to_list()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df_full.loc[df_full['valid'].notna() & df_full['valid'].str.contains('multiple peaks for product A'), ['terminator']].value_counts()\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's look at these molecules\n",
    "mols = []\n",
    "for bb in x.head(5).index.to_list():\n",
    "    mols.append(db.get_mol(short=bb[0]))\n",
    "from rdkit.Chem import Draw\n",
    "Draw.MolsToGridImage(mols, subImgSize=(300,300), legends=[bb[0] for bb in x.index.to_list()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove invalid reactions\n",
    "We move all entries containing \"ERROR\". Warnings can stay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_full.loc[~df_full.has_error]\n",
    "print(f'Number of reactions after removing invalid entries: {len(df)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[~df['product_A_lcms_ratio'].isna()]  # ensure all entries have a measured lcms ratio\n",
    "print(f'Number of reactions after removing NaN entries: {len(df)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a cleaned dataset (which may still contain a few duplicates). Let's try some basic statistics\n",
    "\n",
    "How many reactions are successful (i.e. normalized lmcs ratio for A > threshold ?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add min-max normalized LC-MS responses (each column is normalized separately)\n",
    "lcms_ratio_columns = [f'product_{s}_lcms_ratio' for s in \"ABCDEFGH\"]\n",
    "\n",
    "df[[f'{s}_normalized' for s in \"ABCDEFGH\"]] = (\n",
    "      df[lcms_ratio_columns] - df[lcms_ratio_columns].min()) / (df[lcms_ratio_columns].max() - df[lcms_ratio_columns].min()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add information about reactant classes to df\n",
    "def get_reaction_type_for_series(ser):\n",
    "    new = []\n",
    "    for i in ser:\n",
    "        new.append(db.get_building_block_class(i))\n",
    "    return pd.Series(data=new)\n",
    "        \n",
    "df[['initiator_type', 'monomer_type', 'terminator_type']] = df[['initiator', 'monomer', 'terminator']].apply(get_reaction_type_for_series, axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Statistics <a id=3></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at how well reactions have worked across the (cleaned) data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[col for col in df.columns if 'lcms' in col]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for product_type in \"ABCDEFGH\":\n",
    "    print(f'Number of reactions with any LC-MS signal for {product_type}: {len(df.loc[df[f\"{product_type}_normalized\"] > 0])} ({len(df.loc[df[f\"{product_type}_normalized\"] > 0]) / len(df):.1%})')\n",
    "    for threshold in [1e-10, 0.001, 0.005, 0.01, 0.05, 0.1, 0.2]:\n",
    "        t_and_f = (df[f\"{product_type}_normalized\"] > threshold).value_counts()\n",
    "        print(f'This ratio of reactions is above the {threshold:.1} threshold: {t_and_f[True] / (t_and_f[True]+ t_and_f[False]):.1%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a graphical assessment, we plot the histograms for all normalized product yields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbins = 100\n",
    "fig, axs = plt.subplots(nrows=2, ncols=4, figsize=(7.5, 3.75))\n",
    "sns.histplot(df['A_normalized'], bins=nbins, ax=axs[0][0])\n",
    "sns.histplot(df['B_normalized'], bins=nbins, ax=axs[0][1])\n",
    "sns.histplot(df['C_normalized'], bins=nbins, ax=axs[0][2])\n",
    "sns.histplot(df['D_normalized'], bins=nbins, ax=axs[0][3])\n",
    "sns.histplot(df['E_normalized'], bins=nbins, ax=axs[1][0])\n",
    "sns.histplot(df['F_normalized'], bins=nbins, ax=axs[1][1])\n",
    "sns.histplot(df['G_normalized'], bins=nbins, ax=axs[1][2])\n",
    "sns.histplot(df['H_normalized'], bins=nbins, ax=axs[1][3])\n",
    "fig.tight_layout()\n",
    "fig.savefig(SAVE_DIR / 'normalized_lcms_histograms.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbins = 50\n",
    "fig, axs = plt.subplots(nrows=2, ncols=4, figsize=(7.5, 3.75))\n",
    "sns.histplot(df['A_normalized'] + 1e-5, bins=nbins, ax=axs[0][0], log_scale=(True, True))\n",
    "sns.histplot(df['B_normalized'] + 1e-5, bins=nbins, ax=axs[0][1], log_scale=(True, True))\n",
    "sns.histplot(df['C_normalized'] + 1e-5, bins=nbins, ax=axs[0][2], log_scale=(True, True))\n",
    "sns.histplot(df['D_normalized'] + 1e-5, bins=nbins, ax=axs[0][3], log_scale=(True, True))\n",
    "sns.histplot(df['E_normalized'] + 1e-5, bins=nbins, ax=axs[1][0], log_scale=(True, True))\n",
    "sns.histplot(df['F_normalized'] + 1e-5, bins=nbins, ax=axs[1][1], log_scale=(True, True))\n",
    "sns.histplot(df['G_normalized'] + 1e-5, bins=nbins, ax=axs[1][2], log_scale=(True, True))\n",
    "sns.histplot(df['H_normalized'] + 1e-5, bins=nbins, ax=axs[1][3], log_scale=(True, True))\n",
    "for ax in axs.flatten():\n",
    "    ax.set_xlim(1e-5, 1)\n",
    "    ax.set_ylim(1, 5e4)\n",
    "fig.tight_layout()\n",
    "fig.savefig(SAVE_DIR / 'normalized_lcms_histograms_logscale.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's see product A LC-MS response histogram with log axes\n",
    "plt.figure()\n",
    "sns.histplot(df['A_normalized'] + 1e-5, bins=50, log_scale=(True, True))  # +1e-5 to avoid log(0)=-inf\n",
    "plt.title('Log-log histogramm of product A LC-MS ratios')\n",
    "plt.savefig(SAVE_DIR / 'normalized_lcms_product-A_histogram_logscale.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's see product A LC-MS response in a cumulative histogram\n",
    "plt.figure()\n",
    "sns.displot(df['A_normalized'], cumulative=True)\n",
    "plt.title('Cumulative histogramm of product A LC-MS ratios')\n",
    "plt.savefig(SAVE_DIR / 'normalized_lcms_product-A_cumulative_histogram.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's enlarge the 0 to 1e-2 range\n",
    "plt.figure()\n",
    "sns.displot(df.loc[df[\"A_normalized\"] < 0.01, 'A_normalized'], cumulative=True, bins=1000)\n",
    "plt.title('Cumulative histogram of product A LC-MS ratios, cutout 0 to 0.01')\n",
    "plt.savefig(SAVE_DIR / 'normalized_lcms_product-A_cumulative_histogram_cutout_0to1e-2.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's enlarge the 0 to 1e-3 range\n",
    "plt.figure()\n",
    "sns.displot(df.loc[df[\"A_normalized\"] < 0.001, 'A_normalized'], cumulative=True, bins=1000)\n",
    "plt.title('Cumulative histogram of product A LC-MS ratios')\n",
    "plt.axvline(x=0.0002, color=bode_palette.orange)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion from LCMS threshold search\n",
    "The LC-MS threshold can be set to 2e-4 for product A, as this is the point where the curve starts to flatten out. However, there are very few samples in this region, and it may not make sense to exclude them, particularly given that we already have noise filtering in the LC-MS peak picking step. It would make more sense to include everything >0 as positive, but use the lcms ratios for the other products to determine when only a meaningless amount of A is formed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Curious side question: What products have the highest lcms response for product A?\n",
    "\n",
    "products = []\n",
    "for vl_id in df.sort_values('A_normalized', ascending=False).head(5)['vl_id']:\n",
    "    products.append(db.get_vl_member(vl_id))\n",
    "[Compute2DCoords(p) for p in products]\n",
    "Draw.MolsToGridImage(products)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Block statistics <a id=4></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look into details: __For each initator, how many reactions have worked above the threshold?__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD = 0  # response has to be bigger than this to be counted as success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first add a success True/False column\n",
    "for product in \"ABCDEFGH\":\n",
    "    df[f\"{product}_success\"] = df[f\"{product}_normalized\"] > THRESHOLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_success_rate(df, column, product_type=\"A\", sort_results=True):\n",
    "    \"\"\"\n",
    "    Take a dataframe df, where for one column of interest, for each unique identifier in that column, the frequency\n",
    "    of successful reactions (as indicated by the threshold) is calculated.\n",
    "    :param df: pandas.DataFrame\n",
    "    :param column: Column by which to group, e.g. monomer or monomer_type\n",
    "    :param product_type: Which product to use for success rate calculation\n",
    "    :return: pandas.DataFrame\n",
    "    \"\"\"\n",
    "    # group by the column of interest\n",
    "    grouped = df.groupby([column])\n",
    "    # count positive + all examples and determine success rate\n",
    "    n_all = grouped[f\"{product_type}_success\"].count().rename('n_all')\n",
    "    n_success = grouped[f\"{product_type}_success\"].sum().rename('n_success')\n",
    "    success_rates = (n_success / n_all).rename('success_rate')\n",
    "    # assemble results\n",
    "    df = pd.concat([n_all, n_success, success_rates], axis=1)\n",
    "    df.index.name = \"building_block\"\n",
    "    if sort_results is True:\n",
    "        df = df.sort_index(\n",
    "            #key=alphanumeric_index_sort\n",
    "        )\n",
    "    df = df.reset_index()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain dictionary of building block classes from database\n",
    "reactant_class = {f\"I{i}\": db.get_building_block_class(f\"I{i}\") for i in range(1, 79)}\n",
    "reactant_class.update({f\"M{i}\": db.get_building_block_class(f\"M{i}\") for i in range(1, 75)})\n",
    "reactant_class.update({f\"T{i}\": db.get_building_block_class(f\"T{i}\") for i in range(1, 42)})\n",
    "\n",
    "# assign colors to the different classes for plotting\n",
    "reactant_colors = {\n",
    "    \"KAT_hetarom\": \"blue\",\n",
    "    \"KAT_arom\": \"lightblue\",\n",
    "    \"KAT_al\": \"green\",\n",
    "    \"Mon_fused\": \"blue\",\n",
    "    \"Mon_spiro_2\": \"green\",\n",
    "    \"Mon_spiro_3\": \"lightgreen\",\n",
    "    \"Mon_sub_2\": \"red\",\n",
    "    \"Mon_sub_3\": \"orange\",\n",
    "    \"TerABT\": \"blue\",\n",
    "    \"TerTH\": \"green\",\n",
    "}\n",
    "\n",
    "building_block_colors = {k: reactant_colors[v] for k, v in reactant_class.items()}\n",
    "#building_block_colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we calculate success rates for product A for all initiators\n",
    "success_rates_by_initiator = calc_success_rate(df, 'initiator_long')\n",
    "# let's do the same for monomers\n",
    "success_rates_by_monomer = calc_success_rate(df, 'monomer_long')\n",
    "# and the same for terminators\n",
    "success_rates_by_terminator = calc_success_rate(df, 'terminator_long')\n",
    "\n",
    "success_rates = pd.concat((success_rates_by_initiator, success_rates_by_monomer, success_rates_by_terminator))\n",
    "print(\"Success rates:\")\n",
    "for _, i in success_rates.iterrows():\n",
    "    print(f'{i[\"building_block\"]}: {i[\"success_rate\"]:.1%} ({i[\"n_success\"]}/{i[\"n_all\"]})')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_success_count_by_building_block(success_rates, figsize=(4, 6), save_path=None):\n",
    "    # show how many reactions are successful for each initiator\n",
    "    fig, axs = plt.subplots(nrows=1, ncols=2, figsize=figsize, width_ratios=[3, 1])\n",
    "\n",
    "    # plot a mean line\n",
    "    axs[0].axvline(success_rates['success_rate'].mean(), ls='--', c='black')\n",
    "    # plot the success rates\n",
    "    sns.barplot(x=success_rates['success_rate'], y=success_rates['building_block'], ax=axs[0])\n",
    "    # plot the number of experiments\n",
    "    sns.barplot(x=success_rates['n_all'], y=success_rates['building_block'], ax=axs[1])\n",
    "\n",
    "    # set details\n",
    "    axs[0].set_xlabel(f'Ratio of successful reactions')\n",
    "    axs[0].set_ylabel('Building block')\n",
    "    axs[0].set_xlim(0,1)\n",
    "    axs[1].set_xlabel('Number of reactions')\n",
    "    axs[1].set_ylabel(None)\n",
    "    axs[1].set_yticks([])\n",
    "    fig.tight_layout()\n",
    "    if save_path is not None:\n",
    "        fig.savefig(save_path, dpi=300)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to file but don't show\n",
    "plot_success_count_by_building_block(success_rates_by_initiator, save_path=SAVE_DIR / 'success_rates_by_initiator.png')\n",
    "plot_success_count_by_building_block(success_rates_by_monomer, save_path=SAVE_DIR / 'success_rates_by_monomer.png')\n",
    "plot_success_count_by_building_block(success_rates_by_terminator, save_path=SAVE_DIR / 'success_rates_by_terminator.png', figsize=(4, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_outcome_2D(df, x1, x2, y, product, building_block_colors=None, save_dir=SAVE_DIR):\n",
    "    \"\"\"Plot a 2D heatmap of the success rate of reactions for two building block types averaged over the third type\"\"\"\n",
    "    # set up layout\n",
    "    rel_width = (df[x1].nunique(), df[x2].nunique())\n",
    "    rel_height = df[y].nunique() / (df[x1].nunique() + df[x2].nunique())\n",
    "    fig, ax = plt.subplots(figsize=(18, 18* rel_height), ncols=2, width_ratios=rel_width)\n",
    "    cbar_ax = fig.add_axes([0.92, 0.3, 0.015, 0.4])  # [left, bottom, width, height]\n",
    "\n",
    "    # prepare data\n",
    "    df_heatmap1 = df.groupby([y, x1]).mean(numeric_only=True)[f\"{product}_success\"].unstack()\n",
    "    df_heatmap1 = df_heatmap1.sort_index(key=alphanumeric_index_sort)\n",
    "    df_heatmap1 = df_heatmap1.sort_index(key=alphanumeric_index_sort, axis=1)\n",
    "\n",
    "    df_heatmap2 = df.groupby([y, x2]).mean(numeric_only=True)[f\"{product}_success\"].unstack()\n",
    "    df_heatmap2 = df_heatmap2.sort_index(key=alphanumeric_index_sort)\n",
    "    df_heatmap2 = df_heatmap2.sort_index(key=alphanumeric_index_sort, axis=1)\n",
    "\n",
    "    # plot\n",
    "    sns.heatmap(df_heatmap1, ax=ax[0],  cmap='viridis', vmin=0, vmax=1, center=0, linewidths=0.5, annot=False, fmt='.0%', cbar_ax=cbar_ax)\n",
    "    sns.heatmap(df_heatmap2, ax=ax[1],  cmap='viridis', vmin=0, vmax=1, center=0, linewidths=0.5, annot=False, fmt='.0%', cbar=False, yticklabels=False)\n",
    "\n",
    "    ax[0].set_yticklabels(ax[0].get_yticklabels(), rotation=0)\n",
    "    if building_block_colors:\n",
    "        for axes in ax:\n",
    "            # Set the x-axis tick label background colors for the first heatmap\n",
    "            for label in axes.get_xticklabels():\n",
    "                bbox = dict(boxstyle=\"round\", fc=building_block_colors[label.get_text()], alpha=0.5)\n",
    "                label.set_bbox(bbox)\n",
    "            # Set the y-axis tick label background colors for the first heatmap\n",
    "            for label in axes.get_yticklabels():\n",
    "                bbox = dict(boxstyle=\"round\", fc=building_block_colors[label.get_text()], alpha=0.5)\n",
    "                label.set_bbox(bbox)\n",
    "\n",
    "    ax[0].set_xlabel(x1.capitalize())\n",
    "    ax[0].set_ylabel(y.capitalize())\n",
    "    ax[1].set_xlabel(x2.capitalize())\n",
    "    ax[1].set_ylabel(None)\n",
    "    if save_dir:\n",
    "        fig.savefig(save_dir / f\"heatmap_{y}_product{product}.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# save to file but don't show\n",
    "for product in \"ABCDEFGH\":\n",
    "    plot_outcome_2D(df, \"monomer\", \"terminator\", \"initiator\", product, building_block_colors=building_block_colors)\n",
    "    plot_outcome_2D(df, \"initiator\", \"terminator\", \"monomer\", product, building_block_colors=building_block_colors)\n",
    "    plot_outcome_2D(df, \"initiator\", \"monomer\", \"terminator\", product, building_block_colors=building_block_colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trends within building block classes <a id=5></a>\n",
    "Let's look at how well different types of initiators, monomers, terminators react"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's check Initiators\n",
    "success_rates_by_initiator_type = calc_success_rate(df, 'initiator_type', sort_results=False)\n",
    "for _, i in success_rates_by_initiator_type.iterrows():\n",
    "    print(f'Success rate for {i[\"building_block\"]}: {i[\"success_rate\"]:.1%} ({i[\"n_success\"]}/{i[\"n_all\"]})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's check Monomers\n",
    "success_rates_by_monomer_type = calc_success_rate(df, 'monomer_type', sort_results=False)\n",
    "for _, i in success_rates_by_monomer_type.iterrows():\n",
    "    print(f'Success rate for {i[\"building_block\"]}: {i[\"success_rate\"]:.1%} ({i[\"n_success\"]}/{i[\"n_all\"]})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's check Terminators\n",
    "success_rates_by_terminator_type = calc_success_rate(df, 'terminator_type',  sort_results=False)\n",
    "for _, i in success_rates_by_terminator_type.iterrows():\n",
    "    print(f'Success rate for {i[\"building_block\"]}: {i[\"success_rate\"]:.1%} ({i[\"n_success\"]}/{i[\"n_all\"]})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling the LCMS responses <a id=6></a>\n",
    "- It seems to make sense to use some kind of robust scaling as we definitely expect outliers in LCMS data.\n",
    "- We do not require centering, as the data starts at 0 and is not normally distributed.\n",
    "- We could divide by the interquartile range (IQR) to get a robust scaling.\n",
    "- We could also use the median absolute deviation (MAD) to get a robust scaling.\n",
    "- However, both above ideas seem more suitable for symmetric distributions, while we have a highly skewed distribution with a defined minimum at 0.\n",
    "- Therefore, we could just divide by, say, the 90th percentile to get the equivalent of min-max scaling but robust to outliers.\n",
    "- Which percentile we choose, depends on the number of outliers we expect. The 90th percentile seems reasonable as we already remove internal standard errors and we can expect less than 10% of compounds to be outliers by virtue of ionizability alone. At the same time, choosing the 90th means we expect all products to form in at least 10% of the reactions.\n",
    "- To not be dependent on how often a product forms: Use a lower (say 80th) percentile, but use only non-zero values in calculating the percentile.\n",
    "\n",
    "### An extension to the scaling idea\n",
    "Assume we have achieved robust scaling. There remains a problem:\n",
    "- Consider product A systematically ionizes 10 times better than product B.\n",
    "- Further, consider a reaction where we measure a non-zero response for product A that is at the lower detection limit\n",
    "- It follows that we have no way to tell whether product B was formed or not, we only know it was not formed 10 times as much as A.\n",
    "\n",
    "In general, if product A ionized better than product B by a factor of x, we need to see a response for product A exceeding the lower detection limit by a factor of x to conclude that product A is the major product.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def robust_scale(x: np.ndarray, percentile: int = 75, return_scaling_factor=False) -> Union[np.ndarray, Tuple[np.ndarray, float]]:\n",
    "    \"\"\"\n",
    "    Robust scaling of a numpy array by dividing by the value for a percentile.\n",
    "    The percentile is calculated from all non-zero values.\n",
    "    \"\"\"\n",
    "    x = x.copy()\n",
    "    scaling_factor = np.percentile(x[x > 0], percentile)\n",
    "    x /= scaling_factor\n",
    "    if return_scaling_factor:\n",
    "        return x, scaling_factor\n",
    "    else:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# before we apply scaling, we need to verify how sensitive the scaling is to our choice of the percentile.\n",
    "factors = []\n",
    "for percentile in range(1, 100):\n",
    "    scaled_arrs = []\n",
    "    scaling_factors = []\n",
    "    for s in \"ABCDEFGH\":\n",
    "        arr, factor = robust_scale(df[f'product_{s}_lcms_ratio'].values, percentile=percentile, return_scaling_factor=True)\n",
    "        scaled_arrs.append(arr)\n",
    "        scaling_factors.append(factor)\n",
    "    factors.append(scaling_factors / scaling_factors[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the relative scaling factors for each product (A is always 1) for all percentiles between [1, 99]\n",
    "plt.figure(figsize=(7.5,5))\n",
    "plt.plot(factors)\n",
    "plt.xlabel(\"Percentile\")\n",
    "plt.ylabel(\"Scaling factor, relative to A\")\n",
    "plt.axvline(85, color=\"red\", linestyle=\"--\", label=\"85th\")\n",
    "plt.legend(\"ABCDEFGH\")\n",
    "plt.savefig(SAVE_DIR / \"scaling_factors_by_percentile.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result of the percentile/scaling factor analysis\n",
    "This outcome is good. The range of the scaling factors is mostly 2x - 3x. This seems reasonable given that the molecules always share quite a few residues. Also, except product D, all relative factors are quite stable w.r.t. the choice of percentile. In particular, relative A,B,C are stable. As we expect, in the very high percentiles (>95 for most, >90 for E), the scaling factor becomes unstable as it is now dominated by outliers.\n",
    "\n",
    "**Based on the plot, we choose the 85th percentile**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply scaling to all products and show scaling factors for control\n",
    "scaling_factors = []\n",
    "for s in \"ABCDEFGH\":\n",
    "        arr, factor = robust_scale(df[f'product_{s}_lcms_ratio'].values, percentile=85, return_scaling_factor=True)\n",
    "        df[f'{s}_scaled'] = arr\n",
    "        scaling_factors.append(factor)\n",
    "print(\"Scaling factors relative to A:\")\n",
    "for s, i in zip(\"ABCDEFGH\", scaling_factors):\n",
    "    print(f\"{s}: {i/scaling_factors[0]:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion of scaling factors\n",
    "From the values of the scaling factors, products A, B, E, and  F ionize on a similar scale while product D ionizes 5x better and products C, G, and H ionize 5x worse.\n",
    "\n",
    "Does this make sense bases on chemical intuition?\n",
    "It's hard to predict ionization in MS, but the good ionization of D seems plausible as it is rather small and has a lot of polar groups. H is surprising because a similar argument could be made, but then again, H forms under very specific circumstances, so the side chain diversity is limited and that may bias the ionizability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"A_scaled\", \"B_scaled\", \"C_scaled\", \"D_scaled\", \"E_scaled\", \"F_scaled\", \"G_scaled\", \"H_scaled\"]].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outcome for the main product <a id=7></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign the main product\n",
    "df[\"main_product\"] = df[[\"A_scaled\", \"B_scaled\", \"C_scaled\"]].idxmax(axis=1).str.replace(\"_scaled\", \"\")\n",
    "# are there any reactions where neither A,nor B, nor C appear?\n",
    "df.loc[df[[\"A_scaled\", \"B_scaled\", \"C_scaled\"]].sum(axis=1) == 0, \"main_product\"] = \"none\"\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how are the different success rates related?\n",
    "# scatterplot between A_scaled and others\n",
    "x = df[\"A_scaled\"].values\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,10), ncols=3, nrows=3)\n",
    "for i, s in enumerate(\"BCDEFGH\"):\n",
    "    row = i // 3\n",
    "    col = i % 3\n",
    "    y = np.nan_to_num(df[f\"{s}_scaled\"].values)\n",
    "    # Calculate the point density\n",
    "    xy = np.vstack([x,y])\n",
    "    z = np.log10(gaussian_kde(xy)(xy))\n",
    "\n",
    "    # Sort the points by density, so that the densest points are plotted last\n",
    "    idx = z.argsort()\n",
    "    x, y, z = x[idx], y[idx], z[idx]\n",
    "\n",
    "    ax[row, col].scatter(x, y, c=z, s=5, cmap=\"viridis\")\n",
    "    ax[row, col].set_xlabel('A_scaled')\n",
    "    ax[row, col].set_ylabel(f'{s}_scaled')\n",
    "    ax[row, col].set_xlim(0, 10)  # exclude outliers. We scaled the data to be able to compared them\n",
    "    ax[row, col].set_ylim(0, 10)  # exclude outliers. We scaled the data to be able to compared them\n",
    "# Hide the last two axes\n",
    "ax[2, 1].set_visible(False)\n",
    "ax[2, 2].set_visible(False)\n",
    "plt.tight_layout()\n",
    "plt.savefig(SAVE_DIR / 'LCMS_yields_scatter.png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# among A,B,C, which is the major product?\n",
    "def plot_main_product_2D(df, x1, x2, y, building_block_colors=None, save_dir=SAVE_DIR):\n",
    "    \"\"\"Plot a 2D heatmap of the success rate of reactions for two building block types averaged over the third type\"\"\"\n",
    "    rel_width = (df[x1].nunique(), df[x2].nunique())\n",
    "    rel_height = df[y].nunique() / (df[x1].nunique() + df[x2].nunique())\n",
    "    fig, ax = plt.subplots(figsize=(14, 14 * rel_height), ncols=2, width_ratios=rel_width)\n",
    "    cbar_ax = fig.add_axes([0.92, 0.1, 0.015, 0.8])  # [left, bottom, width, height]\n",
    "    df_heatmap1 = df.groupby([y, x1])[\"main_product\"].describe()[\"top\"].map({\"A\": 0, \"B\": 1, \"C\": 2, \"none\": 3}).unstack()\n",
    "    df_heatmap1 = df_heatmap1.sort_index(key=alphanumeric_index_sort)\n",
    "    df_heatmap1 = df_heatmap1.sort_index(key=alphanumeric_index_sort, axis=1)\n",
    "\n",
    "    df_heatmap2 = df.groupby([y, x2])[\"main_product\"].describe()[\"top\"].map({\"A\": 0, \"B\": 1, \"C\": 2, \"none\": 3}).unstack()\n",
    "    df_heatmap2 = df_heatmap2.sort_index(key=alphanumeric_index_sort)\n",
    "    df_heatmap2 = df_heatmap2.sort_index(key=alphanumeric_index_sort, axis=1)\n",
    "\n",
    "    # Plot the first heatmap\n",
    "    sns.heatmap(df_heatmap1, ax=ax[0], cmap=sns.color_palette(\"colorblind\", n_colors=4), linewidths=0.5, annot=False, fmt='.0%', cbar_ax=cbar_ax, xticklabels=True)\n",
    "\n",
    "    # Plot the second heatmap\n",
    "    sns.heatmap(df_heatmap2, ax=ax[1], cmap=sns.color_palette(\"colorblind\", n_colors=4), linewidths=0.5, annot=False, fmt='.0%', cbar=False, xticklabels=True, yticklabels=False)\n",
    "    \n",
    "    ax[0].set_xticklabels(ax[0].get_xticklabels(), rotation=90)\n",
    "    ax[0].set_yticklabels(ax[0].get_yticklabels(), rotation=0)\n",
    "    ax[0].set_xlabel(ax[0].get_xlabel().capitalize())\n",
    "    ax[0].set_ylabel(ax[0].get_ylabel().capitalize())\n",
    "    ax[1].set_xticklabels(ax[1].get_xticklabels(), rotation=90)\n",
    "    ax[1].set_xlabel(ax[1].get_xlabel().capitalize())\n",
    "    ax[1].set_ylabel(None)\n",
    "\n",
    "    # Set the tick label background colors\n",
    "    if building_block_colors:\n",
    "        for axes in ax:\n",
    "            for label in axes.get_xticklabels():\n",
    "                bbox = dict(boxstyle=\"Round,pad=0.1\", fc=building_block_colors[label.get_text()], alpha=0.5)\n",
    "                label.set_bbox(bbox)\n",
    "            # Set the y-axis tick label background colors for the first heatmap\n",
    "            for label in axes.get_yticklabels():\n",
    "                bbox = dict(boxstyle=\"Round,pad=0.1\", fc=building_block_colors[label.get_text()], alpha=0.5)\n",
    "                label.set_bbox(bbox)\n",
    "\n",
    "    # change the colorbar labels to convey their meaning\n",
    "    cbar_ax.set_ylabel(\"Main product\")\n",
    "    cbar_ax.yaxis.set_label_position(\"left\")\n",
    "    cbar_ax.set_yticks([i * 3 * 0.25 for i in (0.5, 1.5, 2.5, 3.5)], [\"A\", \"B\", \"C\", \"none\"], rotation=0)\n",
    "    #cbar_ax.set_yticks([])  # for slides, remove the ticks\n",
    "    cbar_ax.set_aspect(10)\n",
    "\n",
    "    if save_dir:\n",
    "        fig.savefig(save_dir / f\"heatmap_{y}_main_product.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "plot_main_product_2D(df, \"monomer\", \"terminator\", \"initiator\", building_block_colors=building_block_colors)\n",
    "plot_main_product_2D(df, \"initiator\", \"terminator\", \"monomer\", building_block_colors=building_block_colors)\n",
    "plot_main_product_2D(df, \"initiator\", \"monomer\", \"terminator\", building_block_colors=building_block_colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot with relative success rates\n",
    "The plots above exclude some information because they only show the most successful product. We can also plot the relative success rates of the different products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = df.groupby([\"initiator\", \"monomer\"])\n",
    "all_count = grouped[\"main_product\"].count()\n",
    "a_ratio = grouped[\"main_product\"].sum().str.count(\"A\") / all_count\n",
    "b_ratio = grouped[\"main_product\"].sum().str.count(\"B\") / all_count\n",
    "c_ratio = grouped[\"main_product\"].sum().str.count(\"C\") / all_count\n",
    "none_ratio = grouped[\"main_product\"].sum().str.count(\"none\") / all_count\n",
    "relative_products = pd.concat((a_ratio, b_ratio, c_ratio, 1 - none_ratio), axis=1, ignore_index=True)  # we do 1 - none_ratio because we will use that for alpha\n",
    "relative_products = relative_products.unstack()\n",
    "relative_products = relative_products.sort_index(key=alphanumeric_index_sort)\n",
    "relative_products = relative_products.sort_index(key=alphanumeric_index_sort, axis=1, level=1, sort_remaining=False)\n",
    "arr = relative_products.to_numpy()\n",
    "heatmap_arr = arr.reshape((len(relative_products.index.get_level_values(0).unique()), len(relative_products.columns.get_level_values(1).unique()), 4), order=\"C\")\n",
    "heatmap_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,10), ncols=2, width_ratios=(.8, .2))\n",
    "ax[1].axis(\"off\")\n",
    "ax = ax[0]\n",
    "legend_ax = fig.add_axes([0.8, 0.4, 0.18, 0.2])  # [left, bottom, width, height]\n",
    "im = ax.imshow(heatmap_arr, interpolation='nearest')\n",
    "ax.set_xticks(np.arange(heatmap_arr.shape[1]), labels=relative_products.columns.get_level_values(1).unique(), rotation=90)\n",
    "ax.set_yticks(np.arange(heatmap_arr.shape[0]), labels=relative_products.index)\n",
    "\n",
    "legend_ax.add_patch(Rectangle((0, 0), .1, .1, color=(1,0,0,1)))\n",
    "legend_ax.text(0.05, 0.05, \"A\", fontsize=12, ha=\"center\", va=\"center\")\n",
    "legend_ax.add_patch(Rectangle((0, .1), .1, .1, color=(0,1,0,1)))\n",
    "legend_ax.text(0.05, 0.15, \"B\", fontsize=12, ha=\"center\", va=\"center\")\n",
    "legend_ax.add_patch(Rectangle((0, .2), .1, .1, color=(0,0,1,1)))\n",
    "legend_ax.text(0.05, 0.25, \"C\", fontsize=12, ha=\"center\", va=\"center\")\n",
    "\n",
    "legend_ax.add_patch(Rectangle((.1, 0), .1, .1, color=(.5,.5,0,1)))\n",
    "legend_ax.text(0.15, 0.05, \"A+B\", fontsize=12, ha=\"center\", va=\"center\")\n",
    "legend_ax.add_patch(Rectangle((.1, .1), .1, .1, color=(.5,0,.5,1)))\n",
    "legend_ax.text(0.15, 0.15, \"A+C\", fontsize=12, ha=\"center\", va=\"center\")\n",
    "legend_ax.add_patch(Rectangle((.1, .2), .1, .1, color=(0,.5,.5,1)))\n",
    "legend_ax.text(0.15, 0.25, \"B+C\", fontsize=12, ha=\"center\", va=\"center\")\n",
    "\n",
    "legend_ax.add_patch(Rectangle((.2, .0), .1, .1, color=(.33,.33,.33,1)))\n",
    "legend_ax.text(0.25, 0.05, \"A+B+C\", fontsize=12, ha=\"center\", va=\"center\")\n",
    "legend_ax.set_xlim(0, 0.3)\n",
    "legend_ax.set_ylim(0, 0.3)\n",
    "legend_ax.axis(\"off\")\n",
    "legend_ax.set_title(\"Legend\", fontsize=12)\n",
    "fig.tight_layout()\n",
    "fig.savefig(SAVE_DIR / \"heatmap_IM_relative_success.png\", dpi=300, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can we plot the same information better by using tiny pie charts as the heatmap?\n",
    "def heatmap_with_pies(ax, data_array, row_labels, column_labels, cmap=None, norm=None, building_block_colors=None):\n",
    "\n",
    "    pies=[]\n",
    "    # generate all the individual pies\n",
    "    for row_index, row in enumerate(row_labels):\n",
    "        for column_index, column in enumerate(column_labels):\n",
    "            data_slice = data_array[row_index, column_index]\n",
    "            radius = .4 * data_slice[:3].sum() +.05\n",
    "            if np.isnan(data_slice).any():\n",
    "                # if there are nan value, we have not conducted this reaction, so we plot nothing\n",
    "                continue\n",
    "            if data_slice.sum() == 0:\n",
    "                # if there are no products found, we plot a black circle\n",
    "                wedges, _ = plt.pie((1,), center=(column_index, row_index), radius=radius, colors=(\"black\",), wedgeprops={\"linewidth\": 0})\n",
    "            else:\n",
    "                wedges, _ = plt.pie(data_slice, center=(column_index, row_index), radius=radius, colors=sns.color_palette(\"colorblind\", n_colors=4), wedgeprops={\"linewidth\": 0}, startangle=270)\n",
    "\n",
    "    col = PatchCollection(pies, array=data_array.flatten(), cmap=cmap, norm=norm)\n",
    "\n",
    "    ax.add_collection(col)\n",
    "\n",
    "    ax.set_xticks(np.arange(data_array.shape[1]), labels=column_labels, rotation=90)\n",
    "    ax.set_yticks(np.arange(data_array.shape[0]), labels=row_labels)\n",
    "\n",
    "    # we invert the y-axis (so that the first building block will be at the top)\n",
    "    ax.invert_yaxis()\n",
    "\n",
    "    # put a legend, based only on the last wedges (faster than automatic detection)\n",
    "    ax.legend(wedges, (\"A\", \"B\", \"C\", \"none\"), loc=\"upper left\", bbox_to_anchor=(1, 1), title=\"Products\")\n",
    "\n",
    "    # Set the tick label background colors\n",
    "    if building_block_colors:\n",
    "        for label in ax.get_xticklabels():\n",
    "            bbox = dict(boxstyle=\"round\", fc=building_block_colors[label.get_text()], alpha=0.5)\n",
    "            label.set_bbox(bbox)\n",
    "        # Set the y-axis tick label background colors for the first heatmap\n",
    "        for label in ax.get_yticklabels():\n",
    "            bbox = dict(boxstyle=\"round\", fc=building_block_colors[label.get_text()], alpha=0.5)\n",
    "            label.set_bbox(bbox)\n",
    "\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# plot for initiator and monomer\n",
    "data_array=heatmap_arr.copy()\n",
    "data_array[:,:,3] = 1 - data_array[:,:,3]  # we need the success rate not the failure rate\n",
    "row_labels=relative_products.index\n",
    "column_labels=relative_products.columns.get_level_values(1).unique()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(11, 10))\n",
    "heatmap_with_pies(ax, data_array, row_labels, column_labels, building_block_colors=building_block_colors)\n",
    "ax.set_xlabel(\"Monomer\")\n",
    "ax.set_ylabel(\"Initiator\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(SAVE_DIR / \"heatmap_IM_relative_success_pies.png\", dpi=300, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data for initiator and terminator\n",
    "grouped = df.groupby([\"initiator\", \"terminator\"])\n",
    "all_count = grouped[\"main_product\"].count()\n",
    "a_ratio = grouped[\"main_product\"].sum().str.count(\"A\") / all_count\n",
    "b_ratio = grouped[\"main_product\"].sum().str.count(\"B\") / all_count\n",
    "c_ratio = grouped[\"main_product\"].sum().str.count(\"C\") / all_count\n",
    "none_ratio = grouped[\"main_product\"].sum().str.count(\"none\") / all_count\n",
    "relative_products = pd.concat((a_ratio, b_ratio, c_ratio, 1 - none_ratio), axis=1, ignore_index=True)  # we do 1 - none_ratio because we will use that for alpha\n",
    "relative_products = relative_products.unstack()\n",
    "relative_products = relative_products.sort_index(key=alphanumeric_index_sort)\n",
    "relative_products = relative_products.sort_index(key=alphanumeric_index_sort, axis=1, level=1, sort_remaining=False)\n",
    "arr = relative_products.to_numpy()\n",
    "heatmap_arr = arr.reshape((len(relative_products.index.get_level_values(0).unique()), len(relative_products.columns.get_level_values(1).unique()), 4), order=\"C\")\n",
    "heatmap_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# plot for initiator and terminator\n",
    "data_array=heatmap_arr.copy()\n",
    "data_array[:,:,3] = 1 - data_array[:,:,3]  # we need the success rate not the failure rate\n",
    "row_labels=relative_products.index\n",
    "column_labels=relative_products.columns.get_level_values(1).unique()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(11, 10))\n",
    "heatmap_with_pies(ax, data_array, row_labels, column_labels, building_block_colors=building_block_colors)\n",
    "ax.set_xlabel(\"Terminator\")\n",
    "ax.set_ylabel(\"Initiator\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(SAVE_DIR / \"heatmap_IT_relative_success_pies.png\", dpi=300, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data for monomer and terminator\n",
    "grouped = df.groupby([\"monomer\", \"terminator\"])\n",
    "all_count = grouped[\"main_product\"].count()\n",
    "a_ratio = grouped[\"main_product\"].sum().str.count(\"A\") / all_count\n",
    "b_ratio = grouped[\"main_product\"].sum().str.count(\"B\") / all_count\n",
    "c_ratio = grouped[\"main_product\"].sum().str.count(\"C\") / all_count\n",
    "none_ratio = grouped[\"main_product\"].sum().str.count(\"none\") / all_count\n",
    "relative_products = pd.concat((a_ratio, b_ratio, c_ratio, 1 - none_ratio), axis=1, ignore_index=True)  # we do 1 - none_ratio because we will use that for alpha\n",
    "relative_products = relative_products.unstack()\n",
    "relative_products = relative_products.sort_index(key=alphanumeric_index_sort)\n",
    "relative_products = relative_products.sort_index(key=alphanumeric_index_sort, axis=1, level=1, sort_remaining=False)\n",
    "arr = relative_products.to_numpy()\n",
    "heatmap_arr = arr.reshape((len(relative_products.index.get_level_values(0).unique()), len(relative_products.columns.get_level_values(1).unique()), 4), order=\"C\")\n",
    "heatmap_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# plot for monomer and terminator\n",
    "data_array=heatmap_arr.copy()\n",
    "data_array[:,:,3] = 1 - data_array[:,:,3]  # we need the success rate not the failure rate\n",
    "row_labels=relative_products.index\n",
    "column_labels=relative_products.columns.get_level_values(1).unique()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(11, 10))\n",
    "heatmap_with_pies(ax, data_array, row_labels, column_labels, building_block_colors=building_block_colors)\n",
    "ax.set_xlabel(\"Terminator\")\n",
    "ax.set_ylabel(\"Monomer\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(SAVE_DIR / \"heatmap_MT_relative_success_pies.png\", dpi=300, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outcomes for repeated reactions\n",
    "We ended up creating some duplicate reactions (divergence from the original duplicate-free plan became necessary when we ran out of some building blocks). Let's check how well they reproduce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicated = df.loc[df.duplicated(subset=[\"long_name\"], keep=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of duplicated reactions\n",
    "grouped_duplicates = duplicated.groupby(\"long_name\")\n",
    "len(grouped_duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_duplicates_unstacked = grouped_duplicates[\"A_scaled\"].apply(lambda x: pd.Series(x.values)).unstack()\n",
    "grouped_duplicates_unstacked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(7.5, 7.5), nrows=3, ncols=3)\n",
    "fig.subplots_adjust(left=0.15)\n",
    "product_data = []  # we will want to re-use the data later, so we catch it here\n",
    "for i, s in enumerate(\"ABCDEFGH\"):\n",
    "    row = i // 3\n",
    "    col = i % 3\n",
    "    \n",
    "    # extract data\n",
    "    data = grouped_duplicates[f\"{s}_scaled\"].apply(lambda x: pd.Series(x.values)).unstack()\n",
    "    product_data.append(data)  # not used for plotting, just later re-use\n",
    "    \n",
    "    # plot data\n",
    "    ax[row, col] = sns.scatterplot(data=data, x=0, y=1, ax=ax[row, col], s=8, color=bode_palette.blue)\n",
    "    # plot diagonal (in data coords)\n",
    "    ax[row, col].plot(ax[row, col].get_ylim(), ax[row,col].get_ylim(), c=bode_palette.orange, ls=\":\")\n",
    "    # plot R2 score\n",
    "    ax[row, col].text(x=(ax[row, col].get_xlim()[1] * .8), y=.1, s=f\"$R^2$={r2_score(data[0], data[1]):.2f}\")\n",
    "    # plot a label for the axes\n",
    "    ax[row, col].text(x=-0.1, y=1.1, s=s, fontsize=12, fontweight=\"bold\", ha=\"center\", va=\"center\", transform=ax[row,col].transAxes)\n",
    "    \n",
    "    # set axis labels\n",
    "    if col == 0:\n",
    "        ax[row, col].set_ylabel(\"Replicate 2\")\n",
    "    else:\n",
    "        ax[row, col].set_ylabel(None)\n",
    "    if row == 2:\n",
    "        ax[row, col].set_xlabel(\"Replicate 1\")\n",
    "    else:\n",
    "        ax[row, col].set_xlabel(None)\n",
    "    \n",
    "ax[2, 2].set_visible(False)\n",
    "ax[1, 2].set_xlabel(\"Replicate 1\")\n",
    "fig.tight_layout()\n",
    "fig.savefig(SAVE_DIR / \"replicates_scaled_responses_scatter.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# at a specific threshold for binning into positive and negative class, how often do the labels have the same class?\n",
    "accuracies = []\n",
    "for s, data in zip(\"ABCDEFGH\", product_data):\n",
    "    accuracy = accuracy_score((data > 0)[0], (data > 0)[1])\n",
    "    accuracies.append(accuracy)\n",
    "    print(f\"Product {s}: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot this in a bar plot\n",
    "fig, ax = plt.subplots(figsize=(3.6, 2.4))\n",
    "sns.barplot(x=list(\"ABCDEFGH\"), y=accuracies, ax=ax, color=bode_palette.blues[2])\n",
    "ax.axhline(1, ls=\"--\", c=\"black\")\n",
    "ax.set_xlabel(\"Product\")\n",
    "ax.set_ylabel(\"Accuracy\")\n",
    "ax.set_ylim(0,1)\n",
    "fig.tight_layout()\n",
    "fig.savefig(SAVE_DIR / \"replicates_binned_accuracies.png\", dpi=300)\n",
    "fig.savefig(SAVE_DIR / \"replicates_binned_accuracies.svg\", transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_main_products = grouped_duplicates[\"main_product\"].apply(lambda x: pd.Series(x.values)).unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_mat = confusion_matrix(grouped_main_products[0], grouped_main_products[1])\n",
    "conf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(2.4, 2.4))\n",
    "axislabels = [\"A\", \"B\", \"C\", \"None\"]\n",
    "ax = sns.heatmap(conf_mat, cmap=\"viridis\", annot=True, fmt=\"d\", xticklabels=axislabels, yticklabels=axislabels, ax=ax, cbar=False)\n",
    "ax.set_xlabel(\"Replicate 1\")\n",
    "ax.set_ylabel(\"Replicate 2\")\n",
    "ax.set_yticks(ax.get_yticks(), ax.get_yticklabels(), rotation=0)\n",
    "ax.set_aspect(1)\n",
    "fig.tight_layout()\n",
    "fig.savefig(SAVE_DIR / \"replicates_confusion_matrix.png\", dpi=300)\n",
    "fig.savefig(SAVE_DIR / \"replicates_confusion_matrix.svg\", transparent=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_accuracy_score(grouped_main_products[0], grouped_main_products[1])  # nb, this is average (macro) recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_score(grouped_main_products[0], grouped_main_products[1], average=\"weighted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
