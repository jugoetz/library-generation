{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Splits Truncated\n",
    "\n",
    "## Diastereomers\n",
    "For the 0D split, this is not important, but for 1D/2D/3D, we want to define the groups such that diastereomers will always be in the same group. These splits will receive a `_dia` suffix.\n",
    "\n",
    "## Synthetic data\n",
    "For the 1D/2D/3D problems we use a synthetically amended data set. Splits of the synthetically ammended data set will receive a `_synthetic` suffix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import sys\n",
    "\n",
    "sys.path.append(str(pathlib.Path().resolve().parents[1]))\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GroupShuffleSplit, ShuffleSplit\n",
    "\n",
    "from src.definitions import DATA_DIR\n",
    "from src.util.train_test_split import GroupShuffleSplitND\n",
    "from util import write_indices_and_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data_filename = \"synferm_dataset_2024-04-18_38586records.csv\"\n",
    "data_name = data_filename.rsplit(\"_\", maxsplit=1)[0]\n",
    "df = pd.read_csv(DATA_DIR / \"curated_data\" / data_filename)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# M_long_dia will be to sort diastereomers into the same group on group shuffle splits\n",
    "diastereomers = {\n",
    "    \"Mon001\": \"Mon087\",\n",
    "    \"Mon003\": \"Mon078\",\n",
    "    \"Mon011\": \"Mon088\",\n",
    "    \"Mon013\": \"Mon074\",\n",
    "    \"Mon014\": \"Mon090\",\n",
    "    \"Mon015\": \"Mon076\",\n",
    "    \"Mon016\": \"Mon096\",\n",
    "    \"Mon017\": \"Mon075\",\n",
    "    \"Mon019\": \"Mon091\",\n",
    "    \"Mon020\": \"Mon077\",\n",
    "    \"Mon080\": \"Mon010\",\n",
    "}\n",
    "df[\"M_long_dia\"] = df[\"M_long\"].replace(diastereomers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0D split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_0d(splitter, inner_splitter):\n",
    "    indices = []\n",
    "    sizes = []\n",
    "    pos_class = []\n",
    "    for idx_train_val, idx_test in splitter.split(df):\n",
    "        # inner split\n",
    "        train, val = next(inner_splitter.split(idx_train_val))\n",
    "        # use indices to index indices :P (we need to obtain indices referring to the original dataframe)\n",
    "        idx_train = idx_train_val[train]\n",
    "        idx_val = idx_train_val[val]\n",
    "        # add to list\n",
    "        indices.append((idx_train, idx_val, idx_test))\n",
    "        sizes.append((len(idx_train), len(idx_val), len(idx_test)))\n",
    "        pos_class.append(\n",
    "            (np.sum(df[['binary_A', 'binary_B', 'binary_C']].loc[idx_train]).to_numpy(), \n",
    "             np.sum(df[['binary_A', 'binary_B', 'binary_C']].loc[idx_val]).to_numpy(), \n",
    "             np.sum(df[['binary_A', 'binary_B', 'binary_C']].loc[idx_test]).to_numpy(),\n",
    "            )\n",
    "        )\n",
    "        \n",
    "    return indices, sizes, pos_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = ShuffleSplit(n_splits=9, test_size=0.1, random_state=10)\n",
    "inner_splitter = ShuffleSplit(n_splits=1, test_size=0.1/0.9, random_state=np.random.RandomState(11))  # we use a RandomState instance, not an int, because we will reuse this splitter several times\n",
    "\n",
    "indices, sizes, pos_class = split_0d(splitter, inner_splitter)\n",
    "\n",
    "print(sizes)\n",
    "#print(pos_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_indices_and_stats(\n",
    "    indices, sizes, pos_class, split_dimension=0, save_indices=True, train_size=80, total_size=len(df), data_name=data_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = ShuffleSplit(n_splits=9, test_size=0.3, random_state=12)\n",
    "inner_splitter = ShuffleSplit(n_splits=1, test_size=0.3/0.7, random_state=np.random.RandomState(13))  # we use a RandomState instance, not an int, because we will reuse this splitter several times\n",
    "\n",
    "indices, sizes, pos_class = split_0d(splitter, inner_splitter)\n",
    "\n",
    "print(sizes)\n",
    "#print(pos_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_indices_and_stats(\n",
    "    indices, sizes, pos_class, split_dimension=0, save_indices=True, train_size=40, total_size=len(df), data_name=data_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = ShuffleSplit(n_splits=9, test_size=0.4, random_state=14)\n",
    "inner_splitter = ShuffleSplit(n_splits=1, test_size=0.4/0.6, random_state=np.random.RandomState(15))  # we use a RandomState instance, not an int, because we will reuse this splitter several times\n",
    "\n",
    "indices, sizes, pos_class = split_0d(splitter, inner_splitter)\n",
    "\n",
    "print(sizes)\n",
    "#print(pos_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_indices_and_stats(\n",
    "    indices, sizes, pos_class, split_dimension=0, save_indices=True, train_size=20, total_size=len(df), data_name=data_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = ShuffleSplit(n_splits=9, test_size=0.45, random_state=16)\n",
    "inner_splitter = ShuffleSplit(n_splits=1, test_size=0.45/0.55, random_state=np.random.RandomState(17))  # we use a RandomState instance, not an int, because we will reuse this splitter several times\n",
    "\n",
    "indices, sizes, pos_class = split_0d(splitter, inner_splitter)\n",
    "\n",
    "print(sizes)\n",
    "#print(pos_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_indices_and_stats(\n",
    "    indices, sizes, pos_class, split_dimension=0, save_indices=True, train_size=10, total_size=len(df), data_name=data_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = ShuffleSplit(n_splits=9, test_size=0.45, random_state=18)\n",
    "inner_splitter = ShuffleSplit(n_splits=1, test_size=0.5/0.55, random_state=np.random.RandomState(19))  # we use a RandomState instance, not an int, because we will reuse this splitter several times\n",
    "\n",
    "indices, sizes, pos_class = split_0d(splitter, inner_splitter)\n",
    "\n",
    "print(sizes)\n",
    "#print(pos_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_indices_and_stats(\n",
    "    indices, sizes, pos_class, split_dimension=0, save_indices=True, train_size=5, total_size=len(df), data_name=data_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = ShuffleSplit(n_splits=9, test_size=0.475, random_state=20)\n",
    "inner_splitter = ShuffleSplit(n_splits=1, test_size=0.5/0.525, random_state=np.random.RandomState(21))  # we use a RandomState instance, not an int, because we will reuse this splitter several times\n",
    "\n",
    "indices, sizes, pos_class = split_0d(splitter, inner_splitter)\n",
    "\n",
    "print(sizes)\n",
    "#print(pos_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_indices_and_stats(\n",
    "    indices, sizes, pos_class, split_dimension=0, save_indices=True, train_size=2.5, total_size=len(df), data_name=data_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = ShuffleSplit(n_splits=9, test_size=0.4875, random_state=22)\n",
    "inner_splitter = ShuffleSplit(n_splits=1, test_size=0.5/0.5125, random_state=np.random.RandomState(23))  # we use a RandomState instance, not an int, because we will reuse this splitter several times\n",
    "\n",
    "indices, sizes, pos_class = split_0d(splitter, inner_splitter)\n",
    "\n",
    "print(sizes)\n",
    "#print(pos_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_indices_and_stats(\n",
    "    indices, sizes, pos_class, split_dimension=0, save_indices=True, train_size=1.25, total_size=len(df), data_name=data_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = ShuffleSplit(n_splits=9, test_size=0.49375, random_state=24)\n",
    "inner_splitter = ShuffleSplit(n_splits=1, test_size=0.5/0.50625, random_state=np.random.RandomState(25))  # we use a RandomState instance, not an int, because we will reuse this splitter several times\n",
    "\n",
    "indices, sizes, pos_class = split_0d(splitter, inner_splitter)\n",
    "\n",
    "print(sizes)\n",
    "#print(pos_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_indices_and_stats(\n",
    "    indices, sizes, pos_class, split_dimension=0, save_indices=True, train_size=0.625, total_size=len(df), data_name=data_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1D split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_1d(splitter, inner_splitter):\n",
    "    indices = []\n",
    "    sizes = []\n",
    "    pos_class = []\n",
    "    unique_initiators = []\n",
    "    unique_monomers = []\n",
    "    unique_terminators = []\n",
    "    for idx_train_val, idx_test in splitter.split(list(range(len(df))), groups=df[\"I_long\"]):\n",
    "        # inner split\n",
    "        train, val = next(inner_splitter.split(idx_train_val, groups=df[\"I_long\"][idx_train_val]))\n",
    "        # use indices to index indices :P (we need to obtain indices referring to the original dataframe)\n",
    "        idx_train = idx_train_val[train]\n",
    "        idx_val = idx_train_val[val]\n",
    "        # add to list\n",
    "        indices.append((idx_train, idx_val, idx_test))\n",
    "        sizes.append((len(idx_train), len(idx_val), len(idx_test)))\n",
    "        pos_class.append(\n",
    "            (np.sum(df[['binary_A', 'binary_B', 'binary_C']].loc[idx_train]).to_numpy(), \n",
    "             np.sum(df[['binary_A', 'binary_B', 'binary_C']].loc[idx_val]).to_numpy(), \n",
    "             np.sum(df[['binary_A', 'binary_B', 'binary_C']].loc[idx_test]).to_numpy(),\n",
    "            )\n",
    "        )\n",
    "        unique_initiators.append((len(df['I_long'][idx_train].drop_duplicates()), len(df['I_long'][idx_val].drop_duplicates()), len(df['I_long'][idx_test].drop_duplicates())))\n",
    "        unique_monomers.append((len(df['M_long'][idx_train].drop_duplicates()), len(df['M_long'][idx_val].drop_duplicates()), len(df['M_long'][idx_test].drop_duplicates())))\n",
    "        unique_terminators.append((len(df['T_long'][idx_train].drop_duplicates()), len(df['T_long'][idx_val].drop_duplicates()), len(df['T_long'][idx_test].drop_duplicates())))\n",
    "    \n",
    "    return indices, sizes, pos_class, unique_initiators, unique_monomers, unique_terminators\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = GroupShuffleSplit(n_splits=9, test_size=0.1, random_state=26)\n",
    "inner_splitter = GroupShuffleSplit(n_splits=1, test_size=0.1/0.9, random_state=np.random.RandomState(27))  # we use a RandomState instance, not an int, because we will reuse this splitter several times\n",
    "\n",
    "indices, sizes, pos_class, unique_initiators, unique_monomers, unique_terminators = split_1d(splitter, inner_splitter)\n",
    "\n",
    "print(sizes)\n",
    "print(pos_class)\n",
    "print(unique_initiators)\n",
    "print(unique_monomers)\n",
    "print(unique_terminators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_indices_and_stats(\n",
    "    indices, \n",
    "    sizes, \n",
    "    pos_class,\n",
    "    total_size=len(df),\n",
    "    data_name=data_name,\n",
    "    split_dimension=1, \n",
    "    save_indices=True, \n",
    "    n_initiators=unique_initiators, \n",
    "    n_monomers=unique_monomers, \n",
    "    n_terminators=unique_terminators, \n",
    "    train_size=80\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = GroupShuffleSplit(n_splits=9, test_size=0.3, random_state=28)\n",
    "inner_splitter = GroupShuffleSplit(n_splits=1, test_size=0.3/0.7, random_state=np.random.RandomState(29))  # we use a RandomState instance, not an int, because we will reuse this splitter several times\n",
    "\n",
    "indices, sizes, pos_class, unique_initiators, unique_monomers, unique_terminators = split_1d(splitter, inner_splitter)\n",
    "\n",
    "print(sizes)\n",
    "print(pos_class)\n",
    "print(unique_initiators)\n",
    "print(unique_monomers)\n",
    "print(unique_terminators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_indices_and_stats(\n",
    "    indices, \n",
    "    sizes, \n",
    "    pos_class,\n",
    "    total_size=len(df),\n",
    "    data_name=data_name,\n",
    "    split_dimension=1, \n",
    "    save_indices=True, \n",
    "    n_initiators=unique_initiators, \n",
    "    n_monomers=unique_monomers, \n",
    "    n_terminators=unique_terminators, \n",
    "    train_size=40\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = GroupShuffleSplit(n_splits=9, test_size=0.4, random_state=30)\n",
    "inner_splitter = GroupShuffleSplit(n_splits=1, test_size=0.4/0.6, random_state=np.random.RandomState(31))  # we use a RandomState instance, not an int, because we will reuse this splitter several times\n",
    "\n",
    "indices, sizes, pos_class, unique_initiators, unique_monomers, unique_terminators = split_1d(splitter, inner_splitter)\n",
    "\n",
    "print(sizes)\n",
    "print(pos_class)\n",
    "print(unique_initiators)\n",
    "print(unique_monomers)\n",
    "print(unique_terminators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_indices_and_stats(\n",
    "    indices, \n",
    "    sizes, \n",
    "    pos_class,\n",
    "    total_size=len(df),\n",
    "    data_name=data_name,\n",
    "    split_dimension=1, \n",
    "    save_indices=True, \n",
    "    n_initiators=unique_initiators, \n",
    "    n_monomers=unique_monomers, \n",
    "    n_terminators=unique_terminators, \n",
    "    train_size=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = GroupShuffleSplit(n_splits=9, test_size=0.45, random_state=32)\n",
    "inner_splitter = GroupShuffleSplit(n_splits=1, test_size=0.45/0.55, random_state=np.random.RandomState(33))  # we use a RandomState instance, not an int, because we will reuse this splitter several times\n",
    "\n",
    "indices, sizes, pos_class, unique_initiators, unique_monomers, unique_terminators = split_1d(splitter, inner_splitter)\n",
    "\n",
    "print(sizes)\n",
    "print(pos_class)\n",
    "print(unique_initiators)\n",
    "print(unique_monomers)\n",
    "print(unique_terminators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_indices_and_stats(\n",
    "    indices, \n",
    "    sizes, \n",
    "    pos_class,\n",
    "    total_size=len(df),\n",
    "    data_name=data_name,\n",
    "    split_dimension=1, \n",
    "    save_indices=True, \n",
    "    n_initiators=unique_initiators, \n",
    "    n_monomers=unique_monomers, \n",
    "    n_terminators=unique_terminators, \n",
    "    train_size=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = GroupShuffleSplit(n_splits=9, test_size=0.45, random_state=34)\n",
    "inner_splitter = GroupShuffleSplit(n_splits=1, test_size=0.5/0.55, random_state=np.random.RandomState(35))  # we use a RandomState instance, not an int, because we will reuse this splitter several times\n",
    "\n",
    "indices, sizes, pos_class, unique_initiators, unique_monomers, unique_terminators = split_1d(splitter, inner_splitter)\n",
    "\n",
    "print(sizes)\n",
    "print(pos_class)\n",
    "print(unique_initiators)\n",
    "print(unique_monomers)\n",
    "print(unique_terminators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_indices_and_stats(\n",
    "    indices, \n",
    "    sizes, \n",
    "    pos_class,\n",
    "    total_size=len(df),\n",
    "    data_name=data_name,\n",
    "    split_dimension=1, \n",
    "    save_indices=True, \n",
    "    n_initiators=unique_initiators, \n",
    "    n_monomers=unique_monomers, \n",
    "    n_terminators=unique_terminators, \n",
    "    train_size=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = GroupShuffleSplit(n_splits=9, test_size=0.475, random_state=36)\n",
    "inner_splitter = GroupShuffleSplit(n_splits=1, test_size=0.5/0.525, random_state=np.random.RandomState(37))  # we use a RandomState instance, not an int, because we will reuse this splitter several times\n",
    "\n",
    "indices, sizes, pos_class, unique_initiators, unique_monomers, unique_terminators = split_1d(splitter, inner_splitter)\n",
    "\n",
    "print(sizes)\n",
    "print(pos_class)\n",
    "print(unique_initiators)\n",
    "print(unique_monomers)\n",
    "print(unique_terminators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_indices_and_stats(\n",
    "    indices, \n",
    "    sizes, \n",
    "    pos_class,\n",
    "    total_size=len(df),\n",
    "    data_name=data_name,\n",
    "    split_dimension=1, \n",
    "    save_indices=True, \n",
    "    n_initiators=unique_initiators, \n",
    "    n_monomers=unique_monomers, \n",
    "    n_terminators=unique_terminators, \n",
    "    train_size=2.5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2D split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_2d(splitter, inner_splitter):\n",
    "    indices = []\n",
    "    sizes = []\n",
    "    pos_class = []\n",
    "    unique_initiators = []\n",
    "    unique_monomers = []\n",
    "    unique_terminators = []\n",
    "    for idx_train_val, idx_test in splitter.split(df, groups=df[[\"I_long\", \"M_long_dia\"]]):\n",
    "        train, val = next(inner_splitter.split(df.iloc[idx_train_val], groups=df[[\"I_long\", \"M_long_dia\"]].iloc[idx_train_val]))\n",
    "        # use indices to index indices :P (we need to obtain indices referring to the original dataframe)\n",
    "        idx_train = idx_train_val[train]\n",
    "        idx_val = idx_train_val[val]\n",
    "        indices.append((idx_train, idx_val, idx_test))\n",
    "        sizes.append((len(idx_train), len(idx_val), len(idx_test)))\n",
    "        pos_class.append(\n",
    "            (np.sum(df[['binary_A', 'binary_B', 'binary_C']].loc[idx_train]).to_numpy(), \n",
    "             np.sum(df[['binary_A', 'binary_B', 'binary_C']].loc[idx_val]).to_numpy(), \n",
    "             np.sum(df[['binary_A', 'binary_B', 'binary_C']].loc[idx_test]).to_numpy(),\n",
    "            )\n",
    "        )\n",
    "        unique_initiators.append((len(df['I_long'][idx_train].drop_duplicates()), len(df['I_long'][idx_val].drop_duplicates()), len(df['I_long'][idx_test].drop_duplicates())))\n",
    "        unique_monomers.append((len(df['M_long_dia'][idx_train].drop_duplicates()), len(df['M_long_dia'][idx_val].drop_duplicates()), len(df['M_long'][idx_test].drop_duplicates())))\n",
    "        unique_terminators.append((len(df['T_long'][idx_train].drop_duplicates()), len(df['T_long'][idx_val].drop_duplicates()), len(df['T_long'][idx_test].drop_duplicates())))\n",
    "    \n",
    "    return indices, sizes, pos_class, unique_initiators, unique_monomers, unique_terminators\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = GroupShuffleSplitND(n_splits=9, test_size=0.1, random_state=38)\n",
    "inner_splitter = GroupShuffleSplitND(n_splits=1, test_size=0.1/0.9, random_state=np.random.RandomState(39))  # we use a RandomState instance, not an int, because we will reuse this splitter several times\n",
    "\n",
    "indices, sizes, pos_class, unique_initiators, unique_monomers, unique_terminators = split_2d(splitter, inner_splitter)\n",
    "\n",
    "print(sizes)\n",
    "print(pos_class)\n",
    "print(unique_initiators)\n",
    "print(unique_monomers)\n",
    "print(unique_terminators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_indices_and_stats(\n",
    "    indices, \n",
    "    sizes, \n",
    "    pos_class,\n",
    "    total_size=len(df),\n",
    "    data_name=data_name,\n",
    "    split_dimension=2, \n",
    "    save_indices=True, \n",
    "    n_initiators=unique_initiators, \n",
    "    n_monomers=unique_monomers, \n",
    "    n_terminators=unique_terminators, \n",
    "    train_size=80\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = GroupShuffleSplitND(n_splits=9, test_size=0.2, random_state=40)\n",
    "inner_splitter = GroupShuffleSplitND(n_splits=1, test_size=0.2/0.8, random_state=np.random.RandomState(41))  # we use a RandomState instance, not an int, because we will reuse this splitter several times\n",
    "\n",
    "indices, sizes, pos_class, unique_initiators, unique_monomers, unique_terminators = split_2d(splitter, inner_splitter)\n",
    "\n",
    "print(sizes)\n",
    "print(pos_class)\n",
    "print(unique_initiators)\n",
    "print(unique_monomers)\n",
    "print(unique_terminators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_indices_and_stats(\n",
    "    indices, \n",
    "    sizes, \n",
    "    pos_class,\n",
    "    total_size=len(df),\n",
    "    data_name=data_name,\n",
    "    split_dimension=2, \n",
    "    save_indices=True, \n",
    "    n_initiators=unique_initiators, \n",
    "    n_monomers=unique_monomers, \n",
    "    n_terminators=unique_terminators, \n",
    "    train_size=60\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = GroupShuffleSplitND(n_splits=9, test_size=0.3, random_state=42)\n",
    "inner_splitter = GroupShuffleSplitND(n_splits=1, test_size=0.3/0.7, random_state=np.random.RandomState(43))  # we use a RandomState instance, not an int, because we will reuse this splitter several times\n",
    "\n",
    "indices, sizes, pos_class, unique_initiators, unique_monomers, unique_terminators = split_2d(splitter, inner_splitter)\n",
    "\n",
    "print(sizes)\n",
    "print(pos_class)\n",
    "print(unique_initiators)\n",
    "print(unique_monomers)\n",
    "print(unique_terminators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_indices_and_stats(\n",
    "    indices, \n",
    "    sizes, \n",
    "    pos_class,\n",
    "    total_size=len(df),\n",
    "    data_name=data_name,\n",
    "    split_dimension=2, \n",
    "    save_indices=True, \n",
    "    n_initiators=unique_initiators, \n",
    "    n_monomers=unique_monomers, \n",
    "    n_terminators=unique_terminators, \n",
    "    train_size=40\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = GroupShuffleSplitND(n_splits=9, test_size=0.35, random_state=44)\n",
    "inner_splitter = GroupShuffleSplitND(n_splits=1, test_size=0.35/0.65, random_state=np.random.RandomState(45))  # we use a RandomState instance, not an int, because we will reuse this splitter several times\n",
    "\n",
    "indices, sizes, pos_class, unique_initiators, unique_monomers, unique_terminators = split_2d(splitter, inner_splitter)\n",
    "\n",
    "print(sizes)\n",
    "print(pos_class)\n",
    "print(unique_initiators)\n",
    "print(unique_monomers)\n",
    "print(unique_terminators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_indices_and_stats(\n",
    "    indices, \n",
    "    sizes, \n",
    "    pos_class,\n",
    "    total_size=len(df),\n",
    "    data_name=data_name,\n",
    "    split_dimension=2, \n",
    "    save_indices=True, \n",
    "    n_initiators=unique_initiators, \n",
    "    n_monomers=unique_monomers, \n",
    "    n_terminators=unique_terminators, \n",
    "    train_size=30\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = GroupShuffleSplitND(n_splits=9, test_size=0.4, random_state=46)\n",
    "inner_splitter = GroupShuffleSplitND(n_splits=1, test_size=0.4/0.6, random_state=np.random.RandomState(47))  # we use a RandomState instance, not an int, because we will reuse this splitter several times\n",
    "\n",
    "indices, sizes, pos_class, unique_initiators, unique_monomers, unique_terminators = split_2d(splitter, inner_splitter)\n",
    "\n",
    "print(sizes)\n",
    "print(pos_class)\n",
    "print(unique_initiators)\n",
    "print(unique_monomers)\n",
    "print(unique_terminators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_indices_and_stats(\n",
    "    indices, \n",
    "    sizes, \n",
    "    pos_class,\n",
    "    total_size=len(df),\n",
    "    data_name=data_name,\n",
    "    split_dimension=2, \n",
    "    save_indices=True, \n",
    "    n_initiators=unique_initiators, \n",
    "    n_monomers=unique_monomers, \n",
    "    n_terminators=unique_terminators, \n",
    "    train_size=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = GroupShuffleSplitND(n_splits=9, test_size=0.4, random_state=48)\n",
    "inner_splitter = GroupShuffleSplitND(n_splits=1, test_size=0.45/0.6, random_state=np.random.RandomState(49))  # we use a RandomState instance, not an int, because we will reuse this splitter several times\n",
    "\n",
    "indices, sizes, pos_class, unique_initiators, unique_monomers, unique_terminators = split_2d(splitter, inner_splitter)\n",
    "\n",
    "print(sizes)\n",
    "print(pos_class)\n",
    "print(unique_initiators)\n",
    "print(unique_monomers)\n",
    "print(unique_terminators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_indices_and_stats(\n",
    "    indices, \n",
    "    sizes, \n",
    "    pos_class,\n",
    "    total_size=len(df),\n",
    "    data_name=data_name,\n",
    "    split_dimension=2, \n",
    "    save_indices=True, \n",
    "    n_initiators=unique_initiators, \n",
    "    n_monomers=unique_monomers, \n",
    "    n_terminators=unique_terminators, \n",
    "    train_size=15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = GroupShuffleSplitND(n_splits=9, test_size=0.45, random_state=50)\n",
    "inner_splitter = GroupShuffleSplitND(n_splits=1, test_size=0.45/0.55, random_state=np.random.RandomState(51))  # we use a RandomState instance, not an int, because we will reuse this splitter several times\n",
    "\n",
    "indices, sizes, pos_class, unique_initiators, unique_monomers, unique_terminators = split_2d(splitter, inner_splitter)\n",
    "\n",
    "print(sizes)\n",
    "print(pos_class)\n",
    "print(unique_initiators)\n",
    "print(unique_monomers)\n",
    "print(unique_terminators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_indices_and_stats(\n",
    "    indices, \n",
    "    sizes, \n",
    "    pos_class,\n",
    "    total_size=len(df),\n",
    "    data_name=data_name,\n",
    "    split_dimension=2, \n",
    "    save_indices=True, \n",
    "    n_initiators=unique_initiators, \n",
    "    n_monomers=unique_monomers, \n",
    "    n_terminators=unique_terminators, \n",
    "    train_size=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = GroupShuffleSplitND(n_splits=9, test_size=0.45, random_state=52)\n",
    "inner_splitter = GroupShuffleSplitND(n_splits=1, test_size=0.475/0.55, random_state=np.random.RandomState(53))  # we use a RandomState instance, not an int, because we will reuse this splitter several times\n",
    "\n",
    "indices, sizes, pos_class, unique_initiators, unique_monomers, unique_terminators = split_2d(splitter, inner_splitter)\n",
    "\n",
    "print(sizes)\n",
    "print(pos_class)\n",
    "print(unique_initiators)\n",
    "print(unique_monomers)\n",
    "print(unique_terminators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_indices_and_stats(\n",
    "    indices, \n",
    "    sizes, \n",
    "    pos_class,\n",
    "    total_size=len(df),\n",
    "    data_name=data_name,\n",
    "    split_dimension=2, \n",
    "    save_indices=True, \n",
    "    n_initiators=unique_initiators, \n",
    "    n_monomers=unique_monomers, \n",
    "    n_terminators=unique_terminators, \n",
    "    train_size=7.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = GroupShuffleSplitND(n_splits=9, test_size=0.45, random_state=54)\n",
    "inner_splitter = GroupShuffleSplitND(n_splits=1, test_size=0.5/0.55, random_state=np.random.RandomState(55))  # we use a RandomState instance, not an int, because we will reuse this splitter several times\n",
    "\n",
    "indices, sizes, pos_class, unique_initiators, unique_monomers, unique_terminators = split_2d(splitter, inner_splitter)\n",
    "\n",
    "print(sizes)\n",
    "print(pos_class)\n",
    "print(unique_initiators)\n",
    "print(unique_monomers)\n",
    "print(unique_terminators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_indices_and_stats(\n",
    "    indices, \n",
    "    sizes, \n",
    "    pos_class,\n",
    "    total_size=len(df),\n",
    "    data_name=data_name,\n",
    "    split_dimension=2, \n",
    "    save_indices=True, \n",
    "    n_initiators=unique_initiators, \n",
    "    n_monomers=unique_monomers, \n",
    "    n_terminators=unique_terminators, \n",
    "    train_size=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = GroupShuffleSplitND(n_splits=9, test_size=0.48, random_state=56)\n",
    "inner_splitter = GroupShuffleSplitND(n_splits=1, train_size=2, random_state=np.random.RandomState(57))  # we use a RandomState instance, not an int, because we will reuse this splitter several times\n",
    "\n",
    "indices, sizes, pos_class, unique_initiators, unique_monomers, unique_terminators = split_2d(splitter, inner_splitter)\n",
    "\n",
    "print(sizes)\n",
    "print(pos_class)\n",
    "print(unique_initiators)\n",
    "print(unique_monomers)\n",
    "print(unique_terminators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_indices_and_stats(\n",
    "    indices, \n",
    "    sizes, \n",
    "    pos_class,\n",
    "    total_size=len(df),\n",
    "    data_name=data_name,\n",
    "    split_dimension=2, \n",
    "    save_indices=True, \n",
    "    n_initiators=unique_initiators, \n",
    "    n_monomers=unique_monomers, \n",
    "    n_terminators=unique_terminators, \n",
    "    train_size=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3D split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_3d(splitter, inner_splitter):\n",
    "    indices = []\n",
    "    sizes = []\n",
    "    pos_class = []\n",
    "    unique_initiators = []\n",
    "    unique_monomers = []\n",
    "    unique_terminators = []\n",
    "    for idx_train_val, idx_test in splitter.split(df, groups=df[[\"I_long\", \"M_long_dia\", \"T_long\"]]):\n",
    "        train, val = next(inner_splitter.split(df.iloc[idx_train_val], groups=df[[\"I_long\", \"M_long_dia\", \"T_long\"]].iloc[idx_train_val]))\n",
    "        # use indices to index indices :P (we need to obtain indices referring to the original dataframe)\n",
    "        idx_train = idx_train_val[train]\n",
    "        idx_val = idx_train_val[val]\n",
    "        indices.append((idx_train, idx_val, idx_test))\n",
    "        sizes.append((len(idx_train), len(idx_val), len(idx_test)))\n",
    "        pos_class.append(\n",
    "            (np.sum(df[['binary_A', 'binary_B', 'binary_C']].loc[idx_train]).to_numpy(), \n",
    "             np.sum(df[['binary_A', 'binary_B', 'binary_C']].loc[idx_val]).to_numpy(), \n",
    "             np.sum(df[['binary_A', 'binary_B', 'binary_C']].loc[idx_test]).to_numpy(),\n",
    "            )\n",
    "        )\n",
    "        unique_initiators.append((len(df['I_long'][idx_train].drop_duplicates()), len(df['I_long'][idx_val].drop_duplicates()), len(df['I_long'][idx_test].drop_duplicates())))\n",
    "        unique_monomers.append((len(df['M_long_dia'][idx_train].drop_duplicates()), len(df['M_long_dia'][idx_val].drop_duplicates()), len(df['M_long'][idx_test].drop_duplicates())))\n",
    "        unique_terminators.append((len(df['T_long'][idx_train].drop_duplicates()), len(df['T_long'][idx_val].drop_duplicates()), len(df['T_long'][idx_test].drop_duplicates())))\n",
    "\n",
    "    return indices, sizes, pos_class, unique_initiators, unique_monomers, unique_terminators\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = GroupShuffleSplitND(n_splits=9, test_size=0.1, random_state=np.random.RandomState(358))  # here, we reuse the outer splitter as well, so we use RandomState\n",
    "inner_splitter = GroupShuffleSplitND(n_splits=1, test_size=0.1/0.9, random_state=np.random.RandomState(359))  # we use a RandomState instance, not an int, because we will reuse this splitter several times\n",
    "\n",
    "indices, sizes, pos_class, unique_initiators, unique_monomers, unique_terminators = split_3d(splitter, inner_splitter)\n",
    "\n",
    "print(sizes)\n",
    "print(pos_class)\n",
    "print(unique_initiators)\n",
    "print(unique_monomers)\n",
    "print(unique_terminators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_indices_and_stats(\n",
    "    indices, \n",
    "    sizes, \n",
    "    pos_class,\n",
    "    total_size=len(df),\n",
    "    data_name=data_name,\n",
    "    split_dimension=3, \n",
    "    save_indices=True, \n",
    "    n_initiators=unique_initiators, \n",
    "    n_monomers=unique_monomers, \n",
    "    n_terminators=unique_terminators, \n",
    "    train_size=80\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = GroupShuffleSplitND(n_splits=9, test_size=0.15, random_state=np.random.RandomState(60))  # here, we reuse the outer splitter as well, so we use RandomState\n",
    "inner_splitter = GroupShuffleSplitND(n_splits=1, test_size=0.15/0.85, random_state=np.random.RandomState(61))  # we use a RandomState instance, not an int, because we will reuse this splitter several times\n",
    "\n",
    "indices, sizes, pos_class, unique_initiators, unique_monomers, unique_terminators = split_3d(splitter, inner_splitter)\n",
    "\n",
    "print(sizes)\n",
    "print(pos_class)\n",
    "print(unique_initiators)\n",
    "print(unique_monomers)\n",
    "print(unique_terminators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_indices_and_stats(\n",
    "    indices, \n",
    "    sizes, \n",
    "    pos_class,\n",
    "    total_size=len(df),\n",
    "    data_name=data_name,\n",
    "    split_dimension=3, \n",
    "    save_indices=True, \n",
    "    n_initiators=unique_initiators, \n",
    "    n_monomers=unique_monomers, \n",
    "    n_terminators=unique_terminators, \n",
    "    train_size=70\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = GroupShuffleSplitND(n_splits=9, test_size=0.2, random_state=np.random.RandomState(62))  # here, we reuse the outer splitter as well, so we use RandomState\n",
    "inner_splitter = GroupShuffleSplitND(n_splits=1, test_size=0.2/0.8, random_state=np.random.RandomState(63))  # we use a RandomState instance, not an int, because we will reuse this splitter several times\n",
    "\n",
    "indices, sizes, pos_class, unique_initiators, unique_monomers, unique_terminators = split_3d(splitter, inner_splitter)\n",
    "\n",
    "print(sizes)\n",
    "print(pos_class)\n",
    "print(unique_initiators)\n",
    "print(unique_monomers)\n",
    "print(unique_terminators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_indices_and_stats(\n",
    "    indices, \n",
    "    sizes, \n",
    "    pos_class,\n",
    "    total_size=len(df),\n",
    "    data_name=data_name,\n",
    "    split_dimension=3, \n",
    "    save_indices=True, \n",
    "    n_initiators=unique_initiators, \n",
    "    n_monomers=unique_monomers, \n",
    "    n_terminators=unique_terminators, \n",
    "    train_size=60\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = GroupShuffleSplitND(n_splits=9, test_size=0.25, random_state=np.random.RandomState(64))  # here, we reuse the outer splitter as well, so we use RandomState\n",
    "inner_splitter = GroupShuffleSplitND(n_splits=1, test_size=0.25/0.75, random_state=np.random.RandomState(65))  # we use a RandomState instance, not an int, because we will reuse this splitter several times\n",
    "\n",
    "indices, sizes, pos_class, unique_initiators, unique_monomers, unique_terminators = split_3d(splitter, inner_splitter)\n",
    "\n",
    "print(sizes)\n",
    "print(pos_class)\n",
    "print(unique_initiators)\n",
    "print(unique_monomers)\n",
    "print(unique_terminators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_indices_and_stats(\n",
    "    indices, \n",
    "    sizes, \n",
    "    pos_class,\n",
    "    total_size=len(df),\n",
    "    data_name=data_name,\n",
    "    split_dimension=3, \n",
    "    save_indices=True, \n",
    "    n_initiators=unique_initiators, \n",
    "    n_monomers=unique_monomers, \n",
    "    n_terminators=unique_terminators, \n",
    "    train_size=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = GroupShuffleSplitND(n_splits=9, test_size=0.3, random_state=np.random.RandomState(66))  # here, we reuse the outer splitter as well, so we use RandomState\n",
    "inner_splitter = GroupShuffleSplitND(n_splits=1, test_size=0.3/0.7, random_state=np.random.RandomState(67))  # we use a RandomState instance, not an int, because we will reuse this splitter several times\n",
    "\n",
    "indices, sizes, pos_class, unique_initiators, unique_monomers, unique_terminators = split_3d(splitter, inner_splitter)\n",
    "\n",
    "print(sizes)\n",
    "print(pos_class)\n",
    "print(unique_initiators)\n",
    "print(unique_monomers)\n",
    "print(unique_terminators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_indices_and_stats(\n",
    "    indices, \n",
    "    sizes, \n",
    "    pos_class,\n",
    "    total_size=len(df),\n",
    "    data_name=data_name,\n",
    "    split_dimension=3, \n",
    "    save_indices=True, \n",
    "    n_initiators=unique_initiators, \n",
    "    n_monomers=unique_monomers, \n",
    "    n_terminators=unique_terminators, \n",
    "    train_size=40\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = GroupShuffleSplitND(n_splits=9, test_size=0.33, random_state=np.random.RandomState(68))  # here, we reuse the outer splitter as well, so we use RandomState\n",
    "inner_splitter = GroupShuffleSplitND(n_splits=1, test_size=0.33/0.67, random_state=np.random.RandomState(69))  # we use a RandomState instance, not an int, because we will reuse this splitter several times\n",
    "\n",
    "indices, sizes, pos_class, unique_initiators, unique_monomers, unique_terminators = split_3d(splitter, inner_splitter)\n",
    "\n",
    "print(sizes)\n",
    "print(pos_class)\n",
    "print(unique_initiators)\n",
    "print(unique_monomers)\n",
    "print(unique_terminators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_indices_and_stats(\n",
    "    indices, \n",
    "    sizes, \n",
    "    pos_class,\n",
    "    total_size=len(df),\n",
    "    data_name=data_name,\n",
    "    split_dimension=3, \n",
    "    save_indices=True, \n",
    "    n_initiators=unique_initiators, \n",
    "    n_monomers=unique_monomers, \n",
    "    n_terminators=unique_terminators, \n",
    "    train_size=34\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = GroupShuffleSplitND(n_splits=9, test_size=0.35, random_state=np.random.RandomState(70))  # here, we reuse the outer splitter as well, so we use RandomState\n",
    "inner_splitter = GroupShuffleSplitND(n_splits=1, test_size=0.35/0.65, random_state=np.random.RandomState(71))  # we use a RandomState instance, not an int, because we will reuse this splitter several times\n",
    "\n",
    "indices, sizes, pos_class, unique_initiators, unique_monomers, unique_terminators = split_3d(splitter, inner_splitter)\n",
    "\n",
    "print(sizes)\n",
    "print(pos_class)\n",
    "print(unique_initiators)\n",
    "print(unique_monomers)\n",
    "print(unique_terminators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_indices_and_stats(\n",
    "    indices, \n",
    "    sizes, \n",
    "    pos_class,\n",
    "    total_size=len(df),\n",
    "    data_name=data_name,\n",
    "    split_dimension=3, \n",
    "    save_indices=True, \n",
    "    n_initiators=unique_initiators, \n",
    "    n_monomers=unique_monomers, \n",
    "    n_terminators=unique_terminators, \n",
    "    train_size=30\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = GroupShuffleSplitND(n_splits=9, test_size=0.35, random_state=np.random.RandomState(72))  # here, we reuse the outer splitter as well, so we use RandomState\n",
    "inner_splitter = GroupShuffleSplitND(n_splits=1, test_size=0.40/0.65, random_state=np.random.RandomState(73))  # we use a RandomState instance, not an int, because we will reuse this splitter several times\n",
    "\n",
    "indices, sizes, pos_class, unique_initiators, unique_monomers, unique_terminators = split_3d(splitter, inner_splitter)\n",
    "\n",
    "print(sizes)\n",
    "print(pos_class)\n",
    "print(unique_initiators)\n",
    "print(unique_monomers)\n",
    "print(unique_terminators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_indices_and_stats(\n",
    "    indices, \n",
    "    sizes, \n",
    "    pos_class,\n",
    "    total_size=len(df),\n",
    "    data_name=data_name,\n",
    "    split_dimension=3, \n",
    "    save_indices=True, \n",
    "    n_initiators=unique_initiators, \n",
    "    n_monomers=unique_monomers, \n",
    "    n_terminators=unique_terminators, \n",
    "    train_size=25\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = GroupShuffleSplitND(n_splits=9, test_size=0.40, random_state=np.random.RandomState(74))  # here, we reuse the outer splitter as well, so we use RandomState\n",
    "inner_splitter = GroupShuffleSplitND(n_splits=1, test_size=0.40/0.60, random_state=np.random.RandomState(75))  # we use a RandomState instance, not an int, because we will reuse this splitter several times\n",
    "\n",
    "indices, sizes, pos_class, unique_initiators, unique_monomers, unique_terminators = split_3d(splitter, inner_splitter)\n",
    "\n",
    "print(sizes)\n",
    "print(pos_class)\n",
    "print(unique_initiators)\n",
    "print(unique_monomers)\n",
    "print(unique_terminators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_indices_and_stats(\n",
    "    indices, \n",
    "    sizes, \n",
    "    pos_class,\n",
    "    total_size=len(df),\n",
    "    data_name=data_name,\n",
    "    split_dimension=3, \n",
    "    save_indices=True, \n",
    "    n_initiators=unique_initiators, \n",
    "    n_monomers=unique_monomers, \n",
    "    n_terminators=unique_terminators, \n",
    "    train_size=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = GroupShuffleSplitND(n_splits=9, test_size=0.40, random_state=np.random.RandomState(76))  # here, we reuse the outer splitter as well, so we use RandomState\n",
    "inner_splitter = GroupShuffleSplitND(n_splits=1, test_size=0.45/0.60, random_state=np.random.RandomState(77))  # we use a RandomState instance, not an int, because we will reuse this splitter several times\n",
    "\n",
    "indices, sizes, pos_class, unique_initiators, unique_monomers, unique_terminators = split_3d(splitter, inner_splitter)\n",
    "\n",
    "print(sizes)\n",
    "print(pos_class)\n",
    "print(unique_initiators)\n",
    "print(unique_monomers)\n",
    "print(unique_terminators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_indices_and_stats(\n",
    "    indices, \n",
    "    sizes, \n",
    "    pos_class,\n",
    "    total_size=len(df),\n",
    "    data_name=data_name,\n",
    "    split_dimension=3, \n",
    "    save_indices=True, \n",
    "    n_initiators=unique_initiators, \n",
    "    n_monomers=unique_monomers, \n",
    "    n_terminators=unique_terminators, \n",
    "    train_size=15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = GroupShuffleSplitND(n_splits=9, test_size=0.45, random_state=np.random.RandomState(778))  # here, we reuse the outer splitter as well, so we use RandomState\n",
    "inner_splitter = GroupShuffleSplitND(n_splits=1, test_size=0.45/0.55, random_state=np.random.RandomState(779))  # we use a RandomState instance, not an int, because we will reuse this splitter several times\n",
    "\n",
    "indices, sizes, pos_class, unique_initiators, unique_monomers, unique_terminators = split_3d(splitter, inner_splitter)\n",
    "\n",
    "print(sizes)\n",
    "print(pos_class)\n",
    "print(unique_initiators)\n",
    "print(unique_monomers)\n",
    "print(unique_terminators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_indices_and_stats(\n",
    "    indices, \n",
    "    sizes, \n",
    "    pos_class,\n",
    "    total_size=len(df),\n",
    "    data_name=data_name,\n",
    "    split_dimension=3, \n",
    "    save_indices=True, \n",
    "    n_initiators=unique_initiators, \n",
    "    n_monomers=unique_monomers, \n",
    "    n_terminators=unique_terminators, \n",
    "    train_size=10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthetic data splits\n",
    "\n",
    "### We don't do this for data `2024-04-18` as it was ineffective before and we don't have reason to believe this will change\n",
    "\n",
    "How do we go about this?\n",
    "\n",
    "For synthetic data, we will want to use only real data to evaluate. So the validation and test sets can only contain real data while in the training set, we combine real and synthetic data.\n",
    "\n",
    "To do this, we can split the combined data as usual, but remove synthetic data before saving validation and test sets by comparing to the indices of real data points (`synferm_dataset_2024-01-31_195037records_synthetic_real-indices.txt`) we saved when preparing the synthetic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data_filename = \"synferm_dataset_2024-01-31_195037records_synthetic.csv\"\n",
    "data_name = data_filename.rsplit(\"_\", maxsplit=1)[0]\n",
    "df = pd.read_csv(DATA_DIR / \"curated_data\" / data_filename).astype({\"binary_A\": int, \"binary_B\": int, \"binary_C\": int})\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# M_long_dia will be to sort diastereomers into the same group on group shuffle splits\n",
    "diastereomers = {\n",
    "    \"Mon001\": \"Mon087\",\n",
    "    \"Mon003\": \"Mon078\",\n",
    "    \"Mon011\": \"Mon088\",\n",
    "    \"Mon013\": \"Mon074\",\n",
    "    \"Mon014\": \"Mon090\",\n",
    "    \"Mon015\": \"Mon076\",\n",
    "    \"Mon016\": \"Mon096\",\n",
    "    \"Mon017\": \"Mon075\",\n",
    "    \"Mon019\": \"Mon091\",\n",
    "    \"Mon020\": \"Mon077\",\n",
    "    \"Mon080\": \"Mon010\",\n",
    "}\n",
    "df[\"M_long_dia\"] = df[\"M_long\"].replace(diastereomers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load indices that tell which data points are real\n",
    "with open(DATA_DIR / \"curated_data\" / \"synferm_dataset_2024-01-31_195037records_synthetic_real-indices.txt\", \"r\") as f:\n",
    "    real_idx = np.array([int(line.strip()) for line in f.readlines()])\n",
    "real_idx.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1D synthetic split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_1d_synthetic(splitter, inner_splitter):\n",
    "    indices = []\n",
    "    sizes = []\n",
    "    pos_class = []\n",
    "    unique_initiators = []\n",
    "    unique_monomers = []\n",
    "    unique_terminators = []\n",
    "    for idx_train_val, idx_test in splitter.split(list(range(len(df))), groups=df[\"I_long\"]):\n",
    "        # inner split\n",
    "        train, val = next(inner_splitter.split(idx_train_val, groups=df[\"I_long\"][idx_train_val]))\n",
    "        # use indices to index indices :P (we need to obtain indices referring to the original dataframe)\n",
    "        idx_train = idx_train_val[train]\n",
    "        idx_val = idx_train_val[val]\n",
    "        # eliminate all idx_val and idx_test that do not refer to real data\n",
    "        idx_val = np.intersect1d(idx_val, real_idx)  # n.b. a side effect is that the indices are sorted, but that does not matter as shuffling is controlled in the dataloader\n",
    "        idx_test = np.intersect1d(idx_test, real_idx)\n",
    "        # add to list\n",
    "        indices.append((idx_train, idx_val, idx_test))\n",
    "        sizes.append((len(idx_train), len(idx_val), len(idx_test)))\n",
    "        pos_class.append(\n",
    "            (np.sum(df[['binary_A', 'binary_B', 'binary_C']].loc[idx_train]).to_numpy(), \n",
    "             np.sum(df[['binary_A', 'binary_B', 'binary_C']].loc[idx_val]).to_numpy(), \n",
    "             np.sum(df[['binary_A', 'binary_B', 'binary_C']].loc[idx_test]).to_numpy(),\n",
    "            )\n",
    "        )\n",
    "        unique_initiators.append((len(df['I_long'][idx_train].drop_duplicates()), len(df['I_long'][idx_val].drop_duplicates()), len(df['I_long'][idx_test].drop_duplicates())))\n",
    "        unique_monomers.append((len(df['M_long'][idx_train].drop_duplicates()), len(df['M_long'][idx_val].drop_duplicates()), len(df['M_long'][idx_test].drop_duplicates())))\n",
    "        unique_terminators.append((len(df['T_long'][idx_train].drop_duplicates()), len(df['T_long'][idx_val].drop_duplicates()), len(df['T_long'][idx_test].drop_duplicates())))\n",
    "    \n",
    "    return indices, sizes, pos_class, unique_initiators, unique_monomers, unique_terminators\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = GroupShuffleSplit(n_splits=9, test_size=0.1, random_state=42)\n",
    "inner_splitter = GroupShuffleSplit(n_splits=1, test_size=0.1/0.9, random_state=np.random.RandomState(42))  # we use a RandomState instance, not an int, because we will reuse this splitter several times\n",
    "\n",
    "indices, sizes, pos_class, unique_initiators, unique_monomers, unique_terminators = split_1d_synthetic(splitter, inner_splitter)\n",
    "\n",
    "print(sizes)\n",
    "print()\n",
    "print(pos_class)\n",
    "print()\n",
    "print(unique_initiators)\n",
    "print(unique_monomers)\n",
    "print(unique_terminators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_indices_and_stats(\n",
    "    indices, \n",
    "    sizes, \n",
    "    pos_class,\n",
    "    total_size=len(df),\n",
    "    data_name=\"synferm_dataset_2024-01-31_synthetic\", \n",
    "    split_dimension=1, \n",
    "    save_indices=True, \n",
    "    n_initiators=unique_initiators, \n",
    "    n_monomers=unique_monomers, \n",
    "    n_terminators=unique_terminators, \n",
    "    train_size=80\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2D synthetic split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_2d_synthetic(splitter, inner_splitter):\n",
    "    indices = []\n",
    "    sizes = []\n",
    "    pos_class = []\n",
    "    unique_initiators = []\n",
    "    unique_monomers = []\n",
    "    unique_terminators = []\n",
    "    for idx_train_val, idx_test in splitter.split(df, groups=df[[\"I_long\", \"M_long_dia\"]]):\n",
    "        train, val = next(inner_splitter.split(df.iloc[idx_train_val], groups=df[[\"I_long\", \"M_long_dia\"]].iloc[idx_train_val]))\n",
    "        # use indices to index indices :P (we need to obtain indices referring to the original dataframe)\n",
    "        idx_train = idx_train_val[train]\n",
    "        idx_val = idx_train_val[val]\n",
    "        # eliminate all idx_val and idx_test that do not refer to real data\n",
    "        idx_val = np.intersect1d(idx_val, real_idx)  # n.b. a side effect is that the indices are sorted, but that does not matter as shuffling is controlled in the dataloader\n",
    "        idx_test = np.intersect1d(idx_test, real_idx)\n",
    "        # add to list\n",
    "        indices.append((idx_train, idx_val, idx_test))\n",
    "        sizes.append((len(idx_train), len(idx_val), len(idx_test)))\n",
    "        pos_class.append(\n",
    "            (np.sum(df[['binary_A', 'binary_B', 'binary_C']].loc[idx_train]).to_numpy(), \n",
    "             np.sum(df[['binary_A', 'binary_B', 'binary_C']].loc[idx_val]).to_numpy(), \n",
    "             np.sum(df[['binary_A', 'binary_B', 'binary_C']].loc[idx_test]).to_numpy(),\n",
    "            )\n",
    "        )\n",
    "        unique_initiators.append((len(df['I_long'][idx_train].drop_duplicates()), len(df['I_long'][idx_val].drop_duplicates()), len(df['I_long'][idx_test].drop_duplicates())))\n",
    "        unique_monomers.append((len(df['M_long_dia'][idx_train].drop_duplicates()), len(df['M_long_dia'][idx_val].drop_duplicates()), len(df['M_long'][idx_test].drop_duplicates())))\n",
    "        unique_terminators.append((len(df['T_long'][idx_train].drop_duplicates()), len(df['T_long'][idx_val].drop_duplicates()), len(df['T_long'][idx_test].drop_duplicates())))\n",
    "    \n",
    "    return indices, sizes, pos_class, unique_initiators, unique_monomers, unique_terminators\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = GroupShuffleSplitND(n_splits=9, test_size=0.2, random_state=42)\n",
    "inner_splitter = GroupShuffleSplitND(n_splits=1, test_size=0.2/0.8, random_state=np.random.RandomState(4))  # we use a RandomState instance, not an int, because we will reuse this splitter several times\n",
    "\n",
    "indices, sizes, pos_class, unique_initiators, unique_monomers, unique_terminators = split_2d_synthetic(splitter, inner_splitter)\n",
    "\n",
    "print(sizes)\n",
    "print(pos_class)\n",
    "print(unique_initiators)\n",
    "print(unique_monomers)\n",
    "print(unique_terminators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_indices_and_stats(\n",
    "    indices, \n",
    "    sizes, \n",
    "    pos_class,\n",
    "    total_size=len(df),\n",
    "    data_name=\"synferm_dataset_2024-01-31_synthetic\", \n",
    "    split_dimension=2, \n",
    "    save_indices=True, \n",
    "    n_initiators=unique_initiators, \n",
    "    n_monomers=unique_monomers, \n",
    "    n_terminators=unique_terminators, \n",
    "    train_size=60,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3D synthetic split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_3d_synthetic(splitter, inner_splitter):\n",
    "    indices = []\n",
    "    sizes = []\n",
    "    pos_class = []\n",
    "    unique_initiators = []\n",
    "    unique_monomers = []\n",
    "    unique_terminators = []\n",
    "    for idx_train_val, idx_test in splitter.split(df, groups=df[[\"I_long\", \"M_long_dia\", \"T_long\"]]):\n",
    "        train, val = next(inner_splitter.split(df.iloc[idx_train_val], groups=df[[\"I_long\", \"M_long_dia\", \"T_long\"]].iloc[idx_train_val]))\n",
    "        # use indices to index indices :P (we need to obtain indices referring to the original dataframe)\n",
    "        idx_train = idx_train_val[train]\n",
    "        idx_val = idx_train_val[val]\n",
    "        # eliminate all idx_val and idx_test that do not refer to real data\n",
    "        idx_val = np.intersect1d(idx_val, real_idx)  # n.b. a side effect is that the indices are sorted, but that does not matter as shuffling is controlled in the dataloader\n",
    "        idx_test = np.intersect1d(idx_test, real_idx)\n",
    "        # add to list\n",
    "        indices.append((idx_train, idx_val, idx_test))\n",
    "        sizes.append((len(idx_train), len(idx_val), len(idx_test)))\n",
    "        pos_class.append(\n",
    "            (np.sum(df[['binary_A', 'binary_B', 'binary_C']].loc[idx_train]).to_numpy(), \n",
    "             np.sum(df[['binary_A', 'binary_B', 'binary_C']].loc[idx_val]).to_numpy(), \n",
    "             np.sum(df[['binary_A', 'binary_B', 'binary_C']].loc[idx_test]).to_numpy(),\n",
    "            )\n",
    "        )\n",
    "        unique_initiators.append((len(df['I_long'][idx_train].drop_duplicates()), len(df['I_long'][idx_val].drop_duplicates()), len(df['I_long'][idx_test].drop_duplicates())))\n",
    "        unique_monomers.append((len(df['M_long_dia'][idx_train].drop_duplicates()), len(df['M_long_dia'][idx_val].drop_duplicates()), len(df['M_long'][idx_test].drop_duplicates())))\n",
    "        unique_terminators.append((len(df['T_long'][idx_train].drop_duplicates()), len(df['T_long'][idx_val].drop_duplicates()), len(df['T_long'][idx_test].drop_duplicates())))\n",
    "\n",
    "    return indices, sizes, pos_class, unique_initiators, unique_monomers, unique_terminators\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = GroupShuffleSplitND(n_splits=9, test_size=0.2, random_state=np.random.RandomState(42))  # here, we reuse the outer splitter as well, so we use RandomState\n",
    "inner_splitter = GroupShuffleSplitND(n_splits=1, test_size=0.2/0.8, random_state=np.random.RandomState(42))  # we use a RandomState instance, not an int, because we will reuse this splitter several times\n",
    "\n",
    "indices, sizes, pos_class, unique_initiators, unique_monomers, unique_terminators = split_3d_synthetic(splitter, inner_splitter)\n",
    "\n",
    "print(sizes)\n",
    "print(pos_class)\n",
    "print(unique_initiators)\n",
    "print(unique_monomers)\n",
    "print(unique_terminators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_indices_and_stats(\n",
    "    indices, \n",
    "    sizes, \n",
    "    pos_class,\n",
    "    total_size=len(df),\n",
    "    data_name=\"synferm_dataset_2024-01-31_synthetic\", \n",
    "    split_dimension=3, \n",
    "    save_indices=True, \n",
    "    n_initiators=unique_initiators, \n",
    "    n_monomers=unique_monomers, \n",
    "    n_terminators=unique_terminators, \n",
    "    train_size=60,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
