{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Initial attempt to learn from SF data\n",
    "Just a prototype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import sqlite3\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import cm\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem.Descriptors import rdMolDescriptors\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, f1_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sys.path.append(str(pathlib.Path().resolve().parents[1]))\n",
    "\n",
    "from src.definitions import DB_PATH\n",
    "from src.util.db_utils import SynFermDatabaseConnection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = SynFermDatabaseConnection()  # we will use this for various simple queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "con = sqlite3.connect(DB_PATH)\n",
    "df_full = pd.read_sql('SELECT * FROM experiments', con, index_col='id', coerce_float=False)\n",
    "con.close()\n",
    "print(f'Number of reactions (in total): {len(df_full)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select experiment numbers for further analysis\n",
    "start_exp_nr = 4  # kick out invalid experiments (SOP changed after exp3)\n",
    "end_exp_nr = 29  # (inclusive)\n",
    "\n",
    "df_full = df_full.loc[df_full['exp_nr'].between(start_exp_nr, end_exp_nr)]  \n",
    "                      \n",
    "print(f'Number of reactions considered here: {len(df_full)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# doublecheck that there are no \"empty\" experiments where results are missing\n",
    "df_full[df_full[\"product_A_lcms_ratio\"].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full.loc[~df_full[\"valid\"].str.contains(\"ERROR\", na=False), \"valid\"].value_counts().index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full[\"valid\"].isna().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove everything that says ERROR\n",
    "df_full = df_full.loc[~df_full[\"valid\"].str.contains(\"ERROR\", na=False)]\n",
    "len(df_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot everything with a value > 0\n",
    "df_full.loc[df_full[\"product_A_lcms_ratio\"].between(1e-10,1), \"product_A_lcms_ratio\"].plot.hist(bins=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We assign classes 0 (failure) and 1 (success) by >0\n",
    "\n",
    "df_full[\"product_A_outcome\"] = 0\n",
    "df_full.loc[df_full[\"product_A_lcms_ratio\"] > 0, \"product_A_outcome\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full[\"product_A_outcome\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pos class\n",
    "df_full[\"product_A_outcome\"].value_counts()[1] / len(df_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptor_names = list(rdMolDescriptors.Properties.GetAvailableProperties())\n",
    "\n",
    "get_descriptors = rdMolDescriptors.Properties(descriptor_names)\n",
    "\n",
    "def smi_to_descriptors(smile):\n",
    "    mol = Chem.MolFromSmiles(smile)\n",
    "    descriptors = []\n",
    "    if mol:\n",
    "        descriptors = np.array(get_descriptors.ComputeProperties(mol))\n",
    "    return descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full['descriptors'] = df_full[\"product_A_smiles\"].apply(smi_to_descriptors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred = df_full.dropna(subset=[\"descriptors\", \"product_A_outcome\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract our data\n",
    "X = np.stack(df_pred[\"descriptors\"].values.tolist())\n",
    "y = df_pred[\"product_A_outcome\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-test split (produce indices)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hparam search\n",
    "val_scores = []\n",
    "Cs = np.logspace(-3, 4, num=100)\n",
    "for C in Cs:\n",
    "    model = LogisticRegression(C=C, solver=\"newton-cholesky\")\n",
    "    model.fit(X_train, y_train)\n",
    "    y_hat = model.predict(X_val)\n",
    "    val_scores.append(accuracy_score(y_val, y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show hparam search results\n",
    "plt.plot(Cs, val_scores)\n",
    "plt.xscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best C\n",
    "best_C = Cs[np.argmax(val_scores)]\n",
    "print(best_C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# refit best model and evaluate on test set\n",
    "model = LogisticRegression(C=best_C, solver=\"newton-cholesky\")\n",
    "model.fit(np.concatenate((X_train, X_val)), np.concatenate((y_train, y_val)))\n",
    "y_hat = model.predict(X_test)\n",
    "\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_hat)}\")\n",
    "print(f\"Balanced Accuracy: {np.sqrt(balanced_accuracy_score(y_test, y_hat))}\")\n",
    "print(f\"F1-score: {np.sqrt(f1_score(y_test, y_hat))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(3,2.67))\n",
    "sns.heatmap(confusion_matrix(y_test, y_hat), annot=True, fmt=\"01\", cmap=sns.color_palette(\"mako_r\", as_cmap=True))\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"confusion_matrix_LogReg_props.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get highest coefficients\n",
    "highest_coefs = np.argsort(np.abs(model.coef_.flatten()))[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show most extreme coefficients with their names\n",
    "for idx in highest_coefs[:5]:\n",
    "    print(descriptor_names[idx])\n",
    "    print(model.coef_.flatten()[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function\n",
    "def iterable_to_nested_list(list_to_break_up, inner_len, pad_last=False, pad_value=None):\n",
    "    \"\"\"Break up list into nested lists of given length\"\"\"\n",
    "    # invert list for fast pop()\n",
    "    list_tmp = list(reversed(list_to_break_up))\n",
    "\n",
    "    outer_list = []\n",
    "    while len(list_tmp) > 0:\n",
    "        inner_list = []\n",
    "        while len(inner_list) < inner_len:\n",
    "            if len(list_tmp) == 0:\n",
    "                if pad_last:\n",
    "                    while len(inner_list) < inner_len:\n",
    "                        inner_list.append(pad_value)\n",
    "                break\n",
    "            inner_list.append(list_tmp.pop())\n",
    "        outer_list.append(inner_list)\n",
    "    return outer_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show most extreme descriptors\n",
    "fig, ax = plt.subplots(figsize=(18,4))\n",
    "ax.axis('tight')\n",
    "ax.axis('off')\n",
    "\n",
    "norm = plt.Normalize()\n",
    "colors = iterable_to_nested_list(cm.PuOr(norm(model.coef_.flatten())), 5,True,  np.array([1.,1.,1.,1.]))\n",
    "\n",
    "ax.table(cellText = iterable_to_nested_list(descriptor_names, 5, True, \"-\"), cellColours=colors, loc='center')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's train a different model instead: OHE of reactants + FFN\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ohe = OneHotEncoder(sparse_output=False, handle_unknown=\"infrequent_if_exist\")\n",
    "df_OHE = df_pred[[\"initiator\", \"monomer\", \"terminator\"]]\n",
    "df_OHE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_OHE.values, y)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_OHE = ohe.fit_transform(X_train)\n",
    "X_val_OHE = ohe.transform(X_val)\n",
    "X_test_OHE = ohe.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFN(torch.nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        self.W_i = torch.nn.Linear(input_size, 600, bias=True)\n",
    "        self.W_h = torch.nn.Linear(600, 400, bias=True)\n",
    "        self.W_o = torch.nn.Linear(400, 1, bias=False)\n",
    "        self.activation = torch.nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.W_o(self.activation(self.W_h(self.activation(self.W_i(x)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SFDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FFN(182)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = SFDataset(X_train_OHE, y_train)\n",
    "dataset_val = SFDataset(X_val_OHE, y_val)\n",
    "dataset_test = SFDataset(X_test_OHE, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loader = torch.utils.data.DataLoader(dataset_train, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_loader = torch.utils.data.DataLoader(dataset_val, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(training_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor([0,1]).reshape(-1,1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch_index):\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "\n",
    "    # Here, we use enumerate(training_loader) instead of\n",
    "    # iter(training_loader) so that we can track the batch\n",
    "    # index and do some intra-epoch reporting\n",
    "    for i, data in enumerate(training_loader):\n",
    "        # Every data instance is an input + label pair\n",
    "        inputs, labels = data\n",
    "\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Make predictions for this batch\n",
    "        outputs = model(inputs)\n",
    "        # Compute the loss and its gradients\n",
    "        loss = loss_fn(outputs, labels.reshape(-1, 1))\n",
    "        \n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        last_loss = running_loss / (i+1) # loss per batch\n",
    "        #print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "\n",
    "    return last_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 100\n",
    "\n",
    "best_vloss = 1_000_000.\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print('EPOCH {}:'.format(epoch + 1))\n",
    "\n",
    "    # Make sure gradient tracking is on, and do a pass over the data\n",
    "    model.train(True)\n",
    "    avg_loss = train_one_epoch(epoch)\n",
    "\n",
    "    # We don't need gradients on to do reporting\n",
    "    model.train(False)\n",
    "\n",
    "    running_vloss = 0.0\n",
    "    for i, vdata in enumerate(validation_loader):\n",
    "        vinputs, vlabels = vdata\n",
    "        voutputs = model(vinputs)\n",
    "        vloss = loss_fn(voutputs, vlabels.reshape(-1,1))\n",
    "        running_vloss += vloss\n",
    "\n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "    print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "\n",
    "    \n",
    "\n",
    "    # Track best performance, and save the model's state\n",
    "    if avg_vloss < best_vloss:\n",
    "        best_vloss = avg_vloss\n",
    "        model_path = 'model_{}'.format(epoch)\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = FFN(182)\n",
    "best_model.load_state_dict(torch.load(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    pred = torch.sigmoid(model(dataset_test.X))\n",
    "    print(pred)\n",
    "    predicted, actual = (pred > 0.5).float(), dataset_test.y\n",
    "    print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "(pred > 0.5).count_nonzero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(predicted, actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pos class in test set\n",
    "dataset_test.y.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(3,2.33))\n",
    "sns.heatmap(confusion_matrix(predicted, actual), vmin=0, vmax=len(dataset_test), annot=True, fmt=\"01\", cmap=sns.color_palette(\"mako_r\", as_cmap=True))\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"confusion_matrix_FFN_OHE.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(predicted, actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_accuracy_score(predicted, actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
