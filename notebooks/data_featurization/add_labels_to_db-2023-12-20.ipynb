{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add labels to DB\n",
    "For every _valid_ experiment, add labels to a separate table.\n",
    "We use the following labels:\n",
    "- scaled MS responses for all products\n",
    "- binary outcome for all products\n",
    "- major product (A, B, C, or no_product)\n",
    "\n",
    "We use the methods for scaling that were established in the exploratory data analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling the LCMS responses\n",
    "- It seems to make sense to use some kind of robust scaling as we definitely expect outliers in LCMS data.\n",
    "- We do not require centering, as the data starts at 0 and is not normally distributed.\n",
    "- We could divide by the interquartile range (IQR) to get a robust scaling.\n",
    "- We could also use the median absolute deviation (MAD) to get a robust scaling.\n",
    "- However, both above ideas seem more suitable for symmetric distributions, while we have a highly skewed distribution with a defined minimum at 0.\n",
    "- Therefore, we could just divide by, say, the 90th percentile to get the equivalent of min-max scaling but robust to outliers.\n",
    "- Which percentile we choose, depends on the number of outliers we expect. The 90th percentile seems reasonable as we already remove internal standard errors and we can expect less than 10% of compounds to be outliers by virtue of ionizability alone. At the same time, choosing the 90th means we expect all products to form in at least 10% of the reactions.\n",
    "- To not be dependent on how often a product forms: Use a lower (say 80th) percentile, but use only non-zero values in calculating the percentile.\n",
    "\n",
    "### An extension to the scaling idea\n",
    "Assume we have achieved robust scaling. There remains a problem:\n",
    "- Consider product A systematically ionizes 10 times better than product B.\n",
    "- Further, consider a reaction where we measure a non-zero response for product A that is at the lower detection limit\n",
    "- It follows that we have no way to tell whether product B was formed or not, we only know it was not formed 10 times as much as A.\n",
    "\n",
    "In general, if product A ionized better than product B by a factor of x, we need to see a response for product A exceeding the lower detection limit by a factor of x to conclude that product A is the major product.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import sys\n",
    "from typing import Tuple, Union\n",
    "\n",
    "sys.path.append(str(pathlib.Path().resolve().parents[1]))\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from src.util.db_utils import SynFermDatabaseConnection\n",
    "from src.definitions import DATA_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = SynFermDatabaseConnection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Â get only valid experiments from DB\n",
    "data = con.con.execute(\"SELECT * FROM experiments WHERE exp_nr BETWEEN 4 AND 29 AND (valid NOT LIKE '%ERROR%' OR valid IS NULL)\").fetchall()\n",
    "df = pd.DataFrame(data, columns=[c[1] for c in con.con.execute(\"PRAGMA table_info(experiments)\").fetchall()])\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def robust_scale(x: np.ndarray, percentile: int = 75, return_scaling_factor=False) -> Union[np.ndarray, Tuple[np.ndarray, float]]:\n",
    "    \"\"\"\n",
    "    Robust scaling of a numpy array by dividing by the value for a percentile.\n",
    "    The percentile is calculated from all non-zero values.\n",
    "    \"\"\"\n",
    "    x = x.copy()\n",
    "    scaling_factor = np.percentile(x[x > 0], percentile)\n",
    "    x /= scaling_factor\n",
    "    if return_scaling_factor:\n",
    "        return x, scaling_factor\n",
    "    else:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# before we apply scaling, we need to verify how sensitive the scaling is to our choice of the percentile.\n",
    "factors = []\n",
    "for percentile in range(1, 100):\n",
    "    scaled_arrs = []\n",
    "    scaling_factors = []\n",
    "    for s in \"ABCDEFGH\":\n",
    "        arr, factor = robust_scale(df[f'product_{s}_lcms_ratio'].values, percentile=percentile, return_scaling_factor=True)\n",
    "        scaled_arrs.append(arr)\n",
    "        scaling_factors.append(factor)\n",
    "    factors.append(scaling_factors / scaling_factors[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the relative scaling factors for each product (A is always 1) for all percentiles between [1, 99]\n",
    "plt.figure(figsize=(7.5,5))\n",
    "plt.plot(factors)\n",
    "plt.xlabel(\"Percentile\")\n",
    "plt.ylabel(\"Scaling factor, relative to A\")\n",
    "plt.axvline(85, color=\"red\", linestyle=\"--\", label=\"85th\")\n",
    "plt.legend(\"ABCDEFGH\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result of the percentile/scaling factor analysis\n",
    "This outcome is good. The range of the scaling factors is mostly 2x - 3x. This seems reasonable given that the molecules always share quite a few residues. Also, except product D, all relative factors are quite stable w.r.t. the choice of percentile. In particular, relative A,B,C are stable. As we expect, in the very high percentiles (>95 for most, >90 for E), the scaling factor becomes unstable as it is now dominated by outliers.\n",
    "\n",
    "**Based on the plot, we choose the 85th percentile**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply scaling to all products and show scaling factors for control\n",
    "scaling_factors = []\n",
    "for s in \"ABCDEFGH\":\n",
    "        arr, factor = robust_scale(df[f'product_{s}_lcms_ratio'].values, percentile=85, return_scaling_factor=True)\n",
    "        df[f'scaled_{s}'] = arr\n",
    "        scaling_factors.append(factor)\n",
    "print(\"Scaling factors relative to A:\")\n",
    "for s, i in zip(\"ABCDEFGH\", scaling_factors):\n",
    "    print(f\"{s}: {i/scaling_factors[0]:.3f}\")\n",
    "# save the scaling factors for later use\n",
    "pd.DataFrame(scaling_factors, columns=[\"factor\"], index=list(\"ABCDEFGH\"))\\\n",
    "    .reset_index(names=\"product_type\")\\\n",
    "    .to_csv(DATA_DIR / \"scaling-factors_2023-12-20.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add binary outcome: 1 if product was formed, 0 if not\n",
    "df[\"binary_A\"] = (df[\"scaled_A\"] > 0).astype(int)\n",
    "df[\"binary_B\"] = (df[\"scaled_B\"] > 0).astype(int)\n",
    "df[\"binary_C\"] = (df[\"scaled_C\"] > 0).astype(int)\n",
    "df[\"binary_D\"] = (df[\"scaled_D\"] > 0).astype(int)\n",
    "df[\"binary_E\"] = (df[\"scaled_E\"] > 0).astype(int)\n",
    "df[\"binary_F\"] = (df[\"scaled_F\"] > 0).astype(int)\n",
    "df[\"binary_G\"] = (df[\"scaled_G\"] > 0).astype(int)\n",
    "df[\"binary_H\"] = (df[\"scaled_H\"] > 0).astype(float)\n",
    "df.loc[df[\"scaled_H\"].isna(), \"binary_H\"] = pd.NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign the main product\n",
    "df[\"main_product\"] = df[[\"scaled_A\", \"scaled_B\", \"scaled_C\"]].idxmax(axis=1).str.replace(\"scaled_\", \"\")\n",
    "# are there any reactions where neither A,nor B, nor C appear?\n",
    "df.loc[df[[\"scaled_A\", \"scaled_B\", \"scaled_C\"]].sum(axis=1) == 0, \"main_product\"] = \"no_product\"\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to 'labels' table of DB\n",
    "with con.con:\n",
    "    con.con.executemany('INSERT INTO labels (experiment_id, scaled_A, scaled_B, scaled_C, scaled_D, scaled_E, scaled_F, scaled_G, scaled_H, binary_A, binary_B, binary_C, binary_D, binary_E, binary_F, binary_G, binary_H, \"major_A-C\") VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?);',\n",
    "                        df[[\"id\", \"scaled_A\", \"scaled_B\", \"scaled_C\", \"scaled_D\", \"scaled_E\", \"scaled_F\", \"scaled_G\", \"scaled_H\", \"binary_A\", \"binary_B\", \"binary_C\", \"binary_D\", \"binary_E\", \"binary_F\", \"binary_G\", \"binary_H\", \"main_product\"]].values.tolist()\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
