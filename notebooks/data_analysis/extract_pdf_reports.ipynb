{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "261b6b1e-9b30-4a3f-9684-190bbbd48df0",
   "metadata": {},
   "source": [
    "# Extract MoBiAS PDF reports\n",
    "\n",
    "The PDF reports are a great source of additional information as they contain data for peaks that may not have been assigned to any known product.\n",
    "Mining these reports is much faster than reprocessing of the raw LCMS data.\n",
    "\n",
    "Here, we are in particular interested to identify\n",
    "1. leftover starting materials\n",
    "2. systematic side products that we have not been previously looking for\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5414c150-132b-4c8d-989f-f84b6f1cb3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pathlib\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "sys.path.insert(0, str(pathlib.Path().resolve().parents[1]))\n",
    "\n",
    "from pypdf import PdfReader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from src.util.db_utils import SynFermDatabaseConnection\n",
    "from src.definitions import DATA_DIR, PLATE_LIST_PATH\n",
    "from src.util.rdkit_util import smiles_to_lcms_mass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f737ec6-a3cf-45da-951f-34987a5335b8",
   "metadata": {},
   "source": [
    "## Extract PDF data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42e6e3fb-fa8c-4133-b15e-9e32e554d23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "con = SynFermDatabaseConnection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e225f6ff-be47-47f0-bcba-7f9db2bc0ea2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def import_lcms_full_report(path):\n",
    "    # set up pdf reader\n",
    "    reader = PdfReader(path)\n",
    "    number_of_pages = len(reader.pages)\n",
    "    lines = []\n",
    "    found_data = False\n",
    "    # iterate from second page until entire peak summary table is read completely\n",
    "    for i in range(1, number_of_pages):\n",
    "        page = reader.pages[i]\n",
    "        text = page.extract_text()\n",
    "        line_list = text.splitlines()\n",
    "        if (line_list[0] != '# RT [min] Area I S/N Max. m/z FWHM [min] Area % Int. %') and found_data:  # stop when not encountering another data header\n",
    "            break\n",
    "        else:\n",
    "            found_data = True\n",
    "        lines += line_list\n",
    "    \n",
    "    if len(lines) == 0:\n",
    "        raise RuntimeError(\"No data extracted\")\n",
    "        \n",
    "    # remove spaces in header\n",
    "    cleaned_lines = ['# RT[min] Area I S/N max_m/z FWHM[min] Area% Int%',]\n",
    "    # remove footers and data headers on not-first page\n",
    "    cleaned_lines += [line for line in lines if line[0].isnumeric()] \n",
    "\n",
    "    # split the lines into individual fields\n",
    "    data = [line.split() for line in cleaned_lines]\n",
    "    \n",
    "    # assemble DataFrame from data\n",
    "    df = pd.DataFrame(data[1:], columns=data[0]).astype(\"float\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2cca20-6f31-40b0-8469-900526543a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture output\n",
    "# import the plate list to obtain LCMS identifier - plate_nr relation\n",
    "plate_list = pd.read_csv(PLATE_LIST_PATH)\n",
    "# we will record any files that cause exceptions for manual inspection\n",
    "files_with_exceptions = []\n",
    "\n",
    "# iterate over all experiments/plates\n",
    "for exp_nr in range(1, 30):\n",
    "    for plate_nr in range(1, 7):\n",
    "        # print progress indicator\n",
    "        now = datetime.now()\n",
    "        print(f\"exp {exp_nr}-{plate_nr}, started {now.strftime('%H:%M:%S')}\")\n",
    " \n",
    "        \n",
    "        lcms_id = plate_list.loc[(plate_list[\"exp_nr\"] == exp_nr) & (plate_list[\"plate_nr\"] == plate_nr), \"results_file_name\"].item().split(\"_\")[0]\n",
    "        exp_path = DATA_DIR / \"pdf_reports\" / lcms_id\n",
    "        full_report_paths = list(exp_path.glob(\"*_LCMS_Fullreport.pdf\"))\n",
    "        \n",
    "        for path in full_report_paths:\n",
    "            try:\n",
    "                # get well from filename\n",
    "                regex = r'_P\\d{1}-[A-Z]-\\d{1,2}_'\n",
    "                match = re.search(regex, path.name)\n",
    "                well = \"\".join(match.group().strip(\"_\").split(\"-\")[1:])\n",
    "                reaction_id = con.get_reaction_id((exp_nr, plate_nr, well))\n",
    "                df = import_lcms_full_report(path)\n",
    "                # we persist this to the database for re-use\n",
    "                # first reformat df to fit DB\n",
    "                df.insert(0, \"reaction_id\", reaction_id)\n",
    "                df.insert(2, \"retention_time_s\", (df[\"RT[min]\"] * 60).astype(\"int\"))\n",
    "                df = df.drop(columns=\"RT[min]\")\n",
    "                df = df.rename(columns={\"#\": \"peak_number\", \n",
    "                           \"Area\": \"area\", \n",
    "                           \"I\": \"intensity\",\n",
    "                           \"S/N\": \"signal_to_noise\",\n",
    "                           \"max_m/z\": \"mz_max\",\n",
    "                           \"FWHM[min]\": \"fwhm_min\", \n",
    "                           \"Area%\": \"%area\",\n",
    "                           \"Int%\": \"%intensity\"\n",
    "                          }).astype({\"peak_number\": \"int\",\n",
    "                                     \"area\": \"int\",\n",
    "                                     \"intensity\": \"int\"})\n",
    "                # write all extracted peaks to DB\n",
    "                with con.con:\n",
    "                    con.con.executemany(\n",
    "                        'INSERT INTO lcms_peaks (experiment_id, peak_nr, retention_time_s, area, intensity, signal_to_noise, mz_max, fwhm_min, \"%area\", \"%intensity\") VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?);', \n",
    "                        [tuple(row) for row in df.to_numpy()]\n",
    "                    )\n",
    "            except Exception as e:\n",
    "                print(f\"Something went wrong for {str(path)}\")\n",
    "                files_with_exceptions.append(path)\n",
    "                print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7d73605-486c-47cf-a5cd-c09af7a43e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exp 1-1, started 16:08:03\n",
      "exp 1-2, started 16:08:21\n",
      "exp 1-3, started 16:08:38\n",
      "exp 1-4, started 16:08:55\n",
      "exp 1-5, started 16:09:14\n",
      "exp 1-6, started 16:09:33\n",
      "exp 2-1, started 16:09:59\n",
      "exp 2-2, started 16:10:17\n",
      "exp 2-3, started 16:10:35\n",
      "exp 2-4, started 16:10:53\n",
      "exp 2-5, started 16:11:11\n",
      "exp 2-6, started 16:11:31\n",
      "exp 3-1, started 16:11:50\n",
      "exp 3-2, started 16:12:08\n",
      "exp 3-3, started 16:12:26\n",
      "exp 3-4, started 16:12:45\n",
      "exp 3-5, started 16:13:04\n",
      "exp 3-6, started 16:13:22\n",
      "exp 4-1, started 16:13:42\n",
      "exp 4-2, started 16:13:59\n",
      "exp 4-3, started 16:14:17\n",
      "exp 4-4, started 16:14:35\n",
      "exp 4-5, started 16:14:52\n",
      "exp 4-6, started 16:15:10\n",
      "exp 5-1, started 16:15:29\n",
      "exp 5-2, started 16:15:48\n",
      "exp 5-3, started 16:16:07\n",
      "exp 5-4, started 16:16:26\n",
      "exp 5-5, started 16:16:45\n",
      "exp 5-6, started 16:17:04\n",
      "exp 6-1, started 16:17:23\n",
      "exp 6-2, started 16:17:41\n",
      "exp 6-3, started 16:17:58\n",
      "exp 6-4, started 16:18:17\n",
      "exp 6-5, started 16:18:34\n",
      "exp 6-6, started 16:18:53\n",
      "exp 7-1, started 16:19:10\n",
      "exp 7-2, started 16:19:26\n",
      "exp 7-3, started 16:19:43\n",
      "exp 7-4, started 16:20:00\n",
      "exp 7-5, started 16:20:18\n",
      "exp 7-6, started 16:20:36\n",
      "exp 8-1, started 16:20:54\n",
      "exp 8-2, started 16:21:12\n",
      "exp 8-3, started 16:21:30\n",
      "exp 8-4, started 16:21:48\n",
      "exp 8-5, started 16:22:07\n",
      "exp 8-6, started 16:22:26\n",
      "exp 9-1, started 16:22:45\n",
      "exp 9-2, started 16:23:03\n",
      "exp 9-3, started 16:23:20\n",
      "exp 9-4, started 16:23:38\n",
      "exp 9-5, started 16:23:56\n",
      "exp 9-6, started 16:24:14\n",
      "exp 10-1, started 16:24:33\n",
      "exp 10-2, started 16:24:51\n",
      "exp 10-3, started 16:25:08\n",
      "exp 10-4, started 16:25:26\n",
      "exp 10-5, started 16:25:45\n",
      "exp 10-6, started 16:26:03\n",
      "exp 11-1, started 16:26:22\n",
      "exp 11-2, started 16:26:40\n",
      "exp 11-3, started 16:26:58\n",
      "exp 11-4, started 16:27:16\n",
      "exp 11-5, started 16:27:34\n",
      "exp 11-6, started 16:27:52\n",
      "exp 12-1, started 16:28:10\n",
      "exp 12-2, started 16:28:28\n",
      "exp 12-3, started 16:28:45\n",
      "exp 12-4, started 16:29:04\n",
      "exp 12-5, started 16:29:22\n",
      "exp 12-6, started 16:29:40\n",
      "exp 13-1, started 16:29:58\n",
      "exp 13-2, started 16:30:17\n",
      "exp 13-3, started 16:30:33\n",
      "exp 13-4, started 16:30:50\n",
      "exp 13-5, started 16:31:06\n",
      "exp 13-6, started 16:31:25\n",
      "exp 14-1, started 16:31:43\n",
      "exp 14-2, started 16:32:01\n",
      "exp 14-3, started 16:32:18\n",
      "exp 14-4, started 16:32:36\n",
      "exp 14-5, started 16:32:53\n",
      "exp 14-6, started 16:33:11\n",
      "exp 15-1, started 16:33:27\n",
      "exp 15-2, started 16:33:44\n",
      "exp 15-3, started 16:34:01\n",
      "exp 15-4, started 16:34:18\n",
      "exp 15-5, started 16:34:36\n",
      "exp 15-6, started 16:34:53\n",
      "exp 16-1, started 16:35:10\n",
      "exp 16-2, started 16:35:29\n",
      "exp 16-3, started 16:35:46\n",
      "exp 16-4, started 16:36:04\n",
      "exp 16-5, started 16:36:22\n",
      "exp 16-6, started 16:36:40\n",
      "exp 17-1, started 16:37:00\n",
      "exp 17-2, started 16:37:18\n",
      "exp 17-3, started 16:37:36\n",
      "exp 17-4, started 16:37:54\n",
      "exp 17-5, started 16:38:15\n",
      "exp 17-6, started 16:38:33\n",
      "exp 18-1, started 16:38:53\n",
      "exp 18-2, started 16:39:14\n",
      "exp 18-3, started 16:39:33\n",
      "exp 18-4, started 16:39:52\n",
      "exp 18-5, started 16:40:10\n",
      "exp 18-6, started 16:40:30\n",
      "exp 19-1, started 16:40:50\n",
      "exp 19-2, started 16:41:11\n",
      "exp 19-3, started 16:41:31\n",
      "exp 19-4, started 16:41:51\n",
      "exp 19-5, started 16:42:17\n",
      "exp 19-6, started 16:42:36\n",
      "exp 20-1, started 16:42:56\n",
      "exp 20-2, started 16:43:16\n",
      "exp 20-3, started 16:43:35\n",
      "exp 20-4, started 16:43:53\n",
      "exp 20-5, started 16:44:12\n",
      "exp 20-6, started 16:44:31\n",
      "exp 21-1, started 16:44:49\n",
      "exp 21-2, started 16:45:07\n",
      "exp 21-3, started 16:45:26\n",
      "exp 21-4, started 16:45:44\n",
      "exp 21-5, started 16:46:03\n",
      "exp 21-6, started 16:46:20\n",
      "exp 22-1, started 16:46:37\n",
      "exp 22-2, started 16:46:56\n",
      "exp 22-3, started 16:47:14\n",
      "exp 22-4, started 16:47:35\n",
      "exp 22-5, started 16:47:55\n",
      "exp 22-6, started 16:48:13\n",
      "exp 23-1, started 16:48:32\n",
      "exp 23-2, started 16:48:49\n",
      "exp 23-3, started 16:49:06\n",
      "exp 23-4, started 16:49:24\n",
      "exp 23-5, started 16:49:43\n",
      "exp 23-6, started 16:50:02\n",
      "exp 24-1, started 16:50:21\n",
      "exp 24-2, started 16:50:40\n",
      "exp 24-3, started 16:50:58\n",
      "exp 24-4, started 16:51:16\n",
      "exp 24-5, started 16:51:34\n",
      "exp 24-6, started 16:51:53\n",
      "exp 25-1, started 16:52:25\n",
      "exp 25-2, started 16:52:55\n",
      "exp 25-3, started 16:53:26\n",
      "exp 25-4, started 16:53:57\n",
      "exp 25-5, started 16:54:30\n",
      "exp 25-6, started 16:55:01\n",
      "exp 26-1, started 16:55:31\n",
      "exp 26-2, started 16:56:02\n",
      "exp 26-3, started 16:56:30\n",
      "exp 26-4, started 16:57:00\n",
      "exp 26-5, started 16:57:30\n",
      "exp 26-6, started 16:58:02\n",
      "exp 27-1, started 16:58:31\n",
      "exp 27-2, started 16:59:01\n",
      "exp 27-3, started 16:59:28\n",
      "exp 27-4, started 16:59:57\n",
      "exp 27-5, started 17:00:26\n",
      "exp 27-6, started 17:00:55\n",
      "exp 28-1, started 17:01:29\n",
      "exp 28-2, started 17:02:00\n",
      "exp 28-3, started 17:02:28\n",
      "exp 28-4, started 17:02:57\n",
      "exp 28-5, started 17:03:28\n",
      "exp 28-6, started 17:03:58\n",
      "exp 29-1, started 17:04:37\n",
      "exp 29-2, started 17:05:20\n",
      "exp 29-3, started 17:05:57\n",
      "exp 29-4, started 17:06:26\n",
      "exp 29-5, started 17:07:00\n",
      "exp 29-6, started 17:07:31\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be7b301e-f193-4ffd-bd35-6674643f5657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now which of these are unexpected / not already explained?\n",
    "mz_dmso = [79.0212, 101.0032, 157.0351]  # M+H+, M+Na+ 2M+H+\n",
    "mz_lock = [142.1590, 322.0481]  # tetramethylpiperidine, hexamethoxyphosphazene\n",
    "mz_is = 361.1201  # fenofibrate\n",
    "\n",
    "known_product_smiles = con.get_product_smiles((exp_nr, plate_nr), \"A3\")\n",
    "known_product_mzs = [smiles_to_lcms_mass(smi) for smi in known_product_smiles]\n",
    "\n",
    "known_mzs = mz_dmso + mz_lock + [mz_is] + known_product_mzs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e686a4ab-8724-4a24-af14-a3a04d350bfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RT[min]</th>\n",
       "      <th>Area</th>\n",
       "      <th>I</th>\n",
       "      <th>S/N</th>\n",
       "      <th>max_m/z</th>\n",
       "      <th>FWHM[min]</th>\n",
       "      <th>Area%</th>\n",
       "      <th>Int%</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2.156</td>\n",
       "      <td>2.589547e+05</td>\n",
       "      <td>62241.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>146.1175</td>\n",
       "      <td>0.059</td>\n",
       "      <td>5.39209</td>\n",
       "      <td>3.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2.903</td>\n",
       "      <td>6.490781e+05</td>\n",
       "      <td>146339.0</td>\n",
       "      <td>42.1</td>\n",
       "      <td>365.1712</td>\n",
       "      <td>0.081</td>\n",
       "      <td>13.51545</td>\n",
       "      <td>8.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>3.000</td>\n",
       "      <td>1.114304e+06</td>\n",
       "      <td>440712.0</td>\n",
       "      <td>130.7</td>\n",
       "      <td>365.1709</td>\n",
       "      <td>0.038</td>\n",
       "      <td>23.20264</td>\n",
       "      <td>26.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>3.395</td>\n",
       "      <td>2.613391e+06</td>\n",
       "      <td>1012323.0</td>\n",
       "      <td>306.9</td>\n",
       "      <td>332.0521</td>\n",
       "      <td>0.037</td>\n",
       "      <td>54.41742</td>\n",
       "      <td>60.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>3.592</td>\n",
       "      <td>1.006776e+06</td>\n",
       "      <td>445752.0</td>\n",
       "      <td>132.9</td>\n",
       "      <td>459.1523</td>\n",
       "      <td>0.034</td>\n",
       "      <td>20.96362</td>\n",
       "      <td>26.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>3.730</td>\n",
       "      <td>8.154621e+05</td>\n",
       "      <td>335406.0</td>\n",
       "      <td>100.8</td>\n",
       "      <td>505.1579</td>\n",
       "      <td>0.037</td>\n",
       "      <td>16.97999</td>\n",
       "      <td>20.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>5.032</td>\n",
       "      <td>4.152705e+05</td>\n",
       "      <td>143717.0</td>\n",
       "      <td>1209.5</td>\n",
       "      <td>250.9768</td>\n",
       "      <td>0.043</td>\n",
       "      <td>8.64698</td>\n",
       "      <td>8.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>5.033</td>\n",
       "      <td>3.906193e+05</td>\n",
       "      <td>139087.0</td>\n",
       "      <td>41.9</td>\n",
       "      <td>250.9768</td>\n",
       "      <td>0.043</td>\n",
       "      <td>8.13368</td>\n",
       "      <td>8.29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    RT[min]          Area          I     S/N   max_m/z  FWHM[min]     Area%  \\\n",
       "#                                                                             \n",
       "9     2.156  2.589547e+05    62241.0    17.8  146.1175      0.059   5.39209   \n",
       "12    2.903  6.490781e+05   146339.0    42.1  365.1712      0.081  13.51545   \n",
       "16    3.000  1.114304e+06   440712.0   130.7  365.1709      0.038  23.20264   \n",
       "17    3.395  2.613391e+06  1012323.0   306.9  332.0521      0.037  54.41742   \n",
       "22    3.592  1.006776e+06   445752.0   132.9  459.1523      0.034  20.96362   \n",
       "25    3.730  8.154621e+05   335406.0   100.8  505.1579      0.037  16.97999   \n",
       "36    5.032  4.152705e+05   143717.0  1209.5  250.9768      0.043   8.64698   \n",
       "37    5.033  3.906193e+05   139087.0    41.9  250.9768      0.043   8.13368   \n",
       "\n",
       "     Int%  \n",
       "#          \n",
       "9    3.71  \n",
       "12   8.73  \n",
       "16  26.28  \n",
       "17  60.36  \n",
       "22  26.58  \n",
       "25  20.00  \n",
       "36   8.57  \n",
       "37   8.29  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unexplained_peaks = df.loc[((df[\"Area%\"] > 5)  # only significant peaks\n",
    "        & ~np.isclose(df[\"max_m/z\"].to_numpy()[:, None], known_mzs, rtol=5e-6, atol=0).any(axis=1)  # only peaks that are not already explained\n",
    "       ), \n",
    "    ]\n",
    "unexplained_peaks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e281629a-6d61-44b2-b3d4-87538c641e48",
   "metadata": {},
   "source": [
    "## Identify starting material peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ca77c06-8b7e-47aa-9db8-c80c2ce256ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O=C(c1ccc(C2=NCCO2)cc1)[B-](F)(F)F.[K+]',\n",
       " 'CC(C)C[C@H]1C[C@]2(O[NH2+]1)OC1(CCCCC1)OC2=O.[Cl-]',\n",
       " '[Cl-].[NH3+]NC(=S)c1cccs1']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms = con.get_starting_materials_for_reaction((exp_nr, plate_nr, \"K7\"))\n",
    "sms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f086acfb-2780-4972-86e4-728354c224ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "159.00451664409"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smiles_to_lcms_mass(sms[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d3e8580e-7953-464c-8793-79249f69a518",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['RT[min]', 'Area', 'I', 'S/N', 'max_m/z', 'FWHM[min]', 'Area%', 'Int%'], dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unexplained_peaks.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc206d5-c0ad-44d8-9e17-6161c9a6cb7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
