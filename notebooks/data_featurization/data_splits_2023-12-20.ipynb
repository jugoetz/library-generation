{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Splits\n",
    "We want to split the SynFerm data set into train, validation and test data.\n",
    "For all splits, we will do 9 random repetitions.\n",
    "For 1D and 2D split, which both use 3 different groups to split on, these will divide into 3 random repetitions for each of the 3 groups. \n",
    "\n",
    "### 0D Split\n",
    "For the 0D split, we use a random train-test split.\n",
    "We use a 80/10/10 split into train, val, and test set.\n",
    "\n",
    "### 1D Split\n",
    "For the 1D split, we use a (1D) GroupShuffleSplit.\n",
    "Each individual split will be 70/15/15 train/test (of groups not samples!).\n",
    "As groups, we use either initiator, monomer, or terminator.\n",
    "\n",
    "### 2D Split\n",
    "For the 2D split, we use a (2D) GroupShuffleSplit.\n",
    "Each individual split will use 20% of groups as test set and 25% of remaining groups as validation set. \n",
    "Due to the dimensionality, this means we expect 0.2 * 0.2 = 4% of samples in the test and validation set and 0.800^2 * 0.75^2 = 36.0% of samples in the training set.\n",
    "The remaining samples are not used to prevent leakage.\n",
    "As groups, we use either \\[initiator, monomer], \\[monomer, terminator] or \\[initiator, terminator].\n",
    "\n",
    "### 3D Split\n",
    "For the 3D split, we use a (3D) GroupShuffleSplit.\n",
    "Each individual split will use 25% of groups as test set, 33% of remaining groups as validation set, and the remaining groups as training set.\n",
    "Due to the dimensionality, this means we expect 0.25^3 = 1.5% of samples in the test and validation set and 0.75^3 * 0.67^3 = 12.5% of sample in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import sys\n",
    "\n",
    "sys.path.append(str(pathlib.Path().resolve().parents[1]))\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GroupShuffleSplit, ShuffleSplit\n",
    "\n",
    "from src.definitions import DATA_DIR\n",
    "from src.util.train_test_split import GroupShuffleSplitND\n",
    "from util import write_indices_and_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data_filename = \"synferm_dataset_2023-12-20_39486records.csv\"\n",
    "data_name = data_filename.rsplit(\"_\", maxsplit=1)[0]\n",
    "df = pd.read_csv(DATA_DIR / \"curated_data\" / data_filename)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# M_long_dia will be to sort diastereomers into the same group on group shuffle splits\n",
    "diastereomers = {\n",
    "    \"Mon001\": \"Mon087\",\n",
    "    \"Mon003\": \"Mon078\",\n",
    "    \"Mon011\": \"Mon088\",\n",
    "    \"Mon013\": \"Mon074\",\n",
    "    \"Mon014\": \"Mon090\",\n",
    "    \"Mon015\": \"Mon076\",\n",
    "    \"Mon016\": \"Mon096\",\n",
    "    \"Mon017\": \"Mon075\",\n",
    "    \"Mon019\": \"Mon091\",\n",
    "    \"Mon020\": \"Mon077\",\n",
    "    \"Mon080\": \"Mon010\",\n",
    "}\n",
    "df[\"M_long_dia\"] = df[\"M_long\"].replace(diastereomers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0D split (not needed, see 0D_80 in truncated splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0D split final-retrain\n",
    "To retrain the best model after selection, for the 0D split we only use one fold, split into training and validation set (used for early stopping of FFN training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = ShuffleSplit(n_splits=1, test_size=0.1, random_state=42)\n",
    "\n",
    "indices = []\n",
    "sizes = []\n",
    "pos_class = []\n",
    "for idx_train, idx_val in splitter.split(df):\n",
    "    idx_test = []  # placeholder so we can use the write_indices_and_stats function, just delete fold0_test.csv later\n",
    "    # add to list\n",
    "    indices.append((idx_train, idx_val, idx_test))\n",
    "    sizes.append((len(idx_train), len(idx_val), len(idx_test)))\n",
    "    pos_class.append(\n",
    "        (np.sum(df[['binary_A', 'binary_B', 'binary_C']].loc[idx_train]).to_numpy(), \n",
    "         np.sum(df[['binary_A', 'binary_B', 'binary_C']].loc[idx_val]).to_numpy(), \n",
    "         np.sum(df[['binary_A', 'binary_B', 'binary_C']].loc[idx_test]).to_numpy(), \n",
    "\n",
    "        )\n",
    "    )\n",
    "    \n",
    "print(sizes)\n",
    "print(pos_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_indices_and_stats(\n",
    "    indices, \n",
    "    sizes, \n",
    "    pos_class,\n",
    "    total_size=len(df),\n",
    "    data_name=data_name,\n",
    "    split_dimension=0, \n",
    "    save_indices=True, \n",
    "    train_size=\"final_retrain\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1D split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = GroupShuffleSplit(n_splits=3, test_size=0.15, random_state=np.random.RandomState(42))  # here, we reuse the outer splitter as well, so we use RandomState\n",
    "inner_splitter = GroupShuffleSplit(n_splits=1, test_size=0.15/0.85, random_state=np.random.RandomState(42))  # we use a RandomState instance, not an int, because we will reuse this splitter several times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = []\n",
    "sizes = []\n",
    "pos_class = []\n",
    "for idx_train_val, idx_test in splitter.split(list(range(len(df))), groups=df[\"I_long\"]):\n",
    "    train, val = next(inner_splitter.split(idx_train_val, groups=df[\"I_long\"][idx_train_val]))\n",
    "    # use indices to index indices :P (we need to obtain indices referring to the original dataframe)\n",
    "    idx_train = idx_train_val[train]\n",
    "    idx_val = idx_train_val[val]\n",
    "    indices.append((idx_train, idx_val, idx_test))\n",
    "    sizes.append((len(idx_train), len(idx_val), len(idx_test)))\n",
    "    pos_class.append(\n",
    "        (np.sum(df[['binary_A', 'binary_B', 'binary_C']].loc[idx_train]).to_numpy(), \n",
    "         np.sum(df[['binary_A', 'binary_B', 'binary_C']].loc[idx_val]).to_numpy(), \n",
    "         np.sum(df[['binary_A', 'binary_B', 'binary_C']].loc[idx_test]).to_numpy(),\n",
    "        )\n",
    "    )\n",
    "\n",
    "# note: for M_long, we need to obtain diastereomer relationships to sort them into the same group\n",
    "for idx_train_val, idx_test in splitter.split(list(range(len(df))), groups=df[\"M_long_dia\"]):\n",
    "    train, val = next(inner_splitter.split(idx_train_val, groups=df[\"M_long_dia\"][idx_train_val]))\n",
    "    # use indices to index indices :P (we need to obtain indices referring to the original dataframe)\n",
    "    idx_train = idx_train_val[train]\n",
    "    idx_val = idx_train_val[val]\n",
    "    indices.append((idx_train, idx_val, idx_test))\n",
    "    sizes.append((len(idx_train), len(idx_val), len(idx_test)))\n",
    "    pos_class.append(\n",
    "        (np.sum(df[['binary_A', 'binary_B', 'binary_C']].loc[idx_train]).to_numpy(), \n",
    "         np.sum(df[['binary_A', 'binary_B', 'binary_C']].loc[idx_val]).to_numpy(), \n",
    "         np.sum(df[['binary_A', 'binary_B', 'binary_C']].loc[idx_test]).to_numpy(),\n",
    "        )\n",
    "    )\n",
    "        \n",
    "for idx_train_val, idx_test in splitter.split(list(range(len(df))), groups=df[\"T_long\"]):\n",
    "    train, val = next(inner_splitter.split(idx_train_val, groups=df[\"T_long\"][idx_train_val]))\n",
    "    # use indices to index indices :P (we need to obtain indices referring to the original dataframe)\n",
    "    idx_train = idx_train_val[train]\n",
    "    idx_val = idx_train_val[val]\n",
    "    indices.append((idx_train, idx_val, idx_test))\n",
    "    sizes.append((len(idx_train), len(idx_val), len(idx_test)))\n",
    "    pos_class.append(\n",
    "        (np.sum(df[['binary_A', 'binary_B', 'binary_C']].loc[idx_train]).to_numpy(), \n",
    "         np.sum(df[['binary_A', 'binary_B', 'binary_C']].loc[idx_val]).to_numpy(), \n",
    "         np.sum(df[['binary_A', 'binary_B', 'binary_C']].loc[idx_test]).to_numpy(),\n",
    "        )\n",
    "    )\n",
    "    \n",
    "print(sizes)\n",
    "print(pos_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_indices_and_stats(\n",
    "    indices, \n",
    "    sizes, \n",
    "    pos_class,\n",
    "    total_size=len(df),\n",
    "    data_name=data_name,\n",
    "    split_dimension=1, \n",
    "    save_indices=True, \n",
    "    train_size=\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1D split for Euan\n",
    "Requirement: same split logic as above, but with 3x9 folds instead of 3x3 folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = GroupShuffleSplit(n_splits=9, test_size=0.15, random_state=np.random.RandomState(42))  # here, we reuse the outer splitter as well, so we use RandomState\n",
    "inner_splitter = GroupShuffleSplit(n_splits=1, test_size=0.15/0.85, random_state=np.random.RandomState(42))  # we use a RandomState instance, not an int, because we will reuse this splitter several times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = []\n",
    "sizes = []\n",
    "pos_class = []\n",
    "for idx_train_val, idx_test in splitter.split(list(range(len(df))), groups=df[\"I_long\"]):\n",
    "    train, val = next(inner_splitter.split(idx_train_val, groups=df[\"I_long\"][idx_train_val]))\n",
    "    # use indices to index indices :P (we need to obtain indices referring to the original dataframe)\n",
    "    idx_train = idx_train_val[train]\n",
    "    idx_val = idx_train_val[val]\n",
    "    indices.append((idx_train, idx_val, idx_test))\n",
    "    sizes.append((len(idx_train), len(idx_val), len(idx_test)))\n",
    "    pos_class.append(\n",
    "        (np.sum(df[['binary_A', 'binary_B', 'binary_C']].loc[idx_train]).to_numpy(), \n",
    "         np.sum(df[['binary_A', 'binary_B', 'binary_C']].loc[idx_val]).to_numpy(), \n",
    "         np.sum(df[['binary_A', 'binary_B', 'binary_C']].loc[idx_test]).to_numpy(),\n",
    "        )\n",
    "    )\n",
    "\n",
    "# note: for M_long, we need to obtain diastereomer relationships to sort them into the same group\n",
    "for idx_train_val, idx_test in splitter.split(list(range(len(df))), groups=df[\"M_long_dia\"]):\n",
    "    train, val = next(inner_splitter.split(idx_train_val, groups=df[\"M_long_dia\"][idx_train_val]))\n",
    "    # use indices to index indices :P (we need to obtain indices referring to the original dataframe)\n",
    "    idx_train = idx_train_val[train]\n",
    "    idx_val = idx_train_val[val]\n",
    "    indices.append((idx_train, idx_val, idx_test))\n",
    "    sizes.append((len(idx_train), len(idx_val), len(idx_test)))\n",
    "    pos_class.append(\n",
    "        (np.sum(df[['binary_A', 'binary_B', 'binary_C']].loc[idx_train]).to_numpy(), \n",
    "         np.sum(df[['binary_A', 'binary_B', 'binary_C']].loc[idx_val]).to_numpy(), \n",
    "         np.sum(df[['binary_A', 'binary_B', 'binary_C']].loc[idx_test]).to_numpy(),\n",
    "        )\n",
    "    )\n",
    "        \n",
    "for idx_train_val, idx_test in splitter.split(list(range(len(df))), groups=df[\"T_long\"]):\n",
    "    train, val = next(inner_splitter.split(idx_train_val, groups=df[\"T_long\"][idx_train_val]))\n",
    "    # use indices to index indices :P (we need to obtain indices referring to the original dataframe)\n",
    "    idx_train = idx_train_val[train]\n",
    "    idx_val = idx_train_val[val]\n",
    "    indices.append((idx_train, idx_val, idx_test))\n",
    "    sizes.append((len(idx_train), len(idx_val), len(idx_test)))\n",
    "    pos_class.append(\n",
    "        (np.sum(df[['binary_A', 'binary_B', 'binary_C']].loc[idx_train]).to_numpy(), \n",
    "         np.sum(df[['binary_A', 'binary_B', 'binary_C']].loc[idx_val]).to_numpy(), \n",
    "         np.sum(df[['binary_A', 'binary_B', 'binary_C']].loc[idx_test]).to_numpy(),\n",
    "        )\n",
    "    )\n",
    "    \n",
    "print(sizes)\n",
    "print(pos_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_indices_and_stats(\n",
    "    indices, \n",
    "    sizes, \n",
    "    pos_class,\n",
    "    total_size=len(df),\n",
    "    data_name=data_name,\n",
    "    split_dimension=1, \n",
    "    save_indices=True, \n",
    "    train_size=\"euan27folds\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2D split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = GroupShuffleSplitND(n_splits=3, test_size=0.2, random_state=np.random.RandomState(42))  # here, we reuse the outer splitter as well, so we use RandomState\n",
    "inner_splitter = GroupShuffleSplitND(n_splits=1, test_size=0.2/0.8, random_state=np.random.RandomState(42))  # we use a RandomState instance, not an int, because we will reuse this splitter several times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = []\n",
    "sizes = []\n",
    "pos_class = []\n",
    "for idx_train_val, idx_test in splitter.split(df, groups=df[[\"I_long\", \"M_long_dia\"]]):\n",
    "    train, val = next(inner_splitter.split(df.iloc[idx_train_val], groups=df[[\"I_long\", \"M_long_dia\"]].iloc[idx_train_val]))\n",
    "    # use indices to index indices :P (we need to obtain indices referring to the original dataframe)\n",
    "    idx_train = idx_train_val[train]\n",
    "    idx_val = idx_train_val[val]\n",
    "    indices.append((idx_train, idx_val, idx_test))\n",
    "    sizes.append((len(idx_train), len(idx_val), len(idx_test)))\n",
    "    pos_class.append(\n",
    "        (np.sum(df[['binary_A', 'binary_B', 'binary_C']].loc[idx_train]).to_numpy(), \n",
    "         np.sum(df[['binary_A', 'binary_B', 'binary_C']].loc[idx_val]).to_numpy(), \n",
    "         np.sum(df[['binary_A', 'binary_B', 'binary_C']].loc[idx_test]).to_numpy(),\n",
    "        )\n",
    "    )\n",
    "    \n",
    "for idx_train_val, idx_test in splitter.split(list(range(len(df))), groups=df[[\"M_long_dia\", \"T_long\"]]):\n",
    "    train, val = next(inner_splitter.split(idx_train_val, groups=df[[\"M_long_dia\", \"T_long\"]].iloc[idx_train_val]))\n",
    "    # use indices to index indices :P (we need to obtain indices referring to the original dataframe)\n",
    "    idx_train = idx_train_val[train]\n",
    "    idx_val = idx_train_val[val]\n",
    "    indices.append((idx_train, idx_val, idx_test))\n",
    "    sizes.append((len(idx_train), len(idx_val), len(idx_test)))\n",
    "    pos_class.append(\n",
    "        (np.sum(df[['binary_A', 'binary_B', 'binary_C']].loc[idx_train]).to_numpy(), \n",
    "         np.sum(df[['binary_A', 'binary_B', 'binary_C']].loc[idx_val]).to_numpy(), \n",
    "         np.sum(df[['binary_A', 'binary_B', 'binary_C']].loc[idx_test]).to_numpy(),\n",
    "        )\n",
    "    )\n",
    "    \n",
    "for idx_train_val, idx_test in splitter.split(list(range(len(df))), groups=df[[\"I_long\", \"T_long\"]]):\n",
    "    train, val = next(inner_splitter.split(idx_train_val, groups=df[[\"I_long\", \"T_long\"]].iloc[idx_train_val]))\n",
    "    # use indices to index indices :P (we need to obtain indices referring to the original dataframe)\n",
    "    idx_train = idx_train_val[train]\n",
    "    idx_val = idx_train_val[val]\n",
    "    indices.append((idx_train, idx_val, idx_test))\n",
    "    sizes.append((len(idx_train), len(idx_val), len(idx_test)))\n",
    "    pos_class.append(\n",
    "        (np.sum(df[['binary_A', 'binary_B', 'binary_C']].loc[idx_train]).to_numpy(), \n",
    "         np.sum(df[['binary_A', 'binary_B', 'binary_C']].loc[idx_val]).to_numpy(), \n",
    "         np.sum(df[['binary_A', 'binary_B', 'binary_C']].loc[idx_test]).to_numpy(),\n",
    "        )\n",
    "    )\n",
    "        \n",
    "print(sizes)\n",
    "print(pos_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_indices_and_stats(\n",
    "    indices, \n",
    "    sizes, \n",
    "    pos_class,\n",
    "    total_size=len(df),\n",
    "    data_name=data_name,\n",
    "    split_dimension=2, \n",
    "    save_indices=True, \n",
    "    train_size=\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3D split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = GroupShuffleSplitND(n_splits=9, test_size=0.25, random_state=np.random.RandomState(42))  # here, we reuse the outer splitter as well, so we use RandomState (not true, copyPaste error from before. Anyway, not a problem)\n",
    "inner_splitter = GroupShuffleSplitND(n_splits=1, test_size=0.25/0.75, random_state=np.random.RandomState(42))  # we use a RandomState instance, not an int, because we will reuse this splitter several times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = []\n",
    "sizes = []\n",
    "pos_class = []\n",
    "for idx_train_val, idx_test in splitter.split(df, groups=df[[\"I_long\", \"M_long_dia\", \"T_long\"]]):\n",
    "    train, val = next(inner_splitter.split(df.iloc[idx_train_val], groups=df[[\"I_long\", \"M_long_dia\", \"T_long\"]].iloc[idx_train_val]))\n",
    "    # use indices to index indices :P (we need to obtain indices referring to the original dataframe)\n",
    "    idx_train = idx_train_val[train]\n",
    "    idx_val = idx_train_val[val]\n",
    "    indices.append((idx_train, idx_val, idx_test))\n",
    "    sizes.append((len(idx_train), len(idx_val), len(idx_test)))\n",
    "    pos_class.append(\n",
    "        (np.sum(df[['binary_A', 'binary_B', 'binary_C']].loc[idx_train]).to_numpy(), \n",
    "         np.sum(df[['binary_A', 'binary_B', 'binary_C']].loc[idx_val]).to_numpy(), \n",
    "         np.sum(df[['binary_A', 'binary_B', 'binary_C']].loc[idx_test]).to_numpy(),\n",
    "        )\n",
    "    )\n",
    "    \n",
    "print(sizes)\n",
    "print(pos_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_indices_and_stats(\n",
    "    indices, \n",
    "    sizes, \n",
    "    pos_class,\n",
    "    total_size=len(df),\n",
    "    data_name=data_name,\n",
    "    split_dimension=3, \n",
    "    save_indices=True, \n",
    "    train_size=\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Control: Check splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1D split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_dimension = 1\n",
    "split_dir = DATA_DIR / \"curated_data\" / \"splits\" / f\"{data_name}_{split_dimension}D_split\"\n",
    "    \n",
    "for fold_idx in range(9): # only these are split on monomers\n",
    "    \n",
    "    # import indices\n",
    "    train_idx = pd.read_csv(split_dir / f\"fold{fold_idx}_train.csv\")[\"index\"].to_numpy()\n",
    "    val_idx = pd.read_csv(split_dir / f\"fold{fold_idx}_val.csv\")[\"index\"].to_numpy()\n",
    "    test_idx = pd.read_csv(split_dir / f\"fold{fold_idx}_test.csv\")[\"index\"].to_numpy()\n",
    "\n",
    "    # check mutually exclusive\n",
    "    assert len(np.intersect1d(train_idx, val_idx)) == 0\n",
    "    assert len(np.intersect1d(train_idx, test_idx)) == 0\n",
    "    assert len(np.intersect1d(val_idx, test_idx)) == 0\n",
    "\n",
    "    # check 1D groups are mutually exclusive\n",
    "    if fold_idx < 3: # first three are split on initiator\n",
    "        assert len(np.intersect1d(df[\"I_long\"].iloc[train_idx], df[\"I_long\"].iloc[val_idx])) == 0\n",
    "        assert len(np.intersect1d(df[\"I_long\"].iloc[train_idx], df[\"I_long\"].iloc[test_idx])) == 0\n",
    "        assert len(np.intersect1d(df[\"I_long\"].iloc[val_idx], df[\"I_long\"].iloc[test_idx])) == 0\n",
    "    elif fold_idx < 6:  # next three are split on monomer\n",
    "        assert len(np.intersect1d(df[\"M_long\"].iloc[train_idx], df[\"M_long\"].iloc[val_idx])) == 0\n",
    "        assert len(np.intersect1d(df[\"M_long\"].iloc[train_idx], df[\"M_long\"].iloc[test_idx])) == 0\n",
    "        assert len(np.intersect1d(df[\"M_long\"].iloc[val_idx], df[\"M_long\"].iloc[test_idx])) == 0\n",
    "        assert len(np.intersect1d(df[\"M_long_dia\"].iloc[train_idx], df[\"M_long_dia\"].iloc[val_idx])) == 0\n",
    "        assert len(np.intersect1d(df[\"M_long_dia\"].iloc[train_idx], df[\"M_long_dia\"].iloc[test_idx])) == 0\n",
    "        assert len(np.intersect1d(df[\"M_long_dia\"].iloc[val_idx], df[\"M_long_dia\"].iloc[test_idx])) == 0\n",
    "        \n",
    "    else:  # last three are split on terminator\n",
    "        assert len(np.intersect1d(df[\"T_long\"].iloc[train_idx], df[\"T_long\"].iloc[val_idx])) == 0\n",
    "        assert len(np.intersect1d(df[\"T_long\"].iloc[train_idx], df[\"T_long\"].iloc[test_idx])) == 0\n",
    "        assert len(np.intersect1d(df[\"T_long\"].iloc[val_idx], df[\"T_long\"].iloc[test_idx])) == 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2D split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_dimension = 2\n",
    "split_dir = DATA_DIR / \"curated_data\" / \"splits\" / f\"{data_name}_{split_dimension}D_split\"\n",
    "    \n",
    "for fold_idx in range(9):\n",
    "    # import indices\n",
    "    train_idx = pd.read_csv(split_dir / f\"fold{fold_idx}_train.csv\")[\"index\"].to_numpy()\n",
    "    val_idx = pd.read_csv(split_dir / f\"fold{fold_idx}_val.csv\")[\"index\"].to_numpy()\n",
    "    test_idx = pd.read_csv(split_dir / f\"fold{fold_idx}_test.csv\")[\"index\"].to_numpy()\n",
    "\n",
    "    # check mutually exclusive\n",
    "    assert len(np.intersect1d(train_idx, val_idx)) == 0\n",
    "    assert len(np.intersect1d(train_idx, test_idx)) == 0\n",
    "    assert len(np.intersect1d(val_idx, test_idx)) == 0\n",
    "\n",
    "    # check 2D groups are mutually exclusive\n",
    "    if fold_idx < 3: # first three are split on initiator and monomer\n",
    "        assert len(np.intersect1d(np.unique(df[[\"I_long\", \"M_long_dia\"]].iloc[train_idx]), np.unique(df[[\"I_long\", \"M_long_dia\"]].iloc[val_idx]))) == 0\n",
    "        assert len(np.intersect1d(np.unique(df[[\"I_long\", \"M_long_dia\"]].iloc[train_idx]), np.unique(df[[\"I_long\", \"M_long_dia\"]].iloc[test_idx]))) == 0\n",
    "        assert len(np.intersect1d(np.unique(df[[\"I_long\", \"M_long_dia\"]].iloc[val_idx]), np.unique(df[[\"I_long\", \"M_long_dia\"]].iloc[test_idx]))) == 0\n",
    "    elif fold_idx < 6:  # next three are split on monomer and terminator\n",
    "        assert len(np.intersect1d(np.unique(df[[\"M_long\", \"T_long\"]].iloc[train_idx]), np.unique(df[[\"M_long\", \"T_long\"]].iloc[val_idx]))) == 0\n",
    "        assert len(np.intersect1d(np.unique(df[[\"M_long\", \"T_long\"]].iloc[train_idx]), np.unique(df[[\"M_long\", \"T_long\"]].iloc[test_idx]))) == 0\n",
    "        assert len(np.intersect1d(np.unique(df[[\"M_long\", \"T_long\"]].iloc[val_idx]), np.unique(df[[\"M_long\", \"T_long\"]].iloc[test_idx]))) == 0\n",
    "        assert len(np.intersect1d(np.unique(df[[\"M_long_dia\", \"T_long\"]].iloc[train_idx]), np.unique(df[[\"M_long_dia\", \"T_long\"]].iloc[val_idx]))) == 0\n",
    "        assert len(np.intersect1d(np.unique(df[[\"M_long_dia\", \"T_long\"]].iloc[train_idx]), np.unique(df[[\"M_long_dia\", \"T_long\"]].iloc[test_idx]))) == 0\n",
    "        assert len(np.intersect1d(np.unique(df[[\"M_long_dia\", \"T_long\"]].iloc[val_idx]), np.unique(df[[\"M_long_dia\", \"T_long\"]].iloc[test_idx]))) == 0\n",
    "    else:  # last three are split on initiator and terminator\n",
    "        assert len(np.intersect1d(np.unique(df[[\"I_long\", \"T_long\"]].iloc[train_idx]), np.unique(df[[\"I_long\", \"T_long\"]].iloc[val_idx]))) == 0\n",
    "        assert len(np.intersect1d(np.unique(df[[\"I_long\", \"T_long\"]].iloc[train_idx]), np.unique(df[[\"I_long\", \"T_long\"]].iloc[test_idx]))) == 0\n",
    "        assert len(np.intersect1d(np.unique(df[[\"I_long\", \"T_long\"]].iloc[val_idx]), np.unique(df[[\"I_long\", \"T_long\"]].iloc[test_idx]))) == 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3D split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_dimension = 3\n",
    "split_dir = DATA_DIR / \"curated_data\" / \"splits\" / f\"{data_name}_{split_dimension}D_split\"\n",
    "    \n",
    "for fold_idx in range(9):\n",
    "    # import indices\n",
    "    train_idx = pd.read_csv(split_dir / f\"fold{fold_idx}_train.csv\")[\"index\"].to_numpy()\n",
    "    val_idx = pd.read_csv(split_dir / f\"fold{fold_idx}_val.csv\")[\"index\"].to_numpy()\n",
    "    test_idx = pd.read_csv(split_dir / f\"fold{fold_idx}_test.csv\")[\"index\"].to_numpy()\n",
    "\n",
    "    # check mutually exclusive\n",
    "    assert len(np.intersect1d(train_idx, val_idx)) == 0\n",
    "    assert len(np.intersect1d(train_idx, test_idx)) == 0\n",
    "    assert len(np.intersect1d(val_idx, test_idx)) == 0\n",
    "\n",
    "    # check 3D groups are mutually exclusive\n",
    "    assert len(np.intersect1d(np.unique(df[[\"I_long\", \"M_long\", \"T_long\"]].iloc[train_idx]), np.unique(df[[\"I_long\", \"M_long\", \"T_long\"]].iloc[val_idx]))) == 0\n",
    "    assert len(np.intersect1d(np.unique(df[[\"I_long\", \"M_long\", \"T_long\"]].iloc[train_idx]), np.unique(df[[\"I_long\", \"M_long\", \"T_long\"]].iloc[test_idx]))) == 0\n",
    "    assert len(np.intersect1d(np.unique(df[[\"I_long\", \"M_long\", \"T_long\"]].iloc[val_idx]), np.unique(df[[\"I_long\", \"M_long\", \"T_long\"]].iloc[test_idx]))) == 0\n",
    "    \n",
    "    assert len(np.intersect1d(np.unique(df[[\"I_long\", \"M_long_dia\", \"T_long\"]].iloc[train_idx]), np.unique(df[[\"I_long\", \"M_long_dia\", \"T_long\"]].iloc[val_idx]))) == 0\n",
    "    assert len(np.intersect1d(np.unique(df[[\"I_long\", \"M_long_dia\", \"T_long\"]].iloc[train_idx]), np.unique(df[[\"I_long\", \"M_long_dia\", \"T_long\"]].iloc[test_idx]))) == 0\n",
    "    assert len(np.intersect1d(np.unique(df[[\"I_long\", \"M_long_dia\", \"T_long\"]].iloc[val_idx]), np.unique(df[[\"I_long\", \"M_long_dia\", \"T_long\"]].iloc[test_idx]))) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
