{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Splits Truncated\n",
    "\n",
    "Here, we split in the same way as in `data_splits.ipynb`, but then we restrict the number of points in the training data.\n",
    "There are two ways we could do this:\n",
    "1. Take the original splits and drop random examples. Problem: As we go to smaller training sets, this will lead to us inadvertently removing all the examples of one compound from the training data but not from validation/test.\n",
    "2. We do (procedurally) the same splits as before, but with different train-test ratios.\n",
    "\n",
    "Option 2 is safer to do w.r.t. retaining the split dimensionality.\n",
    "\n",
    "### 0D Split\n",
    "For the 0D split, we use a random train-test split.\n",
    "Standard: We use a 80/10/10 split into train, val, and test set.\n",
    "\n",
    "Truncated: \n",
    "- 40/30/30 -> 16k training samples\n",
    "- 20/40/40 -> 8k training samples\n",
    "- 10/45/45 -> 4k training samples\n",
    "- 5/50/45 -> 2k training samples\n",
    "- 2.5/50/47.5 -> 1k training samples\n",
    "- 1.25/50/48.75 -> 500 training samples\n",
    "(numbers of training samples are averaged over folds and approximate)\n",
    "\n",
    "### 1D Split\n",
    "For the 1D split, we use a (1D) GroupShuffleSplit.\n",
    "Standard: Each individual split will be 80/10/10 train/test (of groups not samples!).\n",
    "As groups, we use either initiator, monomer, or terminator (3x3 splits).\n",
    "\n",
    "Truncated:\n",
    "(for uniform results, we pick only one dimension to split on: Initiators)\n",
    "- 80/10/10 -> 31,250 training samples (this is not equivalent to the \"standard\" 1D split b/c here we split on initiators 9 times)\n",
    "- 40/30/30 -> 15,600 training samples\n",
    "- 20/40/40 -> 8,100 training samples\n",
    "- 10/45/45 -> 3,680 training samples\n",
    "- 5/50/45 -> 1,668 training samples\n",
    "- 2.5/50/47.5 -> 487 training samples\n",
    "- 1.25/50/48.75 -> not possible b/c training set will be empty on some folds\n",
    "(numbers of training samples are averaged over folds and approximate)\n",
    "\n",
    "\n",
    "### other splits\n",
    "For the 2D and 3D split, we don't perform this rather expensive experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T17:04:53.936618Z",
     "start_time": "2023-09-07T17:04:53.237504Z"
    }
   },
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import sys\n",
    "\n",
    "sys.path.append(str(pathlib.Path().resolve().parents[1]))\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GroupShuffleSplit, ShuffleSplit\n",
    "\n",
    "from src.definitions import DATA_DIR\n",
    "from src.util.train_test_split import GroupShuffleSplitND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T17:04:54.279631Z",
     "start_time": "2023-09-07T17:04:53.937616Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40018, 27)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "data_filename = \"synferm_dataset_2023-09-05_40018records.csv\"\n",
    "data_name = data_filename.rsplit(\"_\", maxsplit=1)[0]\n",
    "df = pd.read_csv(DATA_DIR / \"curated_data\" / data_filename)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T17:04:54.290593Z",
     "start_time": "2023-09-07T17:04:54.278995Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>I_long</th>\n",
       "      <th>M_long</th>\n",
       "      <th>T_long</th>\n",
       "      <th>product_A_smiles</th>\n",
       "      <th>I_smiles</th>\n",
       "      <th>M_smiles</th>\n",
       "      <th>T_smiles</th>\n",
       "      <th>reaction_smiles</th>\n",
       "      <th>reaction_smiles_atom_mapped</th>\n",
       "      <th>experiment_id</th>\n",
       "      <th>...</th>\n",
       "      <th>binary_H</th>\n",
       "      <th>scaled_A</th>\n",
       "      <th>scaled_B</th>\n",
       "      <th>scaled_C</th>\n",
       "      <th>scaled_D</th>\n",
       "      <th>scaled_E</th>\n",
       "      <th>scaled_F</th>\n",
       "      <th>scaled_G</th>\n",
       "      <th>scaled_H</th>\n",
       "      <th>major_A-C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2-Pyr003</td>\n",
       "      <td>Fused002</td>\n",
       "      <td>TerABT004</td>\n",
       "      <td>COc1ccc(CCOC(=O)N2C[C@H](NC(=O)c3cccc(Cl)n3)[C...</td>\n",
       "      <td>O=C(c1cccc(Cl)n1)[B-](F)(F)F.[K+]</td>\n",
       "      <td>COc1ccc(CCOC(=O)N2C[C@@H]3NO[C@]4(OC5(CCCCC5)O...</td>\n",
       "      <td>Nc1ccc(F)cc1S</td>\n",
       "      <td>O=C(c1cccc(Cl)n1)[B-](F)(F)F.COc1ccc(CCOC(=O)N...</td>\n",
       "      <td>F[B-](F)(F)[C:2]([c:1]1[cH:16][cH:18][cH:20][c...</td>\n",
       "      <td>56113</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.036021</td>\n",
       "      <td>0.003427</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.020975</td>\n",
       "      <td>0.002958</td>\n",
       "      <td>0.941981</td>\n",
       "      <td>0.914281</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2-Pyr003</td>\n",
       "      <td>Fused002</td>\n",
       "      <td>TerABT007</td>\n",
       "      <td>COc1ccc(CCOC(=O)N2C[C@H](NC(=O)c3cccc(Cl)n3)[C...</td>\n",
       "      <td>O=C(c1cccc(Cl)n1)[B-](F)(F)F.[K+]</td>\n",
       "      <td>COc1ccc(CCOC(=O)N2C[C@@H]3NO[C@]4(OC5(CCCCC5)O...</td>\n",
       "      <td>Nc1cc(Br)ccc1S</td>\n",
       "      <td>O=C(c1cccc(Cl)n1)[B-](F)(F)F.COc1ccc(CCOC(=O)N...</td>\n",
       "      <td>F[B-](F)(F)[C:2]([c:1]1[cH:16][cH:18][cH:20][c...</td>\n",
       "      <td>56114</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006159</td>\n",
       "      <td>0.364398</td>\n",
       "      <td>0.928851</td>\n",
       "      <td>1.106548</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>no_product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2-Pyr003</td>\n",
       "      <td>Fused002</td>\n",
       "      <td>TerABT013</td>\n",
       "      <td>COc1ccc(CCOC(=O)N2C[C@H](NC(=O)c3cccc(Cl)n3)[C...</td>\n",
       "      <td>O=C(c1cccc(Cl)n1)[B-](F)(F)F.[K+]</td>\n",
       "      <td>COc1ccc(CCOC(=O)N2C[C@@H]3NO[C@]4(OC5(CCCCC5)O...</td>\n",
       "      <td>Nc1cc(C(F)(F)F)ccc1S</td>\n",
       "      <td>O=C(c1cccc(Cl)n1)[B-](F)(F)F.COc1ccc(CCOC(=O)N...</td>\n",
       "      <td>F[B-](F)(F)[C:2]([c:1]1[cH:16][cH:18][cH:20][c...</td>\n",
       "      <td>56106</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.014212</td>\n",
       "      <td>2.166420</td>\n",
       "      <td>1.013596</td>\n",
       "      <td>0.537785</td>\n",
       "      <td>0.05686</td>\n",
       "      <td>no_product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2-Pyr003</td>\n",
       "      <td>Fused002</td>\n",
       "      <td>TerABT014</td>\n",
       "      <td>COc1ccc(CCOC(=O)N2C[C@H](NC(=O)c3cccc(Cl)n3)[C...</td>\n",
       "      <td>O=C(c1cccc(Cl)n1)[B-](F)(F)F.[K+]</td>\n",
       "      <td>COc1ccc(CCOC(=O)N2C[C@@H]3NO[C@]4(OC5(CCCCC5)O...</td>\n",
       "      <td>Nc1ccc(Cl)cc1S</td>\n",
       "      <td>O=C(c1cccc(Cl)n1)[B-](F)(F)F.COc1ccc(CCOC(=O)N...</td>\n",
       "      <td>F[B-](F)(F)[C:2]([c:1]1[cH:16][cH:18][cH:20][c...</td>\n",
       "      <td>56112</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.028915</td>\n",
       "      <td>0.005039</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.015578</td>\n",
       "      <td>0.504057</td>\n",
       "      <td>0.992614</td>\n",
       "      <td>0.890646</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2-Pyr003</td>\n",
       "      <td>Fused002</td>\n",
       "      <td>TerTH001</td>\n",
       "      <td>COc1ccc(CCOC(=O)N2C[C@H](NC(=O)c3cccc(Cl)n3)[C...</td>\n",
       "      <td>O=C(c1cccc(Cl)n1)[B-](F)(F)F.[K+]</td>\n",
       "      <td>COc1ccc(CCOC(=O)N2C[C@@H]3NO[C@]4(OC5(CCCCC5)O...</td>\n",
       "      <td>[Cl-].[NH3+]NC(=S)c1ccccc1</td>\n",
       "      <td>O=C(c1cccc(Cl)n1)[B-](F)(F)F.COc1ccc(CCOC(=O)N...</td>\n",
       "      <td>F[B-](F)(F)[C:2]([c:1]1[cH:13][cH:15][cH:17][c...</td>\n",
       "      <td>56109</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.350061</td>\n",
       "      <td>0.643219</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.031689</td>\n",
       "      <td>0.613596</td>\n",
       "      <td>0.109309</td>\n",
       "      <td>0.439018</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     I_long    M_long     T_long  \\\n",
       "0  2-Pyr003  Fused002  TerABT004   \n",
       "1  2-Pyr003  Fused002  TerABT007   \n",
       "2  2-Pyr003  Fused002  TerABT013   \n",
       "3  2-Pyr003  Fused002  TerABT014   \n",
       "4  2-Pyr003  Fused002   TerTH001   \n",
       "\n",
       "                                    product_A_smiles  \\\n",
       "0  COc1ccc(CCOC(=O)N2C[C@H](NC(=O)c3cccc(Cl)n3)[C...   \n",
       "1  COc1ccc(CCOC(=O)N2C[C@H](NC(=O)c3cccc(Cl)n3)[C...   \n",
       "2  COc1ccc(CCOC(=O)N2C[C@H](NC(=O)c3cccc(Cl)n3)[C...   \n",
       "3  COc1ccc(CCOC(=O)N2C[C@H](NC(=O)c3cccc(Cl)n3)[C...   \n",
       "4  COc1ccc(CCOC(=O)N2C[C@H](NC(=O)c3cccc(Cl)n3)[C...   \n",
       "\n",
       "                            I_smiles  \\\n",
       "0  O=C(c1cccc(Cl)n1)[B-](F)(F)F.[K+]   \n",
       "1  O=C(c1cccc(Cl)n1)[B-](F)(F)F.[K+]   \n",
       "2  O=C(c1cccc(Cl)n1)[B-](F)(F)F.[K+]   \n",
       "3  O=C(c1cccc(Cl)n1)[B-](F)(F)F.[K+]   \n",
       "4  O=C(c1cccc(Cl)n1)[B-](F)(F)F.[K+]   \n",
       "\n",
       "                                            M_smiles  \\\n",
       "0  COc1ccc(CCOC(=O)N2C[C@@H]3NO[C@]4(OC5(CCCCC5)O...   \n",
       "1  COc1ccc(CCOC(=O)N2C[C@@H]3NO[C@]4(OC5(CCCCC5)O...   \n",
       "2  COc1ccc(CCOC(=O)N2C[C@@H]3NO[C@]4(OC5(CCCCC5)O...   \n",
       "3  COc1ccc(CCOC(=O)N2C[C@@H]3NO[C@]4(OC5(CCCCC5)O...   \n",
       "4  COc1ccc(CCOC(=O)N2C[C@@H]3NO[C@]4(OC5(CCCCC5)O...   \n",
       "\n",
       "                     T_smiles  \\\n",
       "0               Nc1ccc(F)cc1S   \n",
       "1              Nc1cc(Br)ccc1S   \n",
       "2        Nc1cc(C(F)(F)F)ccc1S   \n",
       "3              Nc1ccc(Cl)cc1S   \n",
       "4  [Cl-].[NH3+]NC(=S)c1ccccc1   \n",
       "\n",
       "                                     reaction_smiles  \\\n",
       "0  O=C(c1cccc(Cl)n1)[B-](F)(F)F.COc1ccc(CCOC(=O)N...   \n",
       "1  O=C(c1cccc(Cl)n1)[B-](F)(F)F.COc1ccc(CCOC(=O)N...   \n",
       "2  O=C(c1cccc(Cl)n1)[B-](F)(F)F.COc1ccc(CCOC(=O)N...   \n",
       "3  O=C(c1cccc(Cl)n1)[B-](F)(F)F.COc1ccc(CCOC(=O)N...   \n",
       "4  O=C(c1cccc(Cl)n1)[B-](F)(F)F.COc1ccc(CCOC(=O)N...   \n",
       "\n",
       "                         reaction_smiles_atom_mapped experiment_id  ...  \\\n",
       "0  F[B-](F)(F)[C:2]([c:1]1[cH:16][cH:18][cH:20][c...         56113  ...   \n",
       "1  F[B-](F)(F)[C:2]([c:1]1[cH:16][cH:18][cH:20][c...         56114  ...   \n",
       "2  F[B-](F)(F)[C:2]([c:1]1[cH:16][cH:18][cH:20][c...         56106  ...   \n",
       "3  F[B-](F)(F)[C:2]([c:1]1[cH:16][cH:18][cH:20][c...         56112  ...   \n",
       "4  F[B-](F)(F)[C:2]([c:1]1[cH:13][cH:15][cH:17][c...         56109  ...   \n",
       "\n",
       "   binary_H  scaled_A  scaled_B  scaled_C  scaled_D  scaled_E  scaled_F  \\\n",
       "0         0  0.036021  0.003427       0.0  0.020975  0.002958  0.941981   \n",
       "1         0  0.000000  0.000000       0.0  0.006159  0.364398  0.928851   \n",
       "2         1  0.000000  0.000000       0.0  0.014212  2.166420  1.013596   \n",
       "3         0  0.028915  0.005039       0.0  0.015578  0.504057  0.992614   \n",
       "4         0  0.350061  0.643219       0.0  0.031689  0.613596  0.109309   \n",
       "\n",
       "   scaled_G  scaled_H   major_A-C  \n",
       "0  0.914281   0.00000           A  \n",
       "1  1.106548   0.00000  no_product  \n",
       "2  0.537785   0.05686  no_product  \n",
       "3  0.890646   0.00000           A  \n",
       "4  0.439018   0.00000           B  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T17:04:54.346356Z",
     "start_time": "2023-09-07T17:04:54.320879Z"
    }
   },
   "outputs": [],
   "source": [
    "#define a write function that we can reuse later\n",
    "def write_indices_and_stats(indices, sizes, pos_class_A, split_dimension, train_size, save_indices=True):\n",
    "    \"\"\"\n",
    "    Write function that can be reused for the other splits\n",
    "    Args:\n",
    "        indices: list of 3-tuples\n",
    "        sizes: list of 3-tuples, length equal to indices\n",
    "        pos_class_A: list of 3-tuples, length equal to indices\n",
    "        split_dimension: str or int, e.g. \"0\", \"1\", \"2\", \"3\"\n",
    "        save_indices: bool, whether to save the indices. Useful if we need to regenerate statistics. Default: True\n",
    "        train_size: ratio of train samples from all samples. Give as percentage\"\n",
    "    \"\"\"\n",
    "    n_folds = len(indices)\n",
    "    save_dir = DATA_DIR / \"curated_data\" / \"splits\" / f\"{data_name}_{split_dimension}D_split_{train_size}\"\n",
    "    save_dir.mkdir(parents=True, exist_ok=True)\n",
    "    if save_indices:\n",
    "        for i, (idx_train, idx_val, idx_test) in enumerate(indices):\n",
    "            with open(save_dir / f\"fold{i}_train.csv\", \"w\") as f:\n",
    "                f.write(\"index\\n\")\n",
    "                f.write(\"\\n\".join([str(i) for i in idx_train]))\n",
    "                \n",
    "            with open(save_dir / f\"fold{i}_val.csv\", \"w\") as f:\n",
    "                f.write(\"index\\n\")\n",
    "                f.write(\"\\n\".join([str(i) for i in idx_val]))\n",
    "                \n",
    "            with open(save_dir / f\"fold{i}_test.csv\", \"w\") as f:\n",
    "                f.write(\"index\\n\")\n",
    "                f.write(\"\\n\".join([str(i) for i in idx_test]))\n",
    "    \n",
    "    for i, (size, count_A) in enumerate(zip(sizes, pos_class_A)):\n",
    "        with open(save_dir / f\"fold{i}_statistics.txt\", \"w\") as f:\n",
    "            f.write(f\"Train samples: {size[0]} ({size[0]/len(df):.1%})\\n\")\n",
    "            f.write(f\"Val samples: {size[1]} ({size[1]/len(df):.1%})\\n\")\n",
    "            f.write(f\"Test samples: {size[2]} ({size[2]/len(df):.1%})\\n\")\n",
    "            if split_dimension > 1:\n",
    "                f.write(f\"Not used: {len(df) - np.sum(size):.0f} ({(len(df) - np.sum(size)) / len(df):.1%})\\n\")\n",
    "            f.write(f\"Train samples binary_A has label 1: {count_A[0]} ({count_A[0]/size[0]:.1%})\\n\")\n",
    "            f.write(f\"Val samples binary_A has label 1: {count_A[1]} ({count_A[1]/size[1]:.1%})\\n\")\n",
    "            f.write(f\"Test samples binary_A has label 1: {count_A[2]} ({count_A[2]/size[2]:.1%})\\n\")\n",
    "            \n",
    "    # summary statistics\n",
    "    sum_pos_class_A = np.sum(pos_class_A, axis=0)\n",
    "    sum_sizes = np.sum(sizes, axis=0)\n",
    "    with open(save_dir / \"summary_statistics.txt\", \"w\") as f:\n",
    "        f.write(f\"Mean Train samples: {sum_sizes[0] / n_folds:.0f} ({sum_sizes[0] / n_folds / len(df):.1%})\\n\")\n",
    "        f.write(f\"Mean Val samples: {sum_sizes[1] / n_folds:.0f} ({sum_sizes[1] / n_folds / len(df):.1%})\\n\")\n",
    "        f.write(f\"Mean Test samples: {sum_sizes[2] / n_folds:.0f} ({sum_sizes[2] / n_folds / len(df):.1%})\\n\")\n",
    "        if split_dimension > 1:\n",
    "            f.write(f\"Not used: {len(df) - np.sum(sum_sizes) / n_folds:.0f} ({(len(df) - np.sum(sum_sizes) / n_folds) / len(df):.1%})\\n\")\n",
    "        f.write(f\"Mean Train samples binary_A has label 1: {sum_pos_class_A[0] / n_folds:.0f} ({sum_pos_class_A[0]/sum_sizes[0]:.1%})\\n\")\n",
    "        f.write(f\"Mean Val samples binary_A has label 1: {sum_pos_class_A[1] / n_folds:.0f} ({sum_pos_class_A[1]/sum_sizes[1]:.1%})\\n\")\n",
    "        f.write(f\"Mean Test samples binary_A has label 1: {sum_pos_class_A[2] / n_folds:.0f} ({sum_pos_class_A[2]/sum_sizes[2]:.1%})\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0D split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T17:04:54.290774Z",
     "start_time": "2023-09-07T17:04:54.288565Z"
    }
   },
   "outputs": [],
   "source": [
    "splitter = ShuffleSplit(n_splits=9, test_size=0.3, random_state=42)\n",
    "inner_splitter = ShuffleSplit(n_splits=1, test_size=0.3/0.7, random_state=np.random.RandomState(42))  # we use a RandomState instance, not an int, because we will reuse this splitter several times\n",
    "\n",
    "indices = []\n",
    "sizes = []\n",
    "pos_class_A = []\n",
    "for idx_train_val, idx_test in splitter.split(df):\n",
    "    # inner split\n",
    "    train, val = next(inner_splitter.split(idx_train_val))\n",
    "    # use indices to index indices :P (we need to obtain indices referring to the original dataframe)\n",
    "    idx_train = idx_train_val[train]\n",
    "    idx_val = idx_train_val[val]\n",
    "    # add to list\n",
    "    indices.append((idx_train, idx_val, idx_test))\n",
    "    sizes.append((len(idx_train), len(idx_val), len(idx_test)))\n",
    "    pos_class_A.append((np.sum(df['binary_A'][idx_train]), np.sum(df['binary_A'][idx_val]), np.sum(df['binary_A'][idx_test])))\n",
    "\n",
    "print(sizes)\n",
    "print(pos_class_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T17:04:54.450085Z",
     "start_time": "2023-09-07T17:04:54.322854Z"
    }
   },
   "outputs": [],
   "source": [
    "write_indices_and_stats(indices, sizes, pos_class_A, split_dimension=0, save_indices=True, train_size=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T17:04:54.290774Z",
     "start_time": "2023-09-07T17:04:54.288565Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(8003, 16007, 16008), (8003, 16007, 16008), (8003, 16007, 16008), (8003, 16007, 16008), (8003, 16007, 16008), (8003, 16007, 16008), (8003, 16007, 16008), (8003, 16007, 16008), (8003, 16007, 16008)]\n",
      "[(6550, 12944, 13038), (6466, 13069, 12997), (6551, 12973, 13008), (6501, 13031, 13000), (6498, 13019, 13015), (6529, 12949, 13054), (6487, 12956, 13089), (6511, 12983, 13038), (6470, 12997, 13065)]\n"
     ]
    }
   ],
   "source": [
    "splitter = ShuffleSplit(n_splits=9, test_size=0.4, random_state=42)\n",
    "inner_splitter = ShuffleSplit(n_splits=1, test_size=0.4/0.6, random_state=np.random.RandomState(42))  # we use a RandomState instance, not an int, because we will reuse this splitter several times\n",
    "\n",
    "indices = []\n",
    "sizes = []\n",
    "pos_class_A = []\n",
    "for idx_train_val, idx_test in splitter.split(df):\n",
    "    # inner split\n",
    "    train, val = next(inner_splitter.split(idx_train_val))\n",
    "    # use indices to index indices :P (we need to obtain indices referring to the original dataframe)\n",
    "    idx_train = idx_train_val[train]\n",
    "    idx_val = idx_train_val[val]\n",
    "    # add to list\n",
    "    indices.append((idx_train, idx_val, idx_test))\n",
    "    sizes.append((len(idx_train), len(idx_val), len(idx_test)))\n",
    "    pos_class_A.append((np.sum(df['binary_A'][idx_train]), np.sum(df['binary_A'][idx_val]), np.sum(df['binary_A'][idx_test])))\n",
    "\n",
    "print(sizes)\n",
    "print(pos_class_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T17:04:54.450085Z",
     "start_time": "2023-09-07T17:04:54.322854Z"
    }
   },
   "outputs": [],
   "source": [
    "write_indices_and_stats(indices, sizes, pos_class_A, split_dimension=0, save_indices=True, train_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T17:04:54.290774Z",
     "start_time": "2023-09-07T17:04:54.288565Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(4001, 18008, 18009), (4001, 18008, 18009), (4001, 18008, 18009), (4001, 18008, 18009), (4001, 18008, 18009), (4001, 18008, 18009), (4001, 18008, 18009), (4001, 18008, 18009), (4001, 18008, 18009)]\n",
      "[(3258, 14585, 14689), (3258, 14636, 14638), (3262, 14628, 14642), (3285, 14622, 14625), (3272, 14611, 14649), (3248, 14584, 14700), (3208, 14597, 14727), (3266, 14622, 14644), (3264, 14572, 14696)]\n"
     ]
    }
   ],
   "source": [
    "splitter = ShuffleSplit(n_splits=9, test_size=0.45, random_state=42)\n",
    "inner_splitter = ShuffleSplit(n_splits=1, test_size=0.45/0.55, random_state=np.random.RandomState(42))  # we use a RandomState instance, not an int, because we will reuse this splitter several times\n",
    "\n",
    "indices = []\n",
    "sizes = []\n",
    "pos_class_A = []\n",
    "for idx_train_val, idx_test in splitter.split(df):\n",
    "    # inner split\n",
    "    train, val = next(inner_splitter.split(idx_train_val))\n",
    "    # use indices to index indices :P (we need to obtain indices referring to the original dataframe)\n",
    "    idx_train = idx_train_val[train]\n",
    "    idx_val = idx_train_val[val]\n",
    "    # add to list\n",
    "    indices.append((idx_train, idx_val, idx_test))\n",
    "    sizes.append((len(idx_train), len(idx_val), len(idx_test)))\n",
    "    pos_class_A.append((np.sum(df['binary_A'][idx_train]), np.sum(df['binary_A'][idx_val]), np.sum(df['binary_A'][idx_test])))\n",
    "\n",
    "print(sizes)\n",
    "print(pos_class_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T17:04:54.450085Z",
     "start_time": "2023-09-07T17:04:54.322854Z"
    }
   },
   "outputs": [],
   "source": [
    "write_indices_and_stats(indices, sizes, pos_class_A, split_dimension=0, save_indices=True, train_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T17:04:54.290774Z",
     "start_time": "2023-09-07T17:04:54.288565Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2000, 20009, 18009), (2000, 20009, 18009), (2000, 20009, 18009), (2000, 20009, 18009), (2000, 20009, 18009), (2000, 20009, 18009), (2000, 20009, 18009), (2000, 20009, 18009), (2000, 20009, 18009)]\n",
      "[(1639, 16204, 14689), (1626, 16268, 14638), (1644, 16246, 14642), (1639, 16268, 14625), (1642, 16241, 14649), (1632, 16200, 14700), (1600, 16205, 14727), (1615, 16273, 14644), (1630, 16206, 14696)]\n"
     ]
    }
   ],
   "source": [
    "splitter = ShuffleSplit(n_splits=9, test_size=0.45, random_state=42)\n",
    "inner_splitter = ShuffleSplit(n_splits=1, test_size=0.5/0.55, random_state=np.random.RandomState(42))  # we use a RandomState instance, not an int, because we will reuse this splitter several times\n",
    "\n",
    "indices = []\n",
    "sizes = []\n",
    "pos_class_A = []\n",
    "for idx_train_val, idx_test in splitter.split(df):\n",
    "    # inner split\n",
    "    train, val = next(inner_splitter.split(idx_train_val))\n",
    "    # use indices to index indices :P (we need to obtain indices referring to the original dataframe)\n",
    "    idx_train = idx_train_val[train]\n",
    "    idx_val = idx_train_val[val]\n",
    "    # add to list\n",
    "    indices.append((idx_train, idx_val, idx_test))\n",
    "    sizes.append((len(idx_train), len(idx_val), len(idx_test)))\n",
    "    pos_class_A.append((np.sum(df['binary_A'][idx_train]), np.sum(df['binary_A'][idx_val]), np.sum(df['binary_A'][idx_test])))\n",
    "\n",
    "print(sizes)\n",
    "print(pos_class_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T17:04:54.450085Z",
     "start_time": "2023-09-07T17:04:54.322854Z"
    }
   },
   "outputs": [],
   "source": [
    "write_indices_and_stats(indices, sizes, pos_class_A, split_dimension=0, save_indices=True, train_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T17:04:54.290774Z",
     "start_time": "2023-09-07T17:04:54.288565Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1000, 20009, 19009), (1000, 20009, 19009), (1000, 20009, 19009), (1000, 20009, 19009), (1000, 20009, 19009), (1000, 20009, 19009), (1000, 20009, 19009), (1000, 20009, 19009), (1000, 20009, 19009)]\n",
      "[(830, 16213, 15489), (810, 16280, 15442), (803, 16283, 15446), (819, 16298, 15415), (813, 16271, 15448), (828, 16174, 15530), (825, 16155, 15552), (814, 16268, 15450), (838, 16198, 15496)]\n"
     ]
    }
   ],
   "source": [
    "splitter = ShuffleSplit(n_splits=9, test_size=0.475, random_state=42)\n",
    "inner_splitter = ShuffleSplit(n_splits=1, test_size=0.5/0.525, random_state=np.random.RandomState(42))  # we use a RandomState instance, not an int, because we will reuse this splitter several times\n",
    "\n",
    "indices = []\n",
    "sizes = []\n",
    "pos_class_A = []\n",
    "for idx_train_val, idx_test in splitter.split(df):\n",
    "    # inner split\n",
    "    train, val = next(inner_splitter.split(idx_train_val))\n",
    "    # use indices to index indices :P (we need to obtain indices referring to the original dataframe)\n",
    "    idx_train = idx_train_val[train]\n",
    "    idx_val = idx_train_val[val]\n",
    "    # add to list\n",
    "    indices.append((idx_train, idx_val, idx_test))\n",
    "    sizes.append((len(idx_train), len(idx_val), len(idx_test)))\n",
    "    pos_class_A.append((np.sum(df['binary_A'][idx_train]), np.sum(df['binary_A'][idx_val]), np.sum(df['binary_A'][idx_test])))\n",
    "\n",
    "print(sizes)\n",
    "print(pos_class_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T17:04:54.450085Z",
     "start_time": "2023-09-07T17:04:54.322854Z"
    }
   },
   "outputs": [],
   "source": [
    "write_indices_and_stats(indices, sizes, pos_class_A, split_dimension=0, save_indices=True, train_size=2.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T17:04:54.290774Z",
     "start_time": "2023-09-07T17:04:54.288565Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(500, 20009, 19509), (500, 20009, 19509), (500, 20009, 19509), (500, 20009, 19509), (500, 20009, 19509), (500, 20009, 19509), (500, 20009, 19509), (500, 20009, 19509), (500, 20009, 19509)]\n",
      "[(425, 16221, 15886), (409, 16283, 15840), (399, 16286, 15847), (421, 16315, 15796), (416, 16251, 15865), (408, 16193, 15931), (394, 16192, 15946), (401, 16269, 15862), (408, 16223, 15901)]\n"
     ]
    }
   ],
   "source": [
    "splitter = ShuffleSplit(n_splits=9, test_size=0.4875, random_state=42)\n",
    "inner_splitter = ShuffleSplit(n_splits=1, test_size=0.5/0.5125, random_state=np.random.RandomState(42))  # we use a RandomState instance, not an int, because we will reuse this splitter several times\n",
    "\n",
    "indices = []\n",
    "sizes = []\n",
    "pos_class_A = []\n",
    "for idx_train_val, idx_test in splitter.split(df):\n",
    "    # inner split\n",
    "    train, val = next(inner_splitter.split(idx_train_val))\n",
    "    # use indices to index indices :P (we need to obtain indices referring to the original dataframe)\n",
    "    idx_train = idx_train_val[train]\n",
    "    idx_val = idx_train_val[val]\n",
    "    # add to list\n",
    "    indices.append((idx_train, idx_val, idx_test))\n",
    "    sizes.append((len(idx_train), len(idx_val), len(idx_test)))\n",
    "    pos_class_A.append((np.sum(df['binary_A'][idx_train]), np.sum(df['binary_A'][idx_val]), np.sum(df['binary_A'][idx_test])))\n",
    "\n",
    "print(sizes)\n",
    "print(pos_class_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T17:04:54.450085Z",
     "start_time": "2023-09-07T17:04:54.322854Z"
    }
   },
   "outputs": [],
   "source": [
    "write_indices_and_stats(indices, sizes, pos_class_A, split_dimension=0, save_indices=True, train_size=1.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1D split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T17:04:54.453760Z",
     "start_time": "2023-09-07T17:04:54.450483Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(30974, 4318, 4726), (32233, 3546, 4239), (32488, 4020, 3510), (29520, 6094, 4404), (31196, 4704, 4118), (32186, 4686, 3146), (30353, 4855, 4810), (30367, 5326, 4325), (31930, 4193, 3895)]\n",
      "[(25183, 3332, 4017), (26439, 2886, 3207), (26725, 3018, 2789), (23667, 5073, 3792), (25283, 3811, 3438), (26321, 4005, 2206), (24318, 4093, 4121), (24334, 4641, 3557), (25753, 3495, 3284)]\n"
     ]
    }
   ],
   "source": [
    "splitter = GroupShuffleSplit(n_splits=9, test_size=0.1, random_state=42)\n",
    "inner_splitter = GroupShuffleSplit(n_splits=1, test_size=0.1/0.9, random_state=np.random.RandomState(42))  # we use a RandomState instance, not an int, because we will reuse this splitter several times\n",
    "\n",
    "indices = []\n",
    "sizes = []\n",
    "pos_class_A = []\n",
    "for idx_train_val, idx_test in splitter.split(list(range(len(df))), groups=df[\"I_long\"]):\n",
    "    # inner split\n",
    "    train, val = next(inner_splitter.split(idx_train_val, groups=df[\"I_long\"][idx_train_val]))\n",
    "    # use indices to index indices :P (we need to obtain indices referring to the original dataframe)\n",
    "    idx_train = idx_train_val[train]\n",
    "    idx_val = idx_train_val[val]\n",
    "    # add to list\n",
    "    indices.append((idx_train, idx_val, idx_test))\n",
    "    sizes.append((len(idx_train), len(idx_val), len(idx_test)))\n",
    "    pos_class_A.append((np.sum(df['binary_A'][idx_train]), np.sum(df['binary_A'][idx_val]), np.sum(df['binary_A'][idx_test])))\n",
    "print(sizes)\n",
    "print(pos_class_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T17:04:54.758200Z",
     "start_time": "2023-09-07T17:04:54.631365Z"
    }
   },
   "outputs": [],
   "source": [
    "write_indices_and_stats(indices, sizes, pos_class_A, split_dimension=1, save_indices=True, train_size=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T17:04:54.453760Z",
     "start_time": "2023-09-07T17:04:54.450483Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(17126, 10733, 12159), (16875, 11529, 11614), (15464, 12107, 12447), (15857, 11436, 12725), (15219, 12660, 12139), (15958, 12613, 11447), (14167, 11735, 14116), (16400, 11582, 12036), (13764, 13270, 12984)]\n",
      "[(13955, 8318, 10259), (13586, 9676, 9270), (12577, 9769, 10186), (13126, 9179, 10227), (12787, 10139, 9606), (13432, 10341, 8759), (11156, 9398, 11978), (13012, 9529, 9991), (10817, 11257, 10458)]\n"
     ]
    }
   ],
   "source": [
    "splitter = GroupShuffleSplit(n_splits=9, test_size=0.3, random_state=42)\n",
    "inner_splitter = GroupShuffleSplit(n_splits=1, test_size=0.3/0.7, random_state=np.random.RandomState(42))  # we use a RandomState instance, not an int, because we will reuse this splitter several times\n",
    "\n",
    "indices = []\n",
    "sizes = []\n",
    "pos_class_A = []\n",
    "for idx_train_val, idx_test in splitter.split(list(range(len(df))), groups=df[\"I_long\"]):\n",
    "    # inner split\n",
    "    train, val = next(inner_splitter.split(idx_train_val, groups=df[\"I_long\"][idx_train_val]))\n",
    "    # use indices to index indices :P (we need to obtain indices referring to the original dataframe)\n",
    "    idx_train = idx_train_val[train]\n",
    "    idx_val = idx_train_val[val]\n",
    "    # add to list\n",
    "    indices.append((idx_train, idx_val, idx_test))\n",
    "    sizes.append((len(idx_train), len(idx_val), len(idx_test)))\n",
    "    pos_class_A.append((np.sum(df['binary_A'][idx_train]), np.sum(df['binary_A'][idx_val]), np.sum(df['binary_A'][idx_test])))\n",
    "print(sizes)\n",
    "print(pos_class_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T17:04:54.758200Z",
     "start_time": "2023-09-07T17:04:54.631365Z"
    }
   },
   "outputs": [],
   "source": [
    "write_indices_and_stats(indices, sizes, pos_class_A, split_dimension=1, save_indices=True, train_size=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T17:04:54.453760Z",
     "start_time": "2023-09-07T17:04:54.450483Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(8818, 16152, 15048), (9070, 14595, 16353), (6805, 17281, 15932), (8445, 15367, 16206), (8844, 15312, 15862), (8004, 15372, 16642), (7254, 15824, 16940), (7631, 15875, 16512), (8090, 15386, 16542)]\n",
      "[(7327, 13007, 12198), (7765, 11387, 13380), (5521, 14416, 12595), (7227, 12271, 13034), (7406, 12375, 12751), (6713, 12612, 13207), (5823, 12409, 14300), (6262, 12499, 13771), (6508, 12661, 13363)]\n"
     ]
    }
   ],
   "source": [
    "splitter = GroupShuffleSplit(n_splits=9, test_size=0.4, random_state=42)\n",
    "inner_splitter = GroupShuffleSplit(n_splits=1, test_size=0.4/0.6, random_state=np.random.RandomState(42))  # we use a RandomState instance, not an int, because we will reuse this splitter several times\n",
    "\n",
    "indices = []\n",
    "sizes = []\n",
    "pos_class_A = []\n",
    "for idx_train_val, idx_test in splitter.split(list(range(len(df))), groups=df[\"I_long\"]):\n",
    "    # inner split\n",
    "    train, val = next(inner_splitter.split(idx_train_val, groups=df[\"I_long\"][idx_train_val]))\n",
    "    # use indices to index indices :P (we need to obtain indices referring to the original dataframe)\n",
    "    idx_train = idx_train_val[train]\n",
    "    idx_val = idx_train_val[val]\n",
    "    # add to list\n",
    "    indices.append((idx_train, idx_val, idx_test))\n",
    "    sizes.append((len(idx_train), len(idx_val), len(idx_test)))\n",
    "    pos_class_A.append((np.sum(df['binary_A'][idx_train]), np.sum(df['binary_A'][idx_val]), np.sum(df['binary_A'][idx_test])))\n",
    "print(sizes)\n",
    "print(pos_class_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T17:04:54.758200Z",
     "start_time": "2023-09-07T17:04:54.631365Z"
    }
   },
   "outputs": [],
   "source": [
    "write_indices_and_stats(indices, sizes, pos_class_A, split_dimension=1, save_indices=True, train_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T17:04:54.453760Z",
     "start_time": "2023-09-07T17:04:54.450483Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3567, 18651, 17800), (4020, 17884, 18114), (4219, 17970, 17829), (3452, 17898, 18668), (4571, 16516, 18931), (3014, 17745, 19259), (2607, 18079, 19332), (3703, 16819, 19496), (3970, 17106, 18942)]\n",
      "[(2839, 15154, 14539), (3381, 14306, 14845), (3556, 14845, 14131), (2843, 14698, 14991), (3966, 13229, 15337), (2433, 14651, 15448), (2127, 14232, 16173), (3037, 13186, 16309), (3279, 13820, 15433)]\n"
     ]
    }
   ],
   "source": [
    "splitter = GroupShuffleSplit(n_splits=9, test_size=0.45, random_state=42)\n",
    "inner_splitter = GroupShuffleSplit(n_splits=1, test_size=0.45/0.55, random_state=np.random.RandomState(42))  # we use a RandomState instance, not an int, because we will reuse this splitter several times\n",
    "\n",
    "indices = []\n",
    "sizes = []\n",
    "pos_class_A = []\n",
    "for idx_train_val, idx_test in splitter.split(list(range(len(df))), groups=df[\"I_long\"]):\n",
    "    # inner split\n",
    "    train, val = next(inner_splitter.split(idx_train_val, groups=df[\"I_long\"][idx_train_val]))\n",
    "    # use indices to index indices :P (we need to obtain indices referring to the original dataframe)\n",
    "    idx_train = idx_train_val[train]\n",
    "    idx_val = idx_train_val[val]\n",
    "    # add to list\n",
    "    indices.append((idx_train, idx_val, idx_test))\n",
    "    sizes.append((len(idx_train), len(idx_val), len(idx_test)))\n",
    "    pos_class_A.append((np.sum(df['binary_A'][idx_train]), np.sum(df['binary_A'][idx_val]), np.sum(df['binary_A'][idx_test])))\n",
    "print(sizes)\n",
    "print(pos_class_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T17:04:54.758200Z",
     "start_time": "2023-09-07T17:04:54.631365Z"
    }
   },
   "outputs": [],
   "source": [
    "write_indices_and_stats(indices, sizes, pos_class_A, split_dimension=1, save_indices=True, train_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T17:04:54.453760Z",
     "start_time": "2023-09-07T17:04:54.450483Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1447, 20771, 17800), (2409, 19495, 18114), (1278, 20911, 17829), (1983, 19367, 18668), (1643, 19444, 18931), (1721, 19038, 19259), (1240, 19446, 19332), (1243, 19279, 19496), (2046, 19030, 18942)]\n",
      "[(1081, 16912, 14539), (2068, 15619, 14845), (1023, 17378, 14131), (1696, 15845, 14991), (1359, 15836, 15337), (1448, 15636, 15448), (1015, 15344, 16173), (980, 15243, 16309), (1710, 15389, 15433)]\n"
     ]
    }
   ],
   "source": [
    "splitter = GroupShuffleSplit(n_splits=9, test_size=0.45, random_state=42)\n",
    "inner_splitter = GroupShuffleSplit(n_splits=1, test_size=0.5/0.55, random_state=np.random.RandomState(42))  # we use a RandomState instance, not an int, because we will reuse this splitter several times\n",
    "\n",
    "indices = []\n",
    "sizes = []\n",
    "pos_class_A = []\n",
    "for idx_train_val, idx_test in splitter.split(list(range(len(df))), groups=df[\"I_long\"]):\n",
    "    # inner split\n",
    "    train, val = next(inner_splitter.split(idx_train_val, groups=df[\"I_long\"][idx_train_val]))\n",
    "    # use indices to index indices :P (we need to obtain indices referring to the original dataframe)\n",
    "    idx_train = idx_train_val[train]\n",
    "    idx_val = idx_train_val[val]\n",
    "    # add to list\n",
    "    indices.append((idx_train, idx_val, idx_test))\n",
    "    sizes.append((len(idx_train), len(idx_val), len(idx_test)))\n",
    "    pos_class_A.append((np.sum(df['binary_A'][idx_train]), np.sum(df['binary_A'][idx_val]), np.sum(df['binary_A'][idx_test])))\n",
    "print(sizes)\n",
    "print(pos_class_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T17:04:54.758200Z",
     "start_time": "2023-09-07T17:04:54.631365Z"
    }
   },
   "outputs": [],
   "source": [
    "write_indices_and_stats(indices, sizes, pos_class_A, split_dimension=1, save_indices=True, train_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T17:04:54.453760Z",
     "start_time": "2023-09-07T17:04:54.450483Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(202, 21461, 18355), (947, 20029, 19042), (483, 21397, 18138), (609, 19716, 19693), (626, 19940, 19452), (472, 19775, 19771), (202, 20239, 19577), (484, 19470, 20064), (359, 20234, 19425)]\n",
      "[(147, 17398, 14987), (821, 16126, 15585), (382, 17770, 14380), (537, 16114, 15881), (501, 16225, 15806), (381, 16344, 15807), (147, 16026, 16359), (387, 15325, 16820), (289, 16428, 15815)]\n"
     ]
    }
   ],
   "source": [
    "splitter = GroupShuffleSplit(n_splits=9, test_size=0.475, random_state=42)\n",
    "inner_splitter = GroupShuffleSplit(n_splits=1, test_size=0.5/0.525, random_state=np.random.RandomState(42))  # we use a RandomState instance, not an int, because we will reuse this splitter several times\n",
    "\n",
    "indices = []\n",
    "sizes = []\n",
    "pos_class_A = []\n",
    "for idx_train_val, idx_test in splitter.split(list(range(len(df))), groups=df[\"I_long\"]):\n",
    "    # inner split\n",
    "    train, val = next(inner_splitter.split(idx_train_val, groups=df[\"I_long\"][idx_train_val]))\n",
    "    # use indices to index indices :P (we need to obtain indices referring to the original dataframe)\n",
    "    idx_train = idx_train_val[train]\n",
    "    idx_val = idx_train_val[val]\n",
    "    # add to list\n",
    "    indices.append((idx_train, idx_val, idx_test))\n",
    "    sizes.append((len(idx_train), len(idx_val), len(idx_test)))\n",
    "    pos_class_A.append((np.sum(df['binary_A'][idx_train]), np.sum(df['binary_A'][idx_val]), np.sum(df['binary_A'][idx_test])))\n",
    "print(sizes)\n",
    "print(pos_class_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T17:04:54.758200Z",
     "start_time": "2023-09-07T17:04:54.631365Z"
    }
   },
   "outputs": [],
   "source": [
    "write_indices_and_stats(indices, sizes, pos_class_A, split_dimension=1, save_indices=True, train_size=2.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T17:04:54.453760Z",
     "start_time": "2023-09-07T17:04:54.450483Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "With n_samples=34, test_size=0.9756097560975611 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m pos_class_A \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx_train_val, idx_test \u001b[38;5;129;01min\u001b[39;00m splitter\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(df))), groups\u001b[38;5;241m=\u001b[39mdf[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI_long\u001b[39m\u001b[38;5;124m\"\u001b[39m]):\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# inner split\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m     train, val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minner_splitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx_train_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mI_long\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx_train_val\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# use indices to index indices :P (we need to obtain indices referring to the original dataframe)\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     idx_train \u001b[38;5;241m=\u001b[39m idx_train_val[train]\n",
      "File \u001b[0;32m~/miniconda3/envs/library-generation/lib/python3.9/site-packages/sklearn/model_selection/_split.py:1726\u001b[0m, in \u001b[0;36mBaseShuffleSplit.split\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m   1696\u001b[0m \u001b[38;5;124;03m\"\"\"Generate indices to split data into training and test set.\u001b[39;00m\n\u001b[1;32m   1697\u001b[0m \n\u001b[1;32m   1698\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1723\u001b[0m \u001b[38;5;124;03mto an integer.\u001b[39;00m\n\u001b[1;32m   1724\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1725\u001b[0m X, y, groups \u001b[38;5;241m=\u001b[39m indexable(X, y, groups)\n\u001b[0;32m-> 1726\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m train, test \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter_indices(X, y, groups):\n\u001b[1;32m   1727\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m train, test\n",
      "File \u001b[0;32m~/miniconda3/envs/library-generation/lib/python3.9/site-packages/sklearn/model_selection/_split.py:1972\u001b[0m, in \u001b[0;36mGroupShuffleSplit._iter_indices\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m   1970\u001b[0m groups \u001b[38;5;241m=\u001b[39m check_array(groups, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgroups\u001b[39m\u001b[38;5;124m\"\u001b[39m, ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m   1971\u001b[0m classes, group_indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(groups, return_inverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m-> 1972\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m group_train, group_test \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m_iter_indices(X\u001b[38;5;241m=\u001b[39mclasses):\n\u001b[1;32m   1973\u001b[0m     \u001b[38;5;66;03m# these are the indices of classes in the partition\u001b[39;00m\n\u001b[1;32m   1974\u001b[0m     \u001b[38;5;66;03m# invert them into data indices\u001b[39;00m\n\u001b[1;32m   1976\u001b[0m     train \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mflatnonzero(np\u001b[38;5;241m.\u001b[39min1d(group_indices, group_train))\n\u001b[1;32m   1977\u001b[0m     test \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mflatnonzero(np\u001b[38;5;241m.\u001b[39min1d(group_indices, group_test))\n",
      "File \u001b[0;32m~/miniconda3/envs/library-generation/lib/python3.9/site-packages/sklearn/model_selection/_split.py:1859\u001b[0m, in \u001b[0;36mShuffleSplit._iter_indices\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m   1857\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_iter_indices\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, groups\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1858\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(X)\n\u001b[0;32m-> 1859\u001b[0m     n_train, n_test \u001b[38;5;241m=\u001b[39m \u001b[43m_validate_shuffle_split\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1860\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1861\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1862\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1863\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdefault_test_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_default_test_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1864\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1866\u001b[0m     rng \u001b[38;5;241m=\u001b[39m check_random_state(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandom_state)\n\u001b[1;32m   1867\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_splits):\n\u001b[1;32m   1868\u001b[0m         \u001b[38;5;66;03m# random partition\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/library-generation/lib/python3.9/site-packages/sklearn/model_selection/_split.py:2273\u001b[0m, in \u001b[0;36m_validate_shuffle_split\u001b[0;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[1;32m   2270\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(n_train), \u001b[38;5;28mint\u001b[39m(n_test)\n\u001b[1;32m   2272\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_train \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2273\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2274\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWith n_samples=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, test_size=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m and train_size=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2275\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresulting train set will be empty. Adjust any of the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2276\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maforementioned parameters.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(n_samples, test_size, train_size)\n\u001b[1;32m   2277\u001b[0m     )\n\u001b[1;32m   2279\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m n_train, n_test\n",
      "\u001b[0;31mValueError\u001b[0m: With n_samples=34, test_size=0.9756097560975611 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."
     ]
    }
   ],
   "source": [
    "splitter = GroupShuffleSplit(n_splits=9, test_size=0.4875, random_state=42)\n",
    "inner_splitter = GroupShuffleSplit(n_splits=1, test_size=0.5/0.5125, random_state=np.random.RandomState(42))  # we use a RandomState instance, not an int, because we will reuse this splitter several times\n",
    "\n",
    "indices = []\n",
    "sizes = []\n",
    "pos_class_A = []\n",
    "for idx_train_val, idx_test in splitter.split(list(range(len(df))), groups=df[\"I_long\"]):\n",
    "    # inner split\n",
    "    train, val = next(inner_splitter.split(idx_train_val, groups=df[\"I_long\"][idx_train_val]))\n",
    "    # use indices to index indices :P (we need to obtain indices referring to the original dataframe)\n",
    "    idx_train = idx_train_val[train]\n",
    "    idx_val = idx_train_val[val]\n",
    "    # add to list\n",
    "    indices.append((idx_train, idx_val, idx_test))\n",
    "    sizes.append((len(idx_train), len(idx_val), len(idx_test)))\n",
    "    pos_class_A.append((np.sum(df['binary_A'][idx_train]), np.sum(df['binary_A'][idx_val]), np.sum(df['binary_A'][idx_test])))\n",
    "print(sizes)\n",
    "print(pos_class_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T17:04:54.758200Z",
     "start_time": "2023-09-07T17:04:54.631365Z"
    }
   },
   "outputs": [],
   "source": [
    "write_indices_and_stats(indices, sizes, pos_class_A, split_dimension=1, save_indices=True, train_size=40)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
