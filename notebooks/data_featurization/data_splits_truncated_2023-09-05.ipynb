{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Splits Truncated\n",
    "Here, we split in the same way as in `data_splits.ipynb`, but then we restrict the number of points in the training data.\n",
    "There are two ways we could do this:\n",
    "1. Take the original splits and drop random examples. Problem: As we go to smaller training sets, this will lead to us inadvertently removing all the examples of one compound from the training data but not from validation/test.\n",
    "2. We do (procedurally) the same splits as before, but with different train-test ratios.\n",
    "\n",
    "Option 2 is safer to do w.r.t. retaining the split dimensionality.\n",
    "\n",
    "### 0D Split\n",
    "For the 0D split, we use a random train-test split.\n",
    "Standard: We use a 80/10/10 split into train, val, and test set.\n",
    "\n",
    "Truncated: \n",
    "- 40/30/30 -> 16k training samples\n",
    "- 20/40/40 -> 8k training samples\n",
    "- 10/45/45 -> 4k training samples\n",
    "- 5/50/45 -> 2k training samples\n",
    "- 2.5/50/47.5 -> 1k training samples\n",
    "- 1.25/50/48.75 -> 500 training samples\n",
    "- 0.625/50/49.375 -> 250 training samples\n",
    "\n",
    "(numbers of training samples are averaged over folds and approximate)\n",
    "\n",
    "### 1D Split\n",
    "For the 1D split, we use a (1D) GroupShuffleSplit.\n",
    "Standard: As groups, we use either initiator, monomer, or terminator (3x3 splits).\n",
    "\n",
    "Truncated:\n",
    "(for uniform results, we pick only one dimension to split on: Initiators)\n",
    "- 80/10/10 -> 31,250 training samples (53 I) (this is not equivalent to the \"standard\" 1D split b/c here we split on initiators 9 times)\n",
    "- 40/30/30 -> 15,600 training samples (26 I)\n",
    "- 20/40/40 -> 8,100 training samples (13 I)\n",
    "- 10/45/45 -> 3,680 training samples (6 I)\n",
    "- 5/50/45 -> 1,668 training samples (3 I)\n",
    "- 2.5/50/47.5 -> 487 training samples (1 I)\n",
    "- 1.25/50/48.75 -> not possible b/c training set will be empty on some folds\n",
    "\n",
    "(numbers of training samples are averaged over folds)\n",
    "\n",
    "### 2D split\n",
    "For the 2D split, we use a (2D) GroupShuffleSplit.\n",
    "Standard: As groups, we use either [initiator, monomer], [monomer, terminator] or [initiator, terminator]. (3x3)\n",
    "\n",
    "Truncated:\n",
    "(for uniform results, we pick only one set of two dimensions to split on: Initiators & Monomers)\n",
    "- 80/10/10 -> 24,562 training samples (53 I, 56 M)\n",
    "- 60/20/20 -> 13,802 training samples (39 I, 41.7 M)\n",
    "- 40/30/30 -> 6,046 training samples (26 I, 27.9 M)\n",
    "- 30/35/35 -> 3,193 training samples (19 I, 20.6 M)\n",
    "- 20/40/40 -> 1,475 training samples (13 I, 13.7 M)\n",
    "- 15/45/40 -> 812 training samples (10 I, 10 M)\n",
    "- 10/45/45 -> 355 training samples (6 I, 6.7 M)\n",
    "- 7.5/47.5/45 -> 163 training samples (4 I, 4.9 M)\n",
    "- 5/50/45 -> 78 training samples (3 I, 2.9 M)\n",
    "- 2/(all-2)/48 -> 38 training samples (2 I, 1.8 M) (here we define the train/val split such that 2 groups are in train)\n",
    "\n",
    "(numbers of training samples are averaged over folds. In total there are 67 I & 72 M)\n",
    "\n",
    "### 3D split\n",
    "For the 3D split, we use a (3D) GroupShuffleSplit.\n",
    "\n",
    "Truncated:\n",
    "- 80/10/10 -> 19,725 training samples (53 I, 55.8 M, 32 T) n.b. this has extremely few val/test samples\n",
    "- 70/15/15 -> 12,975 training samples (46 I, 49.4 M, 28 T)\n",
    "- 60/20/20 -> 7,948 training samples (39 I, 41.4 M, 24 T) (this should be close to the standard split)\n",
    "- 50/25/25 -> 4,747 training samples (33 I, 35.5 M, 20 T)\n",
    "- 40/30/30 -> 2,399 training samples (26 I, 27.8 M, 16 T)\n",
    "- 34/33/33 -> 1,313 training samples (22 I, 23.3 M, 13 T)\n",
    "- 30/35/35 -> 901 training samples (19 I, 20.3 M, 12 T)\n",
    "- 25/40/35 -> 529 training samples (16 I, 16.9 M, 10 T)\n",
    "- 20/40/40 -> 273 training samples (13 I, 13.3 M, 8 T)\n",
    "- 15/45/40 -> 112 training samples (10 I, 9.9 M, 6 T)\n",
    "- 10/45/45 -> 32 training samples (5.8 I, 6 M, 4 T)\n",
    "\n",
    "## Diasteromers\n",
    "For the 0D split, this is not important, but for 1D/2D/3D, we want to define the groups such that diastereomers will always be in the same group. These splits will receive a `_dia` suffix\n",
    "\n",
    "## Synthetic data\n",
    "For the 1D/2D/3D problems we use a synthetically amended data set. Splits of the synthetically ammended data set will receive a `_synthetic` suffix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import sys\n",
    "\n",
    "sys.path.append(str(pathlib.Path().resolve().parents[1]))\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GroupShuffleSplit, ShuffleSplit\n",
    "\n",
    "from src.definitions import DATA_DIR\n",
    "from src.util.train_test_split import GroupShuffleSplitND\n",
    "from util import write_indices_and_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data_filename = \"synferm_dataset_2023-09-05_40018records.csv\"\n",
    "data_name = data_filename.rsplit(\"_\", maxsplit=1)[0]\n",
    "df = pd.read_csv(DATA_DIR / \"curated_data\" / data_filename)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0D split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = ShuffleSplit(n_splits=9, test_size=0.3, random_state=42)\n",
    "inner_splitter = ShuffleSplit(n_splits=1, test_size=0.3/0.7, random_state=np.random.RandomState(42))  # we use a RandomState instance, not an int, because we will reuse this splitter several times\n",
    "\n",
    "indices = []\n",
    "sizes = []\n",
    "pos_class = []\n",
    "for idx_train_val, idx_test in splitter.split(df):\n",
    "    # inner split\n",
    "    train, val = next(inner_splitter.split(idx_train_val))\n",
    "    # use indices to index indices :P (we need to obtain indices referring to the original dataframe)\n",
    "    idx_train = idx_train_val[train]\n",
    "    idx_val = idx_train_val[val]\n",
    "    # add to list\n",
    "    indices.append((idx_train, idx_val, idx_test))\n",
    "    sizes.append((len(idx_train), len(idx_val), len(idx_test)))\n",
    "    pos_class.append(\n",
    "        (np.sum(df[['binary_A', 'binary_B', 'binary_C']].loc[idx_train]).to_numpy(), \n",
    "         np.sum(df[['binary_A', 'binary_B', 'binary_C']].loc[idx_val]).to_numpy(), \n",
    "         np.sum(df[['binary_A', 'binary_B', 'binary_C']].loc[idx_test]).to_numpy(),\n",
    "        )\n",
    "    )\n",
    "\n",
    "print(sizes)\n",
    "print(pos_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_indices_and_stats(\n",
    "    indices, sizes, pos_class, split_dimension=0, save_indices=True, train_size=40, total_size=len(df), data_name=data_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = ShuffleSplit(n_splits=9, test_size=0.4, random_state=42)\n",
    "inner_splitter = ShuffleSplit(n_splits=1, test_size=0.4/0.6, random_state=np.random.RandomState(42))  # we use a RandomState instance, not an int, because we will reuse this splitter several times\n",
    "\n",
    "indices = []\n",
    "sizes = []\n",
    "pos_class = []\n",
    "for idx_train_val, idx_test in splitter.split(df):\n",
    "    # inner split\n",
    "    train, val = next(inner_splitter.split(idx_train_val))\n",
    "    # use indices to index indices :P (we need to obtain indices referring to the original dataframe)\n",
    "    idx_train = idx_train_val[train]\n",
    "    idx_val = idx_train_val[val]\n",
    "    # add to list\n",
    "    indices.append((idx_train, idx_val, idx_test))\n",
    "    sizes.append((len(idx_train), len(idx_val), len(idx_test)))\n",
    "    pos_class.append(\n",
    "        (np.sum(df[['binary_A', 'binary_B', 'binary_C']].loc[idx_train]).to_numpy(), \n",
    "         np.sum(df[['binary_A', 'binary_B', 'binary_C']].loc[idx_val]).to_numpy(), \n",
    "         np.sum(df[['binary_A', 'binary_B', 'binary_C']].loc[idx_test]).to_numpy(),\n",
    "        )\n",
    "    )\n",
    "\n",
    "print(sizes)\n",
    "print(pos_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_indices_and_stats(\n",
    "    indices, sizes, pos_class, split_dimension=0, save_indices=True, train_size=20, total_size=len(df), data_name=data_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = ShuffleSplit(n_splits=9, test_size=0.45, random_state=42)\n",
    "inner_splitter = ShuffleSplit(n_splits=1, test_size=0.45/0.55, random_state=np.random.RandomState(42))  # we use a RandomState instance, not an int, because we will reuse this splitter several times\n",
    "\n",
    "indices = []\n",
    "sizes = []\n",
    "pos_class = []\n",
    "for idx_train_val, idx_test in splitter.split(df):\n",
    "    # inner split\n",
    "    train, val = next(inner_splitter.split(idx_train_val))\n",
    "    # use indices to index indices :P (we need to obtain indices referring to the original dataframe)\n",
    "    idx_train = idx_train_val[train]\n",
    "    idx_val = idx_train_val[val]\n",
    "    # add to list\n",
    "    indices.append((idx_train, idx_val, idx_test))\n",
    "    sizes.append((len(idx_train), len(idx_val), len(idx_test)))\n",
    "    pos_class.append(\n",
    "        (np.sum(df[['binary_A', 'binary_B', 'binary_C']].loc[idx_train]).to_numpy(), \n",
    "         np.sum(df[['binary_A', 'binary_B', 'binary_C']].loc[idx_val]).to_numpy(), \n",
    "         np.sum(df[['binary_A', 'binary_B', 'binary_C']].loc[idx_test]).to_numpy(),\n",
    "        )\n",
    "    )\n",
    "\n",
    "print(sizes)\n",
    "print(pos_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_indices_and_stats(\n",
    "    indices, sizes, pos_class, split_dimension=0, save_indices=True, train_size=10, total_size=len(df), data_name=data_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = ShuffleSplit(n_splits=9, test_size=0.45, random_state=42)\n",
    "inner_splitter = ShuffleSplit(n_splits=1, test_size=0.5/0.55, random_state=np.random.RandomState(42))  # we use a RandomState instance, not an int, because we will reuse this splitter several times\n",
    "\n",
    "indices = []\n",
    "sizes = []\n",
    "pos_class = []\n",
    "for idx_train_val, idx_test in splitter.split(df):\n",
    "    # inner split\n",
    "    train, val = next(inner_splitter.split(idx_train_val))\n",
    "    # use indices to index indices :P (we need to obtain indices referring to the original dataframe)\n",
    "    idx_train = idx_train_val[train]\n",
    "    idx_val = idx_train_val[val]\n",
    "    # add to list\n",
    "    indices.append((idx_train, idx_val, idx_test))\n",
    "    sizes.append((len(idx_train), len(idx_val), len(idx_test)))\n",
    "    pos_class.append(\n",
    "        (np.sum(df[['binary_A', 'binary_B', 'binary_C']].loc[idx_train]).to_numpy(), \n",
    "         np.sum(df[['binary_A', 'binary_B', 'binary_C']].loc[idx_val]).to_numpy(), \n",
    "         np.sum(df[['binary_A', 'binary_B', 'binary_C']].loc[idx_test]).to_numpy(),\n",
    "        )\n",
    "    )\n",
    "\n",
    "print(sizes)\n",
    "print(pos_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_indices_and_stats(\n",
    "    indices, sizes, pos_class, split_dimension=0, save_indices=True, train_size=5, total_size=len(df), data_name=data_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = ShuffleSplit(n_splits=9, test_size=0.475, random_state=42)\n",
    "inner_splitter = ShuffleSplit(n_splits=1, test_size=0.5/0.525, random_state=np.random.RandomState(42))  # we use a RandomState instance, not an int, because we will reuse this splitter several times\n",
    "\n",
    "indices = []\n",
    "sizes = []\n",
    "pos_class = []\n",
    "for idx_train_val, idx_test in splitter.split(df):\n",
    "    # inner split\n",
    "    train, val = next(inner_splitter.split(idx_train_val))\n",
    "    # use indices to index indices :P (we need to obtain indices referring to the original dataframe)\n",
    "    idx_train = idx_train_val[train]\n",
    "    idx_val = idx_train_val[val]\n",
    "    # add to list\n",
    "    indices.append((idx_train, idx_val, idx_test))\n",
    "    sizes.append((len(idx_train), len(idx_val), len(idx_test)))\n",
    "    pos_class.append(\n",
    "        (np.sum(df[['binary_A', 'binary_B', 'binary_C']].loc[idx_train]).to_numpy(), \n",
    "         np.sum(df[['binary_A', 'binary_B', 'binary_C']].loc[idx_val]).to_numpy(), \n",
    "         np.sum(df[['binary_A', 'binary_B', 'binary_C']].loc[idx_test]).to_numpy(),\n",
    "        )\n",
    "    )\n",
    "\n",
    "print(sizes)\n",
    "print(pos_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_indices_and_stats(\n",
    "    indices, sizes, pos_class, split_dimension=0, save_indices=True, train_size=2.5, total_size=len(df), data_name=data_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = ShuffleSplit(n_splits=9, test_size=0.4875, random_state=42)\n",
    "inner_splitter = ShuffleSplit(n_splits=1, test_size=0.5/0.5125, random_state=np.random.RandomState(42))  # we use a RandomState instance, not an int, because we will reuse this splitter several times\n",
    "\n",
    "indices = []\n",
    "sizes = []\n",
    "pos_class = []\n",
    "for idx_train_val, idx_test in splitter.split(df):\n",
    "    # inner split\n",
    "    train, val = next(inner_splitter.split(idx_train_val))\n",
    "    # use indices to index indices :P (we need to obtain indices referring to the original dataframe)\n",
    "    idx_train = idx_train_val[train]\n",
    "    idx_val = idx_train_val[val]\n",
    "    # add to list\n",
    "    indices.append((idx_train, idx_val, idx_test))\n",
    "    sizes.append((len(idx_train), len(idx_val), len(idx_test)))\n",
    "    pos_class.append(\n",
    "        (np.sum(df[['binary_A', 'binary_B', 'binary_C']].loc[idx_train]).to_numpy(), \n",
    "         np.sum(df[['binary_A', 'binary_B', 'binary_C']].loc[idx_val]).to_numpy(), \n",
    "         np.sum(df[['binary_A', 'binary_B', 'binary_C']].loc[idx_test]).to_numpy(),\n",
    "        )\n",
    "    )\n",
    "\n",
    "print(sizes)\n",
    "print(pos_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_indices_and_stats(\n",
    "    indices, sizes, pos_class, split_dimension=0, save_indices=True, train_size=1.25, total_size=len(df), data_name=data_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = ShuffleSplit(n_splits=9, test_size=0.49375, random_state=42)\n",
    "inner_splitter = ShuffleSplit(n_splits=1, test_size=0.5/0.50625, random_state=np.random.RandomState(42))  # we use a RandomState instance, not an int, because we will reuse this splitter several times\n",
    "\n",
    "indices = []\n",
    "sizes = []\n",
    "pos_class = []\n",
    "for idx_train_val, idx_test in splitter.split(df):\n",
    "    # inner split\n",
    "    train, val = next(inner_splitter.split(idx_train_val))\n",
    "    # use indices to index indices :P (we need to obtain indices referring to the original dataframe)\n",
    "    idx_train = idx_train_val[train]\n",
    "    idx_val = idx_train_val[val]\n",
    "    # add to list\n",
    "    indices.append((idx_train, idx_val, idx_test))\n",
    "    sizes.append((len(idx_train), len(idx_val), len(idx_test)))\n",
    "    pos_class.append(\n",
    "        (np.sum(df[['binary_A', 'binary_B', 'binary_C']].loc[idx_train]).to_numpy(), \n",
    "         np.sum(df[['binary_A', 'binary_B', 'binary_C']].loc[idx_val]).to_numpy(), \n",
    "         np.sum(df[['binary_A', 'binary_B', 'binary_C']].loc[idx_test]).to_numpy(),\n",
    "        )\n",
    "    )\n",
    "\n",
    "print(sizes)\n",
    "print(pos_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_indices_and_stats(\n",
    "    indices, sizes, pos_class, split_dimension=0, save_indices=True, train_size=0.625, total_size=len(df), data_name=data_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1D split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_1d(splitter, inner_splitter):\n",
    "    indices = []\n",
    "    sizes = []\n",
    "    pos_class = []\n",
    "    unique_initiators = []\n",
    "    unique_monomers = []\n",
    "    unique_terminators = []\n",
    "    for idx_train_val, idx_test in splitter.split(list(range(len(df))), groups=df[\"I_long\"]):\n",
    "        #Â inner split\n",
    "        train, val = next(inner_splitter.split(idx_train_val, groups=df[\"I_long\"][idx_train_val]))\n",
    "        # use indices to index indices :P (we need to obtain indices referring to the original dataframe)\n",
    "        idx_train = idx_train_val[train]\n",
    "        idx_val = idx_train_val[val]\n",
    "        # add to list\n",
    "        indices.append((idx_train, idx_val, idx_test))\n",
    "        sizes.append((len(idx_train), len(idx_val), len(idx_test)))\n",
    "        pos_class.append(\n",
    "            (np.sum(df[['binary_A', 'binary_B', 'binary_C']].loc[idx_train]).to_numpy(), \n",
    "             np.sum(df[['binary_A', 'binary_B', 'binary_C']].loc[idx_val]).to_numpy(), \n",
    "             np.sum(df[['binary_A', 'binary_B', 'binary_C']].loc[idx_test]).to_numpy(),\n",
    "            )\n",
    "        )\n",
    "        unique_initiators.append((len(df['I_long'][idx_train].drop_duplicates()), len(df['I_long'][idx_val].drop_duplicates()), len(df['I_long'][idx_test].drop_duplicates())))\n",
    "        unique_monomers.append((len(df['M_long'][idx_train].drop_duplicates()), len(df['M_long'][idx_val].drop_duplicates()), len(df['M_long'][idx_test].drop_duplicates())))\n",
    "        unique_terminators.append((len(df['T_long'][idx_train].drop_duplicates()), len(df['T_long'][idx_val].drop_duplicates()), len(df['T_long'][idx_test].drop_duplicates())))\n",
    "    \n",
    "    return indices, sizes, pos_class, unique_initiators, unique_monomers, unique_terminators\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = GroupShuffleSplit(n_splits=9, test_size=0.1, random_state=42)\n",
    "inner_splitter = GroupShuffleSplit(n_splits=1, test_size=0.1/0.9, random_state=np.random.RandomState(42))  # we use a RandomState instance, not an int, because we will reuse this splitter several times\n",
    "\n",
    "indices, sizes, pos_class, unique_initiators, unique_monomers, unique_terminators = split_1d(splitter, inner_splitter)\n",
    "\n",
    "print(sizes)\n",
    "print(pos_class)\n",
    "print(unique_initiators)\n",
    "print(unique_monomers)\n",
    "print(unique_terminators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_indices_and_stats(\n",
    "    indices, \n",
    "    sizes, \n",
    "    pos_class,\n",
    "    total_size=len(df),\n",
    "    data_name=data_name,\n",
    "    split_dimension=1, \n",
    "    save_indices=True, \n",
    "    n_initiators=unique_initiators, \n",
    "    n_monomers=unique_monomers, \n",
    "    n_terminators=unique_terminators, \n",
    "    train_size=80\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = GroupShuffleSplit(n_splits=9, test_size=0.3, random_state=42)\n",
    "inner_splitter = GroupShuffleSplit(n_splits=1, test_size=0.3/0.7, random_state=np.random.RandomState(42))  # we use a RandomState instance, not an int, because we will reuse this splitter several times\n",
    "\n",
    "indices, sizes, pos_class, unique_initiators, unique_monomers, unique_terminators = split_1d(splitter, inner_splitter)\n",
    "\n",
    "print(sizes)\n",
    "print(pos_class)\n",
    "print(unique_initiators)\n",
    "print(unique_monomers)\n",
    "print(unique_terminators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_indices_and_stats(\n",
    "    indices, \n",
    "    sizes, \n",
    "    pos_class,\n",
    "    total_size=len(df),\n",
    "    data_name=data_name,\n",
    "    split_dimension=1, \n",
    "    save_indices=True, \n",
    "    n_initiators=unique_initiators, \n",
    "    n_monomers=unique_monomers, \n",
    "    n_terminators=unique_terminators, \n",
    "    train_size=40\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = GroupShuffleSplit(n_splits=9, test_size=0.4, random_state=42)\n",
    "inner_splitter = GroupShuffleSplit(n_splits=1, test_size=0.4/0.6, random_state=np.random.RandomState(42))  # we use a RandomState instance, not an int, because we will reuse this splitter several times\n",
    "\n",
    "indices, sizes, pos_class, unique_initiators, unique_monomers, unique_terminators = split_1d(splitter, inner_splitter)\n",
    "\n",
    "print(sizes)\n",
    "print(pos_class)\n",
    "print(unique_initiators)\n",
    "print(unique_monomers)\n",
    "print(unique_terminators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_indices_and_stats(\n",
    "    indices, \n",
    "    sizes, \n",
    "    pos_class,\n",
    "    total_size=len(df),\n",
    "    data_name=data_name,\n",
    "    split_dimension=1, \n",
    "    save_indices=True, \n",
    "    n_initiators=unique_initiators, \n",
    "    n_monomers=unique_monomers, \n",
    "    n_terminators=unique_terminators, \n",
    "    train_size=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = GroupShuffleSplit(n_splits=9, test_size=0.45, random_state=42)\n",
    "inner_splitter = GroupShuffleSplit(n_splits=1, test_size=0.45/0.55, random_state=np.random.RandomState(42))  # we use a RandomState instance, not an int, because we will reuse this splitter several times\n",
    "\n",
    "indices, sizes, pos_class, unique_initiators, unique_monomers, unique_terminators = split_1d(splitter, inner_splitter)\n",
    "\n",
    "print(sizes)\n",
    "print(pos_class)\n",
    "print(unique_initiators)\n",
    "print(unique_monomers)\n",
    "print(unique_terminators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_indices_and_stats(\n",
    "    indices, \n",
    "    sizes, \n",
    "    pos_class,\n",
    "    total_size=len(df),\n",
    "    data_name=data_name,\n",
    "    split_dimension=1, \n",
    "    save_indices=True, \n",
    "    n_initiators=unique_initiators, \n",
    "    n_monomers=unique_monomers, \n",
    "    n_terminators=unique_terminators, \n",
    "    train_size=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = GroupShuffleSplit(n_splits=9, test_size=0.45, random_state=42)\n",
    "inner_splitter = GroupShuffleSplit(n_splits=1, test_size=0.5/0.55, random_state=np.random.RandomState(42))  # we use a RandomState instance, not an int, because we will reuse this splitter several times\n",
    "\n",
    "indices, sizes, pos_class, unique_initiators, unique_monomers, unique_terminators = split_1d(splitter, inner_splitter)\n",
    "\n",
    "print(sizes)\n",
    "print(pos_class)\n",
    "print(unique_initiators)\n",
    "print(unique_monomers)\n",
    "print(unique_terminators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_indices_and_stats(\n",
    "    indices, \n",
    "    sizes, \n",
    "    pos_class,\n",
    "    total_size=len(df),\n",
    "    data_name=data_name,\n",
    "    split_dimension=1, \n",
    "    save_indices=True, \n",
    "    n_initiators=unique_initiators, \n",
    "    n_monomers=unique_monomers, \n",
    "    n_terminators=unique_terminators, \n",
    "    train_size=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = GroupShuffleSplit(n_splits=9, test_size=0.475, random_state=42)\n",
    "inner_splitter = GroupShuffleSplit(n_splits=1, test_size=0.5/0.525, random_state=np.random.RandomState(42))  # we use a RandomState instance, not an int, because we will reuse this splitter several times\n",
    "\n",
    "indices, sizes, pos_class, unique_initiators, unique_monomers, unique_terminators = split_1d(splitter, inner_splitter)\n",
    "\n",
    "print(sizes)\n",
    "print(pos_class)\n",
    "print(unique_initiators)\n",
    "print(unique_monomers)\n",
    "print(unique_terminators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_indices_and_stats(\n",
    "    indices, \n",
    "    sizes, \n",
    "    pos_class,\n",
    "    total_size=len(df),\n",
    "    data_name=data_name,\n",
    "    split_dimension=1, \n",
    "    save_indices=True, \n",
    "    n_initiators=unique_initiators, \n",
    "    n_monomers=unique_monomers, \n",
    "    n_terminators=unique_terminators, \n",
    "    train_size=2.5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2D split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_2d(splitter, inner_splitter):\n",
    "    indices = []\n",
    "    sizes = []\n",
    "    pos_class = []\n",
    "    unique_initiators = []\n",
    "    unique_monomers = []\n",
    "    unique_terminators = []\n",
    "    for idx_train_val, idx_test in splitter.split(df, groups=df[[\"I_long\", \"M_long\"]]):\n",
    "        train, val = next(inner_splitter.split(df.iloc[idx_train_val], groups=df[[\"I_long\", \"M_long\"]].iloc[idx_train_val]))\n",
    "        # use indices to index indices :P (we need to obtain indices referring to the original dataframe)\n",
    "        idx_train = idx_train_val[train]\n",
    "        idx_val = idx_train_val[val]\n",
    "        indices.append((idx_train, idx_val, idx_test))\n",
    "        sizes.append((len(idx_train), len(idx_val), len(idx_test)))\n",
    "        pos_class.append(\n",
    "            (np.sum(df[['binary_A', 'binary_B', 'binary_C']].loc[idx_train]).to_numpy(), \n",
    "             np.sum(df[['binary_A', 'binary_B', 'binary_C']].loc[idx_val]).to_numpy(), \n",
    "             np.sum(df[['binary_A', 'binary_B', 'binary_C']].loc[idx_test]).to_numpy(),\n",
    "            )\n",
    "        )\n",
    "        unique_initiators.append((len(df['I_long'][idx_train].drop_duplicates()), len(df['I_long'][idx_val].drop_duplicates()), len(df['I_long'][idx_test].drop_duplicates())))\n",
    "        unique_monomers.append((len(df['M_long'][idx_train].drop_duplicates()), len(df['M_long'][idx_val].drop_duplicates()), len(df['M_long'][idx_test].drop_duplicates())))\n",
    "        unique_terminators.append((len(df['T_long'][idx_train].drop_duplicates()), len(df['T_long'][idx_val].drop_duplicates()), len(df['T_long'][idx_test].drop_duplicates())))\n",
    "    \n",
    "    return indices, sizes, pos_class, unique_initiators, unique_monomers, unique_terminators\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = GroupShuffleSplitND(n_splits=9, test_size=0.1, random_state=42)\n",
    "inner_splitter = GroupShuffleSplitND(n_splits=1, test_size=0.1/0.9, random_state=np.random.RandomState(42))  # we use a RandomState instance, not an int, because we will reuse this splitter several times\n",
    "\n",
    "indices, sizes, pos_class, unique_initiators, unique_monomers, unique_terminators = split_2d(splitter, inner_splitter)\n",
    "\n",
    "print(sizes)\n",
    "print(pos_class)\n",
    "print(unique_initiators)\n",
    "print(unique_monomers)\n",
    "print(unique_terminators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_indices_and_stats(\n",
    "    indices, \n",
    "    sizes, \n",
    "    pos_class,\n",
    "    total_size=len(df),\n",
    "    data_name=data_name,\n",
    "    split_dimension=2, \n",
    "    save_indices=True, \n",
    "    n_initiators=unique_initiators, \n",
    "    n_monomers=unique_monomers, \n",
    "    n_terminators=unique_terminators, \n",
    "    train_size=80\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = GroupShuffleSplitND(n_splits=9, test_size=0.2, random_state=42)\n",
    "inner_splitter = GroupShuffleSplitND(n_splits=1, test_size=0.2/0.8, random_state=np.random.RandomState(42))  # we use a RandomState instance, not an int, because we will reuse this splitter several times\n",
    "\n",
    "indices, sizes, pos_class, unique_initiators, unique_monomers, unique_terminators = split_2d(splitter, inner_splitter)\n",
    "\n",
    "print(sizes)\n",
    "print(pos_class)\n",
    "print(unique_initiators)\n",
    "print(unique_monomers)\n",
    "print(unique_terminators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_indices_and_stats(\n",
    "    indices, \n",
    "    sizes, \n",
    "    pos_class,\n",
    "    total_size=len(df),\n",
    "    data_name=data_name,\n",
    "    split_dimension=2, \n",
    "    save_indices=True, \n",
    "    n_initiators=unique_initiators, \n",
    "    n_monomers=unique_monomers, \n",
    "    n_terminators=unique_terminators, \n",
    "    train_size=60\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = GroupShuffleSplitND(n_splits=9, test_size=0.3, random_state=42)\n",
    "inner_splitter = GroupShuffleSplitND(n_splits=1, test_size=0.3/0.7, random_state=np.random.RandomState(42))  # we use a RandomState instance, not an int, because we will reuse this splitter several times\n",
    "\n",
    "indices, sizes, pos_class, unique_initiators, unique_monomers, unique_terminators = split_2d(splitter, inner_splitter)\n",
    "\n",
    "print(sizes)\n",
    "print(pos_class)\n",
    "print(unique_initiators)\n",
    "print(unique_monomers)\n",
    "print(unique_terminators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_indices_and_stats(\n",
    "    indices, \n",
    "    sizes, \n",
    "    pos_class,\n",
    "    total_size=len(df),\n",
    "    data_name=data_name,\n",
    "    split_dimension=2, \n",
    "    save_indices=True, \n",
    "    n_initiators=unique_initiators, \n",
    "    n_monomers=unique_monomers, \n",
    "    n_terminators=unique_terminators, \n",
    "    train_size=40\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = GroupShuffleSplitND(n_splits=9, test_size=0.35, random_state=42)\n",
    "inner_splitter = GroupShuffleSplitND(n_splits=1, test_size=0.35/0.65, random_state=np.random.RandomState(42))  # we use a RandomState instance, not an int, because we will reuse this splitter several times\n",
    "\n",
    "indices, sizes, pos_class, unique_initiators, unique_monomers, unique_terminators = split_2d(splitter, inner_splitter)\n",
    "\n",
    "print(sizes)\n",
    "print(pos_class)\n",
    "print(unique_initiators)\n",
    "print(unique_monomers)\n",
    "print(unique_terminators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_indices_and_stats(\n",
    "    indices, \n",
    "    sizes, \n",
    "    pos_class,\n",
    "    total_size=len(df),\n",
    "    data_name=data_name,\n",
    "    split_dimension=2, \n",
    "    save_indices=True, \n",
    "    n_initiators=unique_initiators, \n",
    "    n_monomers=unique_monomers, \n",
    "    n_terminators=unique_terminators, \n",
    "    train_size=30\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = GroupShuffleSplitND(n_splits=9, test_size=0.4, random_state=42)\n",
    "inner_splitter = GroupShuffleSplitND(n_splits=1, test_size=0.4/0.6, random_state=np.random.RandomState(42))  # we use a RandomState instance, not an int, because we will reuse this splitter several times\n",
    "\n",
    "indices, sizes, pos_class, unique_initiators, unique_monomers, unique_terminators = split_2d(splitter, inner_splitter)\n",
    "\n",
    "print(sizes)\n",
    "print(pos_class)\n",
    "print(unique_initiators)\n",
    "print(unique_monomers)\n",
    "print(unique_terminators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_indices_and_stats(\n",
    "    indices, \n",
    "    sizes, \n",
    "    pos_class,\n",
    "    total_size=len(df),\n",
    "    data_name=data_name,\n",
    "    split_dimension=2, \n",
    "    save_indices=True, \n",
    "    n_initiators=unique_initiators, \n",
    "    n_monomers=unique_monomers, \n",
    "    n_terminators=unique_terminators, \n",
    "    train_size=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = GroupShuffleSplitND(n_splits=9, test_size=0.4, random_state=42)\n",
    "inner_splitter = GroupShuffleSplitND(n_splits=1, test_size=0.45/0.6, random_state=np.random.RandomState(42))  # we use a RandomState instance, not an int, because we will reuse this splitter several times\n",
    "\n",
    "indices, sizes, pos_class, unique_initiators, unique_monomers, unique_terminators = split_2d(splitter, inner_splitter)\n",
    "\n",
    "print(sizes)\n",
    "print(pos_class)\n",
    "print(unique_initiators)\n",
    "print(unique_monomers)\n",
    "print(unique_terminators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_indices_and_stats(\n",
    "    indices, \n",
    "    sizes, \n",
    "    pos_class,\n",
    "    total_size=len(df),\n",
    "    data_name=data_name,\n",
    "    split_dimension=2, \n",
    "    save_indices=True, \n",
    "    n_initiators=unique_initiators, \n",
    "    n_monomers=unique_monomers, \n",
    "    n_terminators=unique_terminators, \n",
    "    train_size=15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = GroupShuffleSplitND(n_splits=9, test_size=0.45, random_state=42)\n",
    "inner_splitter = GroupShuffleSplitND(n_splits=1, test_size=0.45/0.55, random_state=np.random.RandomState(42))  # we use a RandomState instance, not an int, because we will reuse this splitter several times\n",
    "\n",
    "indices, sizes, pos_class, unique_initiators, unique_monomers, unique_terminators = split_2d(splitter, inner_splitter)\n",
    "\n",
    "print(sizes)\n",
    "print(pos_class)\n",
    "print(unique_initiators)\n",
    "print(unique_monomers)\n",
    "print(unique_terminators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_indices_and_stats(\n",
    "    indices, \n",
    "    sizes, \n",
    "    pos_class,\n",
    "    total_size=len(df),\n",
    "    data_name=data_name,\n",
    "    split_dimension=2, \n",
    "    save_indices=True, \n",
    "    n_initiators=unique_initiators, \n",
    "    n_monomers=unique_monomers, \n",
    "    n_terminators=unique_terminators, \n",
    "    train_size=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = GroupShuffleSplitND(n_splits=9, test_size=0.45, random_state=42)\n",
    "inner_splitter = GroupShuffleSplitND(n_splits=1, test_size=0.475/0.55, random_state=np.random.RandomState(42))  # we use a RandomState instance, not an int, because we will reuse this splitter several times\n",
    "\n",
    "indices, sizes, pos_class, unique_initiators, unique_monomers, unique_terminators = split_2d(splitter, inner_splitter)\n",
    "\n",
    "print(sizes)\n",
    "print(pos_class)\n",
    "print(unique_initiators)\n",
    "print(unique_monomers)\n",
    "print(unique_terminators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_indices_and_stats(\n",
    "    indices, \n",
    "    sizes, \n",
    "    pos_class,\n",
    "    total_size=len(df),\n",
    "    data_name=data_name,\n",
    "    split_dimension=2, \n",
    "    save_indices=True, \n",
    "    n_initiators=unique_initiators, \n",
    "    n_monomers=unique_monomers, \n",
    "    n_terminators=unique_terminators, \n",
    "    train_size=7.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = GroupShuffleSplitND(n_splits=9, test_size=0.45, random_state=42)\n",
    "inner_splitter = GroupShuffleSplitND(n_splits=1, test_size=0.5/0.55, random_state=np.random.RandomState(42))  # we use a RandomState instance, not an int, because we will reuse this splitter several times\n",
    "\n",
    "indices, sizes, pos_class, unique_initiators, unique_monomers, unique_terminators = split_2d(splitter, inner_splitter)\n",
    "\n",
    "print(sizes)\n",
    "print(pos_class)\n",
    "print(unique_initiators)\n",
    "print(unique_monomers)\n",
    "print(unique_terminators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_indices_and_stats(\n",
    "    indices, \n",
    "    sizes, \n",
    "    pos_class,\n",
    "    total_size=len(df),\n",
    "    data_name=data_name,\n",
    "    split_dimension=2, \n",
    "    save_indices=True, \n",
    "    n_initiators=unique_initiators, \n",
    "    n_monomers=unique_monomers, \n",
    "    n_terminators=unique_terminators, \n",
    "    train_size=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = GroupShuffleSplitND(n_splits=9, test_size=0.48, random_state=42)\n",
    "inner_splitter = GroupShuffleSplitND(n_splits=1, train_size=2, random_state=np.random.RandomState(42))  # we use a RandomState instance, not an int, because we will reuse this splitter several times\n",
    "\n",
    "indices, sizes, pos_class, unique_initiators, unique_monomers, unique_terminators = split_2d(splitter, inner_splitter)\n",
    "\n",
    "print(sizes)\n",
    "print(pos_class)\n",
    "print(unique_initiators)\n",
    "print(unique_monomers)\n",
    "print(unique_terminators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_indices_and_stats(\n",
    "    indices, \n",
    "    sizes, \n",
    "    pos_class,\n",
    "    total_size=len(df),\n",
    "    data_name=data_name,\n",
    "    split_dimension=2, \n",
    "    save_indices=True, \n",
    "    n_initiators=unique_initiators, \n",
    "    n_monomers=unique_monomers, \n",
    "    n_terminators=unique_terminators, \n",
    "    train_size=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3D split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_3d(splitter, inner_splitter):\n",
    "    indices = []\n",
    "    sizes = []\n",
    "    pos_class = []\n",
    "    unique_initiators = []\n",
    "    unique_monomers = []\n",
    "    unique_terminators = []\n",
    "    for idx_train_val, idx_test in splitter.split(df, groups=df[[\"I_long\", \"M_long\", \"T_long\"]]):\n",
    "        train, val = next(inner_splitter.split(df.iloc[idx_train_val], groups=df[[\"I_long\", \"M_long\", \"T_long\"]].iloc[idx_train_val]))\n",
    "        # use indices to index indices :P (we need to obtain indices referring to the original dataframe)\n",
    "        idx_train = idx_train_val[train]\n",
    "        idx_val = idx_train_val[val]\n",
    "        indices.append((idx_train, idx_val, idx_test))\n",
    "        sizes.append((len(idx_train), len(idx_val), len(idx_test)))\n",
    "        pos_class.append(\n",
    "            (np.sum(df[['binary_A', 'binary_B', 'binary_C']].loc[idx_train]).to_numpy(), \n",
    "             np.sum(df[['binary_A', 'binary_B', 'binary_C']].loc[idx_val]).to_numpy(), \n",
    "             np.sum(df[['binary_A', 'binary_B', 'binary_C']].loc[idx_test]).to_numpy(),\n",
    "            )\n",
    "        )\n",
    "        unique_initiators.append((len(df['I_long'][idx_train].drop_duplicates()), len(df['I_long'][idx_val].drop_duplicates()), len(df['I_long'][idx_test].drop_duplicates())))\n",
    "        unique_monomers.append((len(df['M_long'][idx_train].drop_duplicates()), len(df['M_long'][idx_val].drop_duplicates()), len(df['M_long'][idx_test].drop_duplicates())))\n",
    "        unique_terminators.append((len(df['T_long'][idx_train].drop_duplicates()), len(df['T_long'][idx_val].drop_duplicates()), len(df['T_long'][idx_test].drop_duplicates())))\n",
    "\n",
    "    return indices, sizes, pos_class, unique_initiators, unique_monomers, unique_terminators\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = GroupShuffleSplitND(n_splits=9, test_size=0.1, random_state=np.random.RandomState(42))  # here, we reuse the outer splitter as well, so we use RandomState\n",
    "inner_splitter = GroupShuffleSplitND(n_splits=1, test_size=0.1/0.9, random_state=np.random.RandomState(42))  # we use a RandomState instance, not an int, because we will reuse this splitter several times\n",
    "\n",
    "indices, sizes, pos_class, unique_initiators, unique_monomers, unique_terminators = split_3d(splitter, inner_splitter)\n",
    "\n",
    "print(sizes)\n",
    "print(pos_class)\n",
    "print(unique_initiators)\n",
    "print(unique_monomers)\n",
    "print(unique_terminators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_indices_and_stats(\n",
    "    indices, \n",
    "    sizes, \n",
    "    pos_class,\n",
    "    total_size=len(df),\n",
    "    data_name=data_name,\n",
    "    split_dimension=3, \n",
    "    save_indices=True, \n",
    "    n_initiators=unique_initiators, \n",
    "    n_monomers=unique_monomers, \n",
    "    n_terminators=unique_terminators, \n",
    "    train_size=80\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = GroupShuffleSplitND(n_splits=9, test_size=0.15, random_state=np.random.RandomState(42))  # here, we reuse the outer splitter as well, so we use RandomState\n",
    "inner_splitter = GroupShuffleSplitND(n_splits=1, test_size=0.15/0.85, random_state=np.random.RandomState(42))  # we use a RandomState instance, not an int, because we will reuse this splitter several times\n",
    "\n",
    "indices, sizes, pos_class, unique_initiators, unique_monomers, unique_terminators = split_3d(splitter, inner_splitter)\n",
    "\n",
    "print(sizes)\n",
    "print(pos_class)\n",
    "print(unique_initiators)\n",
    "print(unique_monomers)\n",
    "print(unique_terminators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_indices_and_stats(\n",
    "    indices, \n",
    "    sizes, \n",
    "    pos_class,\n",
    "    total_size=len(df),\n",
    "    data_name=data_name,\n",
    "    split_dimension=3, \n",
    "    save_indices=True, \n",
    "    n_initiators=unique_initiators, \n",
    "    n_monomers=unique_monomers, \n",
    "    n_terminators=unique_terminators, \n",
    "    train_size=70\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = GroupShuffleSplitND(n_splits=9, test_size=0.2, random_state=np.random.RandomState(42))  # here, we reuse the outer splitter as well, so we use RandomState\n",
    "inner_splitter = GroupShuffleSplitND(n_splits=1, test_size=0.2/0.8, random_state=np.random.RandomState(42))  # we use a RandomState instance, not an int, because we will reuse this splitter several times\n",
    "\n",
    "indices, sizes, pos_class, unique_initiators, unique_monomers, unique_terminators = split_3d(splitter, inner_splitter)\n",
    "\n",
    "print(sizes)\n",
    "print(pos_class)\n",
    "print(unique_initiators)\n",
    "print(unique_monomers)\n",
    "print(unique_terminators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_indices_and_stats(\n",
    "    indices, \n",
    "    sizes, \n",
    "    pos_class,\n",
    "    total_size=len(df),\n",
    "    data_name=data_name,\n",
    "    split_dimension=3, \n",
    "    save_indices=True, \n",
    "    n_initiators=unique_initiators, \n",
    "    n_monomers=unique_monomers, \n",
    "    n_terminators=unique_terminators, \n",
    "    train_size=60\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = GroupShuffleSplitND(n_splits=9, test_size=0.25, random_state=np.random.RandomState(42))  # here, we reuse the outer splitter as well, so we use RandomState\n",
    "inner_splitter = GroupShuffleSplitND(n_splits=1, test_size=0.25/0.75, random_state=np.random.RandomState(42))  # we use a RandomState instance, not an int, because we will reuse this splitter several times\n",
    "\n",
    "indices, sizes, pos_class, unique_initiators, unique_monomers, unique_terminators = split_3d(splitter, inner_splitter)\n",
    "\n",
    "print(sizes)\n",
    "print(pos_class)\n",
    "print(unique_initiators)\n",
    "print(unique_monomers)\n",
    "print(unique_terminators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_indices_and_stats(\n",
    "    indices, \n",
    "    sizes, \n",
    "    pos_class,\n",
    "    total_size=len(df),\n",
    "    data_name=data_name,\n",
    "    split_dimension=3, \n",
    "    save_indices=True, \n",
    "    n_initiators=unique_initiators, \n",
    "    n_monomers=unique_monomers, \n",
    "    n_terminators=unique_terminators, \n",
    "    train_size=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = GroupShuffleSplitND(n_splits=9, test_size=0.3, random_state=np.random.RandomState(42))  # here, we reuse the outer splitter as well, so we use RandomState\n",
    "inner_splitter = GroupShuffleSplitND(n_splits=1, test_size=0.3/0.7, random_state=np.random.RandomState(42))  # we use a RandomState instance, not an int, because we will reuse this splitter several times\n",
    "\n",
    "indices, sizes, pos_class, unique_initiators, unique_monomers, unique_terminators = split_3d(splitter, inner_splitter)\n",
    "\n",
    "print(sizes)\n",
    "print(pos_class)\n",
    "print(unique_initiators)\n",
    "print(unique_monomers)\n",
    "print(unique_terminators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_indices_and_stats(\n",
    "    indices, \n",
    "    sizes, \n",
    "    pos_class,\n",
    "    total_size=len(df),\n",
    "    data_name=data_name,\n",
    "    split_dimension=3, \n",
    "    save_indices=True, \n",
    "    n_initiators=unique_initiators, \n",
    "    n_monomers=unique_monomers, \n",
    "    n_terminators=unique_terminators, \n",
    "    train_size=40\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = GroupShuffleSplitND(n_splits=9, test_size=0.33, random_state=np.random.RandomState(42))  # here, we reuse the outer splitter as well, so we use RandomState\n",
    "inner_splitter = GroupShuffleSplitND(n_splits=1, test_size=0.33/0.67, random_state=np.random.RandomState(42))  # we use a RandomState instance, not an int, because we will reuse this splitter several times\n",
    "\n",
    "indices, sizes, pos_class, unique_initiators, unique_monomers, unique_terminators = split_3d(splitter, inner_splitter)\n",
    "\n",
    "print(sizes)\n",
    "print(pos_class)\n",
    "print(unique_initiators)\n",
    "print(unique_monomers)\n",
    "print(unique_terminators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_indices_and_stats(\n",
    "    indices, \n",
    "    sizes, \n",
    "    pos_class,\n",
    "    total_size=len(df),\n",
    "    data_name=data_name,\n",
    "    split_dimension=3, \n",
    "    save_indices=True, \n",
    "    n_initiators=unique_initiators, \n",
    "    n_monomers=unique_monomers, \n",
    "    n_terminators=unique_terminators, \n",
    "    train_size=34\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = GroupShuffleSplitND(n_splits=9, test_size=0.35, random_state=np.random.RandomState(42))  # here, we reuse the outer splitter as well, so we use RandomState\n",
    "inner_splitter = GroupShuffleSplitND(n_splits=1, test_size=0.35/0.65, random_state=np.random.RandomState(42))  # we use a RandomState instance, not an int, because we will reuse this splitter several times\n",
    "\n",
    "indices, sizes, pos_class, unique_initiators, unique_monomers, unique_terminators = split_3d(splitter, inner_splitter)\n",
    "\n",
    "print(sizes)\n",
    "print(pos_class)\n",
    "print(unique_initiators)\n",
    "print(unique_monomers)\n",
    "print(unique_terminators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_indices_and_stats(\n",
    "    indices, \n",
    "    sizes, \n",
    "    pos_class,\n",
    "    total_size=len(df),\n",
    "    data_name=data_name,\n",
    "    split_dimension=3, \n",
    "    save_indices=True, \n",
    "    n_initiators=unique_initiators, \n",
    "    n_monomers=unique_monomers, \n",
    "    n_terminators=unique_terminators, \n",
    "    train_size=30\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = GroupShuffleSplitND(n_splits=9, test_size=0.35, random_state=np.random.RandomState(42))  # here, we reuse the outer splitter as well, so we use RandomState\n",
    "inner_splitter = GroupShuffleSplitND(n_splits=1, test_size=0.40/0.65, random_state=np.random.RandomState(42))  # we use a RandomState instance, not an int, because we will reuse this splitter several times\n",
    "\n",
    "indices, sizes, pos_class, unique_initiators, unique_monomers, unique_terminators = split_3d(splitter, inner_splitter)\n",
    "\n",
    "print(sizes)\n",
    "print(pos_class)\n",
    "print(unique_initiators)\n",
    "print(unique_monomers)\n",
    "print(unique_terminators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_indices_and_stats(\n",
    "    indices, \n",
    "    sizes, \n",
    "    pos_class,\n",
    "    total_size=len(df),\n",
    "    data_name=data_name,\n",
    "    split_dimension=3, \n",
    "    save_indices=True, \n",
    "    n_initiators=unique_initiators, \n",
    "    n_monomers=unique_monomers, \n",
    "    n_terminators=unique_terminators, \n",
    "    train_size=25\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = GroupShuffleSplitND(n_splits=9, test_size=0.40, random_state=np.random.RandomState(42))  # here, we reuse the outer splitter as well, so we use RandomState\n",
    "inner_splitter = GroupShuffleSplitND(n_splits=1, test_size=0.40/0.60, random_state=np.random.RandomState(42))  # we use a RandomState instance, not an int, because we will reuse this splitter several times\n",
    "\n",
    "indices, sizes, pos_class, unique_initiators, unique_monomers, unique_terminators = split_3d(splitter, inner_splitter)\n",
    "\n",
    "print(sizes)\n",
    "print(pos_class)\n",
    "print(unique_initiators)\n",
    "print(unique_monomers)\n",
    "print(unique_terminators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_indices_and_stats(\n",
    "    indices, \n",
    "    sizes, \n",
    "    pos_class,\n",
    "    total_size=len(df),\n",
    "    data_name=data_name,\n",
    "    split_dimension=3, \n",
    "    save_indices=True, \n",
    "    n_initiators=unique_initiators, \n",
    "    n_monomers=unique_monomers, \n",
    "    n_terminators=unique_terminators, \n",
    "    train_size=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = GroupShuffleSplitND(n_splits=9, test_size=0.40, random_state=np.random.RandomState(42))  # here, we reuse the outer splitter as well, so we use RandomState\n",
    "inner_splitter = GroupShuffleSplitND(n_splits=1, test_size=0.45/0.60, random_state=np.random.RandomState(42))  # we use a RandomState instance, not an int, because we will reuse this splitter several times\n",
    "\n",
    "indices, sizes, pos_class, unique_initiators, unique_monomers, unique_terminators = split_3d(splitter, inner_splitter)\n",
    "\n",
    "print(sizes)\n",
    "print(pos_class)\n",
    "print(unique_initiators)\n",
    "print(unique_monomers)\n",
    "print(unique_terminators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_indices_and_stats(\n",
    "    indices, \n",
    "    sizes, \n",
    "    pos_class,\n",
    "    total_size=len(df),\n",
    "    data_name=data_name,\n",
    "    split_dimension=3, \n",
    "    save_indices=True, \n",
    "    n_initiators=unique_initiators, \n",
    "    n_monomers=unique_monomers, \n",
    "    n_terminators=unique_terminators, \n",
    "    train_size=15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = GroupShuffleSplitND(n_splits=9, test_size=0.45, random_state=np.random.RandomState(42))  # here, we reuse the outer splitter as well, so we use RandomState\n",
    "inner_splitter = GroupShuffleSplitND(n_splits=1, test_size=0.45/0.55, random_state=np.random.RandomState(42))  # we use a RandomState instance, not an int, because we will reuse this splitter several times\n",
    "\n",
    "indices, sizes, pos_class, unique_initiators, unique_monomers, unique_terminators = split_3d(splitter, inner_splitter)\n",
    "\n",
    "print(sizes)\n",
    "print(pos_class)\n",
    "print(unique_initiators)\n",
    "print(unique_monomers)\n",
    "print(unique_terminators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_indices_and_stats(\n",
    "    indices, \n",
    "    sizes, \n",
    "    pos_class,\n",
    "    total_size=len(df),\n",
    "    data_name=data_name,\n",
    "    split_dimension=3, \n",
    "    save_indices=True, \n",
    "    n_initiators=unique_initiators, \n",
    "    n_monomers=unique_monomers, \n",
    "    n_terminators=unique_terminators, \n",
    "    train_size=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
