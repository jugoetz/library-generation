{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Data Splits\n",
    "\n",
    "We want to split the SynFerm data set into train, validation and test data.\n",
    "For all splits, we will do 9 random repetitions.\n",
    "For 1D and 2D split, which both use 3 different groups to split on, these will divide into 3 random repetitions for each of the 3 groups. \n",
    "\n",
    "### 0D Split\n",
    "For the 0D split, we use a random train-test split.\n",
    "We use a 80/10/10 split into train, val, and test set.\n",
    "\n",
    "### 1D Split\n",
    "For the 1D split, we use a (1D) GroupShuffleSplit.\n",
    "Each individual split will be 80/10/10 train/test (of groups not samples!).\n",
    "As groups, we use either initiator, monomer, or terminator.\n",
    "\n",
    "### 2D Split\n",
    "For the 2D split, we use a (2D) GroupShuffleSplit.\n",
    "Each individual split will use 10% of groups as test set and 12.5% of remaining groups as validation set. \n",
    "Due to the dimensionality, this means we expect 0.1 * 0.1 = 1% of samples in the test and validation set and 0.900^2 * 0.875^2 = 62.0% of sample in the training set.\n",
    "The remaining samples are not used to prevent leakage.\n",
    "As groups, we use either \\[initiator, monomer], \\[monomer, terminator] or \\[initiator, terminator].\n",
    "\n",
    "### 3D Split\n",
    "For the 3D split, we use a (3D) GroupShuffleSplit.\n",
    "Each individual split will use 20% of groups as test set, 25% of remaining groups as validation set, and the remaining groups as training set.\n",
    "Due to the dimensionality, this means we expect 0.2^3 = 0.8% of samples in the test and validation set and 0.800^3 * 0.750^3 = 21.6% of sample in the training set."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import sys\n",
    "\n",
    "sys.path.append(str(pathlib.Path().resolve().parents[1]))\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GroupShuffleSplit, ShuffleSplit\n",
    "\n",
    "from src.definitions import DATA_DIR\n",
    "from src.util.train_test_split import GroupShuffleSplitND"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T14:32:49.080606Z",
     "start_time": "2023-09-07T14:32:48.462426Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "(40018, 27)"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "data_filename = \"synferm_dataset_2023-09-05_40018records.csv\"\n",
    "data_name = data_filename.rsplit(\"_\", maxsplit=1)[0]\n",
    "df = pd.read_csv(DATA_DIR / \"curated_data\" / data_filename)\n",
    "df.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T14:32:49.420123Z",
     "start_time": "2023-09-07T14:32:49.081396Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "     I_long    M_long     T_long  \\\n0  2-Pyr003  Fused002  TerABT004   \n1  2-Pyr003  Fused002  TerABT007   \n2  2-Pyr003  Fused002  TerABT013   \n3  2-Pyr003  Fused002  TerABT014   \n4  2-Pyr003  Fused002   TerTH001   \n\n                                    product_A_smiles  \\\n0  COc1ccc(CCOC(=O)N2C[C@H](NC(=O)c3cccc(Cl)n3)[C...   \n1  COc1ccc(CCOC(=O)N2C[C@H](NC(=O)c3cccc(Cl)n3)[C...   \n2  COc1ccc(CCOC(=O)N2C[C@H](NC(=O)c3cccc(Cl)n3)[C...   \n3  COc1ccc(CCOC(=O)N2C[C@H](NC(=O)c3cccc(Cl)n3)[C...   \n4  COc1ccc(CCOC(=O)N2C[C@H](NC(=O)c3cccc(Cl)n3)[C...   \n\n                            I_smiles  \\\n0  O=C(c1cccc(Cl)n1)[B-](F)(F)F.[K+]   \n1  O=C(c1cccc(Cl)n1)[B-](F)(F)F.[K+]   \n2  O=C(c1cccc(Cl)n1)[B-](F)(F)F.[K+]   \n3  O=C(c1cccc(Cl)n1)[B-](F)(F)F.[K+]   \n4  O=C(c1cccc(Cl)n1)[B-](F)(F)F.[K+]   \n\n                                            M_smiles  \\\n0  COc1ccc(CCOC(=O)N2C[C@@H]3NO[C@]4(OC5(CCCCC5)O...   \n1  COc1ccc(CCOC(=O)N2C[C@@H]3NO[C@]4(OC5(CCCCC5)O...   \n2  COc1ccc(CCOC(=O)N2C[C@@H]3NO[C@]4(OC5(CCCCC5)O...   \n3  COc1ccc(CCOC(=O)N2C[C@@H]3NO[C@]4(OC5(CCCCC5)O...   \n4  COc1ccc(CCOC(=O)N2C[C@@H]3NO[C@]4(OC5(CCCCC5)O...   \n\n                     T_smiles  \\\n0               Nc1ccc(F)cc1S   \n1              Nc1cc(Br)ccc1S   \n2        Nc1cc(C(F)(F)F)ccc1S   \n3              Nc1ccc(Cl)cc1S   \n4  [Cl-].[NH3+]NC(=S)c1ccccc1   \n\n                                     reaction_smiles  \\\n0  O=C(c1cccc(Cl)n1)[B-](F)(F)F.COc1ccc(CCOC(=O)N...   \n1  O=C(c1cccc(Cl)n1)[B-](F)(F)F.COc1ccc(CCOC(=O)N...   \n2  O=C(c1cccc(Cl)n1)[B-](F)(F)F.COc1ccc(CCOC(=O)N...   \n3  O=C(c1cccc(Cl)n1)[B-](F)(F)F.COc1ccc(CCOC(=O)N...   \n4  O=C(c1cccc(Cl)n1)[B-](F)(F)F.COc1ccc(CCOC(=O)N...   \n\n                         reaction_smiles_atom_mapped experiment_id  ...  \\\n0  F[B-](F)(F)[C:2]([c:1]1[cH:16][cH:18][cH:20][c...         56113  ...   \n1  F[B-](F)(F)[C:2]([c:1]1[cH:16][cH:18][cH:20][c...         56114  ...   \n2  F[B-](F)(F)[C:2]([c:1]1[cH:16][cH:18][cH:20][c...         56106  ...   \n3  F[B-](F)(F)[C:2]([c:1]1[cH:16][cH:18][cH:20][c...         56112  ...   \n4  F[B-](F)(F)[C:2]([c:1]1[cH:13][cH:15][cH:17][c...         56109  ...   \n\n   binary_H  scaled_A  scaled_B  scaled_C  scaled_D  scaled_E  scaled_F  \\\n0         0  0.036021  0.003427       0.0  0.020975  0.002958  0.941981   \n1         0  0.000000  0.000000       0.0  0.006159  0.364398  0.928851   \n2         1  0.000000  0.000000       0.0  0.014212  2.166420  1.013596   \n3         0  0.028915  0.005039       0.0  0.015578  0.504057  0.992614   \n4         0  0.350061  0.643219       0.0  0.031689  0.613596  0.109309   \n\n   scaled_G  scaled_H   major_A-C  \n0  0.914281   0.00000           A  \n1  1.106548   0.00000  no_product  \n2  0.537785   0.05686  no_product  \n3  0.890646   0.00000           A  \n4  0.439018   0.00000           B  \n\n[5 rows x 27 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>I_long</th>\n      <th>M_long</th>\n      <th>T_long</th>\n      <th>product_A_smiles</th>\n      <th>I_smiles</th>\n      <th>M_smiles</th>\n      <th>T_smiles</th>\n      <th>reaction_smiles</th>\n      <th>reaction_smiles_atom_mapped</th>\n      <th>experiment_id</th>\n      <th>...</th>\n      <th>binary_H</th>\n      <th>scaled_A</th>\n      <th>scaled_B</th>\n      <th>scaled_C</th>\n      <th>scaled_D</th>\n      <th>scaled_E</th>\n      <th>scaled_F</th>\n      <th>scaled_G</th>\n      <th>scaled_H</th>\n      <th>major_A-C</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2-Pyr003</td>\n      <td>Fused002</td>\n      <td>TerABT004</td>\n      <td>COc1ccc(CCOC(=O)N2C[C@H](NC(=O)c3cccc(Cl)n3)[C...</td>\n      <td>O=C(c1cccc(Cl)n1)[B-](F)(F)F.[K+]</td>\n      <td>COc1ccc(CCOC(=O)N2C[C@@H]3NO[C@]4(OC5(CCCCC5)O...</td>\n      <td>Nc1ccc(F)cc1S</td>\n      <td>O=C(c1cccc(Cl)n1)[B-](F)(F)F.COc1ccc(CCOC(=O)N...</td>\n      <td>F[B-](F)(F)[C:2]([c:1]1[cH:16][cH:18][cH:20][c...</td>\n      <td>56113</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0.036021</td>\n      <td>0.003427</td>\n      <td>0.0</td>\n      <td>0.020975</td>\n      <td>0.002958</td>\n      <td>0.941981</td>\n      <td>0.914281</td>\n      <td>0.00000</td>\n      <td>A</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2-Pyr003</td>\n      <td>Fused002</td>\n      <td>TerABT007</td>\n      <td>COc1ccc(CCOC(=O)N2C[C@H](NC(=O)c3cccc(Cl)n3)[C...</td>\n      <td>O=C(c1cccc(Cl)n1)[B-](F)(F)F.[K+]</td>\n      <td>COc1ccc(CCOC(=O)N2C[C@@H]3NO[C@]4(OC5(CCCCC5)O...</td>\n      <td>Nc1cc(Br)ccc1S</td>\n      <td>O=C(c1cccc(Cl)n1)[B-](F)(F)F.COc1ccc(CCOC(=O)N...</td>\n      <td>F[B-](F)(F)[C:2]([c:1]1[cH:16][cH:18][cH:20][c...</td>\n      <td>56114</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.006159</td>\n      <td>0.364398</td>\n      <td>0.928851</td>\n      <td>1.106548</td>\n      <td>0.00000</td>\n      <td>no_product</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2-Pyr003</td>\n      <td>Fused002</td>\n      <td>TerABT013</td>\n      <td>COc1ccc(CCOC(=O)N2C[C@H](NC(=O)c3cccc(Cl)n3)[C...</td>\n      <td>O=C(c1cccc(Cl)n1)[B-](F)(F)F.[K+]</td>\n      <td>COc1ccc(CCOC(=O)N2C[C@@H]3NO[C@]4(OC5(CCCCC5)O...</td>\n      <td>Nc1cc(C(F)(F)F)ccc1S</td>\n      <td>O=C(c1cccc(Cl)n1)[B-](F)(F)F.COc1ccc(CCOC(=O)N...</td>\n      <td>F[B-](F)(F)[C:2]([c:1]1[cH:16][cH:18][cH:20][c...</td>\n      <td>56106</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.014212</td>\n      <td>2.166420</td>\n      <td>1.013596</td>\n      <td>0.537785</td>\n      <td>0.05686</td>\n      <td>no_product</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2-Pyr003</td>\n      <td>Fused002</td>\n      <td>TerABT014</td>\n      <td>COc1ccc(CCOC(=O)N2C[C@H](NC(=O)c3cccc(Cl)n3)[C...</td>\n      <td>O=C(c1cccc(Cl)n1)[B-](F)(F)F.[K+]</td>\n      <td>COc1ccc(CCOC(=O)N2C[C@@H]3NO[C@]4(OC5(CCCCC5)O...</td>\n      <td>Nc1ccc(Cl)cc1S</td>\n      <td>O=C(c1cccc(Cl)n1)[B-](F)(F)F.COc1ccc(CCOC(=O)N...</td>\n      <td>F[B-](F)(F)[C:2]([c:1]1[cH:16][cH:18][cH:20][c...</td>\n      <td>56112</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0.028915</td>\n      <td>0.005039</td>\n      <td>0.0</td>\n      <td>0.015578</td>\n      <td>0.504057</td>\n      <td>0.992614</td>\n      <td>0.890646</td>\n      <td>0.00000</td>\n      <td>A</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2-Pyr003</td>\n      <td>Fused002</td>\n      <td>TerTH001</td>\n      <td>COc1ccc(CCOC(=O)N2C[C@H](NC(=O)c3cccc(Cl)n3)[C...</td>\n      <td>O=C(c1cccc(Cl)n1)[B-](F)(F)F.[K+]</td>\n      <td>COc1ccc(CCOC(=O)N2C[C@@H]3NO[C@]4(OC5(CCCCC5)O...</td>\n      <td>[Cl-].[NH3+]NC(=S)c1ccccc1</td>\n      <td>O=C(c1cccc(Cl)n1)[B-](F)(F)F.COc1ccc(CCOC(=O)N...</td>\n      <td>F[B-](F)(F)[C:2]([c:1]1[cH:13][cH:15][cH:17][c...</td>\n      <td>56109</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0.350061</td>\n      <td>0.643219</td>\n      <td>0.0</td>\n      <td>0.031689</td>\n      <td>0.613596</td>\n      <td>0.109309</td>\n      <td>0.439018</td>\n      <td>0.00000</td>\n      <td>B</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 27 columns</p>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T13:14:27.607722Z",
     "start_time": "2023-09-07T13:14:27.596203Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 0D split"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "outputs": [],
   "source": [
    "splitter = ShuffleSplit(n_splits=9, test_size=0.1, random_state=42)\n",
    "inner_splitter = ShuffleSplit(n_splits=1, test_size=0.1/0.9, random_state=np.random.RandomState(42))  # we use a RandomState instance, not an int, because we will reuse this splitter several times"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T09:03:29.483804Z",
     "start_time": "2023-09-07T09:03:29.464652Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(32014, 4002, 4002), (32014, 4002, 4002), (32014, 4002, 4002), (32014, 4002, 4002), (32014, 4002, 4002), (32014, 4002, 4002), (32014, 4002, 4002), (32014, 4002, 4002), (32014, 4002, 4002)]\n",
      "[(25899, 3257, 3266), (25887, 3269, 3221), (25911, 3245, 3262), (25932, 3224, 3244), (25953, 3203, 3272), (25931, 3225, 3262), (25896, 3260, 3272), (25892, 3264, 3248), (25902, 3254, 3285)]\n"
     ]
    }
   ],
   "source": [
    "indices = []\n",
    "sizes = []\n",
    "pos_class_A = []\n",
    "for idx_train_val, idx_test in splitter.split(list(range(len(df)))):\n",
    "    idx_train, idx_val = next(inner_splitter.split(idx_train_val))\n",
    "    indices.append((idx_train, idx_val, idx_test))\n",
    "    sizes.append((len(idx_train), len(idx_val), len(idx_test)))\n",
    "    pos_class_A.append((np.sum(df['binary_A'][idx_train]), np.sum(df['binary_A'][idx_val]), np.sum(df['binary_A'][idx_test])))\n",
    "\n",
    "print(sizes)\n",
    "print(pos_class_A)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T09:03:29.910717Z",
     "start_time": "2023-09-07T09:03:29.881183Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "#define a write function that we can reuse later\n",
    "def write_indices_and_stats(indices, sizes, pos_class_A, split_dimension, save_indices=True):\n",
    "    \"\"\"\n",
    "    Write function that can be reused for the other splits\n",
    "    Args:\n",
    "        indices: list of 3-tuples\n",
    "        sizes: list of 3-tuples, length equal to indices\n",
    "        pos_class_A: list of 3-tuples, length equal to indices\n",
    "        split_dimension: str or int, e.g. \"0\", \"1\", \"2\", \"3\"\n",
    "        save_indices: bool, whether to save the indices. Useful if we need to regenerate statistics. Default: True\n",
    "    \"\"\"\n",
    "    n_folds = len(indices)\n",
    "    save_dir = DATA_DIR / \"curated_data\" / \"splits\" / f\"{data_name}_{split_dimension}D_split\"\n",
    "    save_dir.mkdir(parents=True, exist_ok=True)\n",
    "    if save_indices:\n",
    "        for i, (idx_train, idx_val, idx_test) in enumerate(indices):\n",
    "            with open(save_dir / f\"fold{i}_train.csv\", \"w\") as f:\n",
    "                f.write(\"index\\n\")\n",
    "                f.write(\"\\n\".join([str(i) for i in idx_train]))\n",
    "                \n",
    "            with open(save_dir / f\"fold{i}_val.csv\", \"w\") as f:\n",
    "                f.write(\"index\\n\")\n",
    "                f.write(\"\\n\".join([str(i) for i in idx_val]))\n",
    "                \n",
    "            with open(save_dir / f\"fold{i}_test.csv\", \"w\") as f:\n",
    "                f.write(\"index\\n\")\n",
    "                f.write(\"\\n\".join([str(i) for i in idx_test]))\n",
    "    \n",
    "    for i, (size, count_A) in enumerate(zip(sizes, pos_class_A)):\n",
    "        with open(save_dir / f\"fold{i}_statistics.txt\", \"w\") as f:\n",
    "            f.write(f\"Train samples: {size[0]} ({size[0]/len(df):.1%})\\n\")\n",
    "            f.write(f\"Val samples: {size[1]} ({size[1]/len(df):.1%})\\n\")\n",
    "            f.write(f\"Test samples: {size[2]} ({size[2]/len(df):.1%})\\n\")\n",
    "            if split_dimension > 1:\n",
    "                f.write(f\"Not used: {len(df) - np.sum(size):.0f} ({(len(df) - np.sum(size)) / len(df):.1%})\\n\")\n",
    "            f.write(f\"Train samples binary_A has label 1: {count_A[0]} ({count_A[0]/size[0]:.1%})\\n\")\n",
    "            f.write(f\"Val samples binary_A has label 1: {count_A[1]} ({count_A[1]/size[1]:.1%})\\n\")\n",
    "            f.write(f\"Test samples binary_A has label 1: {count_A[2]} ({count_A[2]/size[2]:.1%})\\n\")\n",
    "            \n",
    "    # summary statistics\n",
    "    sum_pos_class_A = np.sum(pos_class_A, axis=0)\n",
    "    sum_sizes = np.sum(sizes, axis=0)\n",
    "    with open(save_dir / \"summary_statistics.txt\", \"w\") as f:\n",
    "        f.write(f\"Mean Train samples: {sum_sizes[0] / n_folds:.0f} ({sum_sizes[0] / n_folds / len(df):.1%})\\n\")\n",
    "        f.write(f\"Mean Val samples: {sum_sizes[1] / n_folds:.0f} ({sum_sizes[1] / n_folds / len(df):.1%})\\n\")\n",
    "        f.write(f\"Mean Test samples: {sum_sizes[2] / n_folds:.0f} ({sum_sizes[2] / n_folds / len(df):.1%})\\n\")\n",
    "        if split_dimension > 1:\n",
    "            f.write(f\"Not used: {len(df) - np.sum(sum_sizes) / n_folds:.0f} ({(len(df) - np.sum(sum_sizes) / n_folds) / len(df):.1%})\\n\")\n",
    "        f.write(f\"Mean Train samples binary_A has label 1: {sum_pos_class_A[0] / n_folds:.0f} ({sum_pos_class_A[0]/sum_sizes[0]:.1%})\\n\")\n",
    "        f.write(f\"Mean Val samples binary_A has label 1: {sum_pos_class_A[1] / n_folds:.0f} ({sum_pos_class_A[1]/sum_sizes[1]:.1%})\\n\")\n",
    "        f.write(f\"Mean Test samples binary_A has label 1: {sum_pos_class_A[2] / n_folds:.0f} ({sum_pos_class_A[2]/sum_sizes[2]:.1%})\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T14:34:23.288902Z",
     "start_time": "2023-09-07T14:34:23.275813Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "outputs": [],
   "source": [
    "write_indices_and_stats(indices, sizes, pos_class_A, split_dimension=0, save_indices=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T09:03:30.898900Z",
     "start_time": "2023-09-07T09:03:30.770656Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1D split"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "outputs": [],
   "source": [
    "splitter = GroupShuffleSplit(n_splits=3, test_size=0.1, random_state=np.random.RandomState(42))  # here, we reuse the outer splitter as well, so we use RandomState\n",
    "inner_splitter = GroupShuffleSplit(n_splits=1, test_size=0.1/0.9, random_state=np.random.RandomState(42))  # we use a RandomState instance, not an int, because we will reuse this splitter several times"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T09:03:31.484312Z",
     "start_time": "2023-09-07T09:03:31.466760Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(30974, 4318, 4726), (32233, 3546, 4239), (32488, 4020, 3510), (30560, 4729, 4729), (31529, 3911, 4578), (31828, 4387, 3803), (31225, 3725, 5068), (31084, 4497, 4437), (32258, 3504, 4256)]\n",
      "[(25110, 3433, 4017), (26053, 2880, 3207), (26311, 3263, 2789), (24626, 3916, 4348), (25534, 3134, 3834), (25799, 3545, 3328), (25222, 3034, 4087), (25138, 3621, 2978), (26069, 2847, 3377)]\n"
     ]
    }
   ],
   "source": [
    "indices = []\n",
    "sizes = []\n",
    "pos_class_A = []\n",
    "for idx_train_val, idx_test in splitter.split(list(range(len(df))), groups=df[\"I_long\"]):\n",
    "    idx_train, idx_val = next(inner_splitter.split(idx_train_val, groups=df[\"I_long\"][idx_train_val]))\n",
    "    indices.append((idx_train, idx_val, idx_test))\n",
    "    sizes.append((len(idx_train), len(idx_val), len(idx_test)))\n",
    "    pos_class_A.append((np.sum(df['binary_A'][idx_train]), np.sum(df['binary_A'][idx_val]), np.sum(df['binary_A'][idx_test])))\n",
    "\n",
    "for idx_train_val, idx_test in splitter.split(list(range(len(df))), groups=df[\"M_long\"]):\n",
    "    idx_train, idx_val = next(inner_splitter.split(idx_train_val, groups=df[\"M_long\"][idx_train_val]))\n",
    "    indices.append((idx_train, idx_val, idx_test))\n",
    "    sizes.append((len(idx_train), len(idx_val), len(idx_test)))\n",
    "    pos_class_A.append((np.sum(df['binary_A'][idx_train]), np.sum(df['binary_A'][idx_val]), np.sum(df['binary_A'][idx_test])))\n",
    "    \n",
    "for idx_train_val, idx_test in splitter.split(list(range(len(df))), groups=df[\"T_long\"]):\n",
    "    idx_train, idx_val = next(inner_splitter.split(idx_train_val, groups=df[\"T_long\"][idx_train_val]))\n",
    "    indices.append((idx_train, idx_val, idx_test))\n",
    "    sizes.append((len(idx_train), len(idx_val), len(idx_test)))\n",
    "    pos_class_A.append((np.sum(df['binary_A'][idx_train]), np.sum(df['binary_A'][idx_val]), np.sum(df['binary_A'][idx_test])))\n",
    "\n",
    "print(sizes)\n",
    "print(pos_class_A)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T09:03:31.934686Z",
     "start_time": "2023-09-07T09:03:31.767646Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "outputs": [],
   "source": [
    "write_indices_and_stats(indices, sizes, pos_class_A, split_dimension=1, save_indices=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T09:03:33.169202Z",
     "start_time": "2023-09-07T09:03:33.025835Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2D split"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "splitter = GroupShuffleSplitND(n_splits=3, test_size=0.1, random_state=np.random.RandomState(42))  # here, we reuse the outer splitter as well, so we use RandomState\n",
    "inner_splitter = GroupShuffleSplitND(n_splits=1, test_size=0.1/0.9, random_state=np.random.RandomState(42))  # we use a RandomState instance, not an int, because we will reuse this splitter several times"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T14:33:11.678971Z",
     "start_time": "2023-09-07T14:33:11.665185Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(23868, 608, 565), (23681, 510, 520), (24526, 439, 528), (25765, 338, 602), (23780, 395, 559), (24445, 469, 518), (23951, 395, 687), (24688, 413, 460), (23946, 536, 599)]\n",
      "[(19233, 504, 464), (19138, 410, 498), (19818, 368, 406), (20796, 271, 429), (19269, 316, 481), (19738, 386, 406), (19397, 333, 630), (19965, 340, 337), (19601, 378, 490)]\n"
     ]
    }
   ],
   "source": [
    "indices = []\n",
    "sizes = []\n",
    "pos_class_A = []\n",
    "for idx_train_val, idx_test in splitter.split(df, groups=df[[\"I_long\", \"M_long\"]]):\n",
    "    idx_train, idx_val = next(inner_splitter.split(df.iloc[idx_train_val], groups=df[[\"I_long\", \"M_long\"]].iloc[idx_train_val]))\n",
    "    indices.append((idx_train, idx_val, idx_test))\n",
    "    sizes.append((len(idx_train), len(idx_val), len(idx_test)))\n",
    "    pos_class_A.append((np.sum(df['binary_A'][idx_train]), np.sum(df['binary_A'][idx_val]), np.sum(df['binary_A'][idx_test])))\n",
    "\n",
    "for idx_train_val, idx_test in splitter.split(list(range(len(df))), groups=df[[\"M_long\", \"T_long\"]]):\n",
    "    idx_train, idx_val = next(inner_splitter.split(idx_train_val, groups=df[[\"M_long\", \"T_long\"]].iloc[idx_train_val]))\n",
    "    indices.append((idx_train, idx_val, idx_test))\n",
    "    sizes.append((len(idx_train), len(idx_val), len(idx_test)))\n",
    "    pos_class_A.append((np.sum(df['binary_A'][idx_train]), np.sum(df['binary_A'][idx_val]), np.sum(df['binary_A'][idx_test])))\n",
    "    \n",
    "for idx_train_val, idx_test in splitter.split(list(range(len(df))), groups=df[[\"I_long\", \"T_long\"]]):\n",
    "    idx_train, idx_val = next(inner_splitter.split(idx_train_val, groups=df[[\"I_long\", \"T_long\"]].iloc[idx_train_val]))\n",
    "    indices.append((idx_train, idx_val, idx_test))\n",
    "    sizes.append((len(idx_train), len(idx_val), len(idx_test)))\n",
    "    pos_class_A.append((np.sum(df['binary_A'][idx_train]), np.sum(df['binary_A'][idx_val]), np.sum(df['binary_A'][idx_test])))\n",
    "\n",
    "print(sizes)\n",
    "print(pos_class_A)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T14:33:12.609222Z",
     "start_time": "2023-09-07T14:33:12.225879Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "write_indices_and_stats(indices, sizes, pos_class_A, split_dimension=2, save_indices=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T14:34:29.041886Z",
     "start_time": "2023-09-07T14:34:28.935422Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3D split"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "splitter = GroupShuffleSplitND(n_splits=9, test_size=0.2, random_state=np.random.RandomState(42))  # here, we reuse the outer splitter as well, so we use RandomState\n",
    "inner_splitter = GroupShuffleSplitND(n_splits=1, test_size=0.2/0.8, random_state=np.random.RandomState(42))  # we use a RandomState instance, not an int, because we will reuse this splitter several times"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T14:45:03.613229Z",
     "start_time": "2023-09-07T14:45:03.595789Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(8180, 355, 367), (7995, 305, 439), (7184, 424, 455), (8145, 272, 528), (8042, 338, 416), (7640, 428, 358), (8029, 398, 304), (7631, 423, 359), (8682, 261, 368)]\n",
      "[(6719, 270, 239), (6678, 246, 310), (5996, 344, 394), (6801, 223, 437), (6681, 275, 371), (6349, 360, 317), (6671, 308, 248), (6334, 347, 274), (7186, 208, 265)]\n"
     ]
    }
   ],
   "source": [
    "indices = []\n",
    "sizes = []\n",
    "pos_class_A = []\n",
    "for idx_train_val, idx_test in splitter.split(df, groups=df[[\"I_long\", \"M_long\", \"T_long\"]]):\n",
    "    idx_train, idx_val = next(inner_splitter.split(df.iloc[idx_train_val], groups=df[[\"I_long\", \"M_long\", \"T_long\"]].iloc[idx_train_val]))\n",
    "    indices.append((idx_train, idx_val, idx_test))\n",
    "    sizes.append((len(idx_train), len(idx_val), len(idx_test)))\n",
    "    pos_class_A.append((np.sum(df['binary_A'][idx_train]), np.sum(df['binary_A'][idx_val]), np.sum(df['binary_A'][idx_test])))\n",
    "\n",
    "print(sizes)\n",
    "print(pos_class_A)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T14:45:04.415157Z",
     "start_time": "2023-09-07T14:45:04.058182Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "write_indices_and_stats(indices, sizes, pos_class_A, split_dimension=3, save_indices=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T14:45:04.736402Z",
     "start_time": "2023-09-07T14:45:04.690770Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Control: Show statistics for previously prepared splits"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0D split: 36389 train, 4044 (10.0%) test\n",
      "\tTraining set 'major_A-C' class A: 53.2%\n",
      "\tTraining set 'major_A-C' class B: 21.9%\n",
      "\tTraining set 'major_A-C' class C: 8.6%\n",
      "\tTraining set 'major_A-C' class no_product: 16.2%\n",
      "\tTest set 'major_A-C' class A: 53.9%\n",
      "\tTest set 'major_A-C' class B: 21.5%\n",
      "\tTest set 'major_A-C' class C: 8.9%\n",
      "\tTest set 'major_A-C' class no_product: 15.7%\n"
     ]
    }
   ],
   "source": [
    "# 0D split\n",
    "lines = []\n",
    "train_idx = pd.read_csv(DATA_DIR / \"curated_data\" / \"splits\" / \"0D_split\" / \"train_idx.csv\")[\"index\"].values\n",
    "test_idx = pd.read_csv(DATA_DIR / \"curated_data\" / \"splits\" / \"0D_split\" / \"test_idx.csv\")[\"index\"].values\n",
    "lines.append(f\"0D split: {len(train_idx)} train, {len(test_idx)} ({len(test_idx)/(len(test_idx)+len(train_idx)):.1%}) test\")\n",
    "for i, item in (df[\"major_A-C\"].iloc[train_idx].value_counts().sort_index() / len(train_idx)).items():\n",
    "    lines.append(f\"\\tTraining set 'major_A-C' class {i}: {item:.1%}\")\n",
    "for i, item in (df[\"major_A-C\"].iloc[test_idx].value_counts().sort_index() / len(test_idx)).items():\n",
    "    lines.append(f\"\\tTest set 'major_A-C' class {i}: {item:.1%}\")\n",
    "# save stats to file\n",
    "with open(DATA_DIR / \"curated_data\" / \"splits\" / \"0D_split\" / \"split_statistics.txt\", \"w\") as f:\n",
    "    f.write(\"\\n\".join(lines))\n",
    "print(\"\\n\".join(lines))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-23T15:40:06.111835Z",
     "start_time": "2023-07-23T15:40:06.102860Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0: 35657 train, 4776 (11.8%) test\n",
      "\tTraining set 'major_A-C' class A: 52.3%\n",
      "\tTraining set 'major_A-C' class B: 21.8%\n",
      "\tTraining set 'major_A-C' class C: 9.3%\n",
      "\tTraining set 'major_A-C' class no_product: 16.6%\n",
      "\tTest set 'major_A-C' class A: 60.5%\n",
      "\tTest set 'major_A-C' class B: 22.2%\n",
      "\tTest set 'major_A-C' class C: 4.0%\n",
      "\tTest set 'major_A-C' class no_product: 13.3%\n",
      "Fold 1: 36166 train, 4267 (10.6%) test\n",
      "\tTraining set 'major_A-C' class A: 53.6%\n",
      "\tTraining set 'major_A-C' class B: 21.8%\n",
      "\tTraining set 'major_A-C' class C: 8.8%\n",
      "\tTraining set 'major_A-C' class no_product: 15.7%\n",
      "\tTest set 'major_A-C' class A: 50.1%\n",
      "\tTest set 'major_A-C' class B: 22.2%\n",
      "\tTest set 'major_A-C' class C: 7.5%\n",
      "\tTest set 'major_A-C' class no_product: 20.3%\n",
      "Fold 2: 36895 train, 3538 (8.8%) test\n",
      "\tTraining set 'major_A-C' class A: 53.1%\n",
      "\tTraining set 'major_A-C' class B: 21.8%\n",
      "\tTraining set 'major_A-C' class C: 9.2%\n",
      "\tTraining set 'major_A-C' class no_product: 16.0%\n",
      "\tTest set 'major_A-C' class A: 54.9%\n",
      "\tTest set 'major_A-C' class B: 23.0%\n",
      "\tTest set 'major_A-C' class C: 3.6%\n",
      "\tTest set 'major_A-C' class no_product: 18.4%\n",
      "Fold 3: 35686 train, 4747 (11.7%) test\n",
      "\tTraining set 'major_A-C' class A: 51.5%\n",
      "\tTraining set 'major_A-C' class B: 22.0%\n",
      "\tTraining set 'major_A-C' class C: 9.2%\n",
      "\tTraining set 'major_A-C' class no_product: 17.3%\n",
      "\tTest set 'major_A-C' class A: 66.6%\n",
      "\tTest set 'major_A-C' class B: 20.8%\n",
      "\tTest set 'major_A-C' class C: 4.9%\n",
      "\tTest set 'major_A-C' class no_product: 7.8%\n",
      "Fold 4: 35854 train, 4579 (11.3%) test\n",
      "\tTraining set 'major_A-C' class A: 52.2%\n",
      "\tTraining set 'major_A-C' class B: 22.4%\n",
      "\tTraining set 'major_A-C' class C: 8.9%\n",
      "\tTraining set 'major_A-C' class no_product: 16.5%\n",
      "\tTest set 'major_A-C' class A: 61.5%\n",
      "\tTest set 'major_A-C' class B: 17.4%\n",
      "\tTest set 'major_A-C' class C: 7.3%\n",
      "\tTest set 'major_A-C' class no_product: 13.9%\n",
      "Fold 5: 36615 train, 3818 (9.4%) test\n",
      "\tTraining set 'major_A-C' class A: 51.9%\n",
      "\tTraining set 'major_A-C' class B: 22.5%\n",
      "\tTraining set 'major_A-C' class C: 8.9%\n",
      "\tTraining set 'major_A-C' class no_product: 16.7%\n",
      "\tTest set 'major_A-C' class A: 66.3%\n",
      "\tTest set 'major_A-C' class B: 15.5%\n",
      "\tTest set 'major_A-C' class C: 6.5%\n",
      "\tTest set 'major_A-C' class no_product: 11.6%\n",
      "Fold 6: 35317 train, 5116 (12.7%) test\n",
      "\tTraining set 'major_A-C' class A: 54.6%\n",
      "\tTraining set 'major_A-C' class B: 20.7%\n",
      "\tTraining set 'major_A-C' class C: 8.4%\n",
      "\tTraining set 'major_A-C' class no_product: 16.2%\n",
      "\tTest set 'major_A-C' class A: 44.0%\n",
      "\tTest set 'major_A-C' class B: 29.7%\n",
      "\tTest set 'major_A-C' class C: 10.4%\n",
      "\tTest set 'major_A-C' class no_product: 15.8%\n",
      "Fold 7: 35957 train, 4476 (11.1%) test\n",
      "\tTraining set 'major_A-C' class A: 53.5%\n",
      "\tTraining set 'major_A-C' class B: 23.0%\n",
      "\tTraining set 'major_A-C' class C: 9.0%\n",
      "\tTraining set 'major_A-C' class no_product: 14.5%\n",
      "\tTest set 'major_A-C' class A: 51.1%\n",
      "\tTest set 'major_A-C' class B: 13.1%\n",
      "\tTest set 'major_A-C' class C: 5.9%\n",
      "\tTest set 'major_A-C' class no_product: 29.9%\n",
      "Fold 8: 36110 train, 4323 (10.7%) test\n",
      "\tTraining set 'major_A-C' class A: 54.6%\n",
      "\tTraining set 'major_A-C' class B: 21.1%\n",
      "\tTraining set 'major_A-C' class C: 8.5%\n",
      "\tTraining set 'major_A-C' class no_product: 15.8%\n",
      "\tTest set 'major_A-C' class A: 42.4%\n",
      "\tTest set 'major_A-C' class B: 28.0%\n",
      "\tTest set 'major_A-C' class C: 10.5%\n",
      "\tTest set 'major_A-C' class no_product: 19.1%\n"
     ]
    }
   ],
   "source": [
    "# 1D split\n",
    "lines = []\n",
    "for i in range(9):\n",
    "    train_idx = pd.read_csv(DATA_DIR / \"curated_data\" / \"splits\" / \"1D_split\" / f\"fold_{i}_train_idx.csv\")[\"index\"].values\n",
    "    test_idx = pd.read_csv(DATA_DIR / \"curated_data\" / \"splits\" / \"1D_split\" / f\"fold_{i}_test_idx.csv\")[\"index\"].values\n",
    "    lines.append(f\"Fold {i}: {len(train_idx)} train, {len(test_idx)} ({len(test_idx)/(len(test_idx)+len(train_idx)):.1%}) test\")\n",
    "    for i, item in (df[\"major_A-C\"].iloc[train_idx].value_counts().sort_index() / len(train_idx)).items():\n",
    "        lines.append(f\"\\tTraining set 'major_A-C' class {i}: {item:.1%}\")\n",
    "    for i, item in (df[\"major_A-C\"].iloc[test_idx].value_counts().sort_index() / len(test_idx)).items():\n",
    "        lines.append(f\"\\tTest set 'major_A-C' class {i}: {item:.1%}\")\n",
    "\n",
    "# save stats to file\n",
    "with open(DATA_DIR / \"curated_data\" / \"splits\" / \"1D_split\" / \"split_statistics.txt\", \"w\") as f:\n",
    "    f.write(\"\\n\".join(lines))\n",
    "print(\"\\n\".join(lines))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-23T15:39:10.877429Z",
     "start_time": "2023-07-23T15:39:10.837874Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
